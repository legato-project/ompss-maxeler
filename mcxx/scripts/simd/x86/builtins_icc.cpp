//
// Generator of src/frontend/cxx-iccbuiltin-sse.h for icc 16
//
// Use make generate_builtins_icc_simd (at the top level) to compile this file
//

#include <immintrin.h>

#include <iostream>
#include <sstream>
#include "builtins-common.hpp"
#include "builtins-common-icc.hpp"

// --------------------------------------------
// End of specific generators for ICC SIMD
// --------------------------------------------

#define END

#define VECTOR_INTRINSICS_LIST \
VECTOR_INTRIN(_addcarry_u32) \
VECTOR_INTRIN(_addcarry_u64) \
VECTOR_INTRIN(_addcarryx_u32) \
VECTOR_INTRIN(_addcarryx_u64) \
VECTOR_INTRIN(_allow_cpu_features) \
VECTOR_INTRIN(_bextr_u32) \
VECTOR_INTRIN(_bextr_u64) \
VECTOR_INTRIN(_bit_scan_forward) \
VECTOR_INTRIN(_BitScanForward) \
VECTOR_INTRIN(_BitScanForward64) \
VECTOR_INTRIN(_bit_scan_reverse) \
VECTOR_INTRIN(_BitScanReverse) \
VECTOR_INTRIN(_BitScanReverse64) \
VECTOR_INTRIN(_bittest) \
VECTOR_INTRIN(_bittest64) \
VECTOR_INTRIN(_bittestandcomplement) \
VECTOR_INTRIN(_bittestandcomplement64) \
VECTOR_INTRIN(_bittestandreset) \
VECTOR_INTRIN(_bittestandreset64) \
VECTOR_INTRIN(_bittestandset) \
VECTOR_INTRIN(_bittestandset64) \
VECTOR_INTRIN(_blsi_u32) \
VECTOR_INTRIN(_blsi_u64) \
VECTOR_INTRIN(_blsmsk_u32) \
VECTOR_INTRIN(_blsmsk_u64) \
VECTOR_INTRIN(_blsr_u32) \
VECTOR_INTRIN(_blsr_u64) \
VECTOR_INTRIN(_bnd_chk_ptr_bounds) \
VECTOR_INTRIN(_bnd_chk_ptr_lbounds) \
VECTOR_INTRIN(_bnd_chk_ptr_ubounds) \
VECTOR_INTRIN(_bnd_copy_ptr_bounds) \
VECTOR_INTRIN(_bnd_get_ptr_lbound) \
VECTOR_INTRIN(_bnd_get_ptr_ubound) \
VECTOR_INTRIN(_bnd_init_ptr_bounds) \
VECTOR_INTRIN(_bnd_narrow_ptr_bounds) \
VECTOR_INTRIN(_bnd_set_ptr_bounds) \
VECTOR_INTRIN(_bnd_store_ptr_bounds) \
VECTOR_INTRIN(_bswap) \
VECTOR_INTRIN(_bswap64) \
VECTOR_INTRIN(_bzhi_u32) \
VECTOR_INTRIN(_bzhi_u64) \
VECTOR_INTRIN(_castf32_u32) \
VECTOR_INTRIN(_castf64_u64) \
VECTOR_INTRIN(_castu32_f32) \
VECTOR_INTRIN(_castu64_f64) \
VECTOR_INTRIN(_cvtsh_ss) \
VECTOR_INTRIN(_cvtss_sh) \
VECTOR_INTRIN(_fxrstor) \
VECTOR_INTRIN(_fxrstor64) \
VECTOR_INTRIN(_fxsave) \
VECTOR_INTRIN(_fxsave64) \
VECTOR_INTRIN(_invpcid) \
VECTOR_INTRIN(_loadbe_i16) \
VECTOR_INTRIN(_loadbe_i32) \
VECTOR_INTRIN(_loadbe_i64) \
VECTOR_INTRIN(_lrotl) \
VECTOR_INTRIN(_lrotr) \
VECTOR_INTRIN(_lzcnt_u32) \
VECTOR_INTRIN(_lzcnt_u64) \
VECTOR_INTRIN(_may_i_use_cpu_feature) \
VECTOR_INTRIN(_m_empty) \
VECTOR_INTRIN(_m_from_int) \
VECTOR_INTRIN(_m_from_int64) \
VECTOR_INTRIN(_mm256_abs_epi16) \
VECTOR_INTRIN(_mm256_abs_epi32) \
VECTOR_INTRIN(_mm256_abs_epi64) \
VECTOR_INTRIN(_mm256_abs_epi8) \
VECTOR_INTRIN(_mm256_acosh_pd) \
VECTOR_INTRIN(_mm256_acosh_ps) \
VECTOR_INTRIN(_mm256_acos_pd) \
VECTOR_INTRIN(_mm256_acos_ps) \
VECTOR_INTRIN(_mm256_add_epi16) \
VECTOR_INTRIN(_mm256_add_epi32) \
VECTOR_INTRIN(_mm256_add_epi64) \
VECTOR_INTRIN(_mm256_add_epi8) \
VECTOR_INTRIN(_mm256_add_pd) \
VECTOR_INTRIN(_mm256_add_ps) \
VECTOR_INTRIN(_mm256_adds_epi16) \
VECTOR_INTRIN(_mm256_adds_epi8) \
VECTOR_INTRIN(_mm256_adds_epu16) \
VECTOR_INTRIN(_mm256_adds_epu8) \
VECTOR_INTRIN(_mm256_addsub_pd) \
VECTOR_INTRIN(_mm256_addsub_ps) \
VECTOR_INTRIN(_mm256_alignr_epi32) \
VECTOR_INTRIN(_mm256_alignr_epi64) \
VECTOR_INTRIN(_mm256_alignr_epi8) \
VECTOR_INTRIN(_mm256_andnot_pd) \
VECTOR_INTRIN(_mm256_andnot_ps) \
VECTOR_INTRIN(_mm256_andnot_si256) \
VECTOR_INTRIN(_mm256_and_pd) \
VECTOR_INTRIN(_mm256_and_ps) \
VECTOR_INTRIN(_mm256_and_si256) \
VECTOR_INTRIN(_mm256_asinh_pd) \
VECTOR_INTRIN(_mm256_asinh_ps) \
VECTOR_INTRIN(_mm256_asin_pd) \
VECTOR_INTRIN(_mm256_asin_ps) \
VECTOR_INTRIN(_mm256_atan2_pd) \
VECTOR_INTRIN(_mm256_atan2_ps) \
VECTOR_INTRIN(_mm256_atanh_pd) \
VECTOR_INTRIN(_mm256_atanh_ps) \
VECTOR_INTRIN(_mm256_atan_pd) \
VECTOR_INTRIN(_mm256_atan_ps) \
VECTOR_INTRIN(_mm256_avg_epu16) \
VECTOR_INTRIN(_mm256_avg_epu8) \
VECTOR_INTRIN(_mm256_blend_epi16) \
VECTOR_INTRIN(_mm256_blend_epi32) \
VECTOR_INTRIN(_mm256_blend_pd) \
VECTOR_INTRIN(_mm256_blend_ps) \
VECTOR_INTRIN(_mm256_blendv_epi8) \
VECTOR_INTRIN(_mm256_blendv_pd) \
VECTOR_INTRIN(_mm256_blendv_ps) \
VECTOR_INTRIN(_mm256_broadcastb_epi8) \
VECTOR_INTRIN(_mm256_broadcastd_epi32) \
VECTOR_INTRIN(_mm256_broadcast_f32x2) \
VECTOR_INTRIN(_mm256_broadcast_f32x4) \
VECTOR_INTRIN(_mm256_broadcast_f64x2) \
VECTOR_INTRIN(_mm256_broadcast_i32x2) \
VECTOR_INTRIN(_mm256_broadcast_i32x4) \
VECTOR_INTRIN(_mm256_broadcast_i64x2) \
VECTOR_INTRIN(_mm256_broadcastmb_epi64) \
VECTOR_INTRIN(_mm256_broadcastmw_epi32) \
VECTOR_INTRIN(_mm256_broadcast_pd) \
VECTOR_INTRIN(_mm256_broadcast_ps) \
VECTOR_INTRIN(_mm256_broadcastq_epi64) \
VECTOR_INTRIN(_mm256_broadcast_sd) \
VECTOR_INTRIN(_mm256_broadcastsd_pd) \
VECTOR_INTRIN(_mm256_broadcastsi128_si256) \
VECTOR_INTRIN(_mm256_broadcast_ss) \
VECTOR_INTRIN(_mm256_broadcastss_ps) \
VECTOR_INTRIN(_mm256_broadcastw_epi16) \
VECTOR_INTRIN(_mm256_bslli_epi128) \
VECTOR_INTRIN(_mm256_bsrli_epi128) \
VECTOR_INTRIN(_mm256_castpd128_pd256) \
VECTOR_INTRIN(_mm256_castpd256_pd128) \
VECTOR_INTRIN(_mm256_castpd_ps) \
VECTOR_INTRIN(_mm256_castpd_si256) \
VECTOR_INTRIN(_mm256_castps128_ps256) \
VECTOR_INTRIN(_mm256_castps256_ps128) \
VECTOR_INTRIN(_mm256_castps_pd) \
VECTOR_INTRIN(_mm256_castps_si256) \
VECTOR_INTRIN(_mm256_castsi128_si256) \
VECTOR_INTRIN(_mm256_castsi256_pd) \
VECTOR_INTRIN(_mm256_castsi256_ps) \
VECTOR_INTRIN(_mm256_castsi256_si128) \
VECTOR_INTRIN(_mm256_cbrt_pd) \
VECTOR_INTRIN(_mm256_cbrt_ps) \
VECTOR_INTRIN(_mm256_cdfnorminv_pd) \
VECTOR_INTRIN(_mm256_cdfnorminv_ps) \
VECTOR_INTRIN(_mm256_cdfnorm_pd) \
VECTOR_INTRIN(_mm256_cdfnorm_ps) \
VECTOR_INTRIN(_mm256_cexp_ps) \
VECTOR_INTRIN(_mm256_clog_ps) \
VECTOR_INTRIN(_mm256_cmp_epi16_mask) \
VECTOR_INTRIN(_mm256_cmp_epi32_mask) \
VECTOR_INTRIN(_mm256_cmp_epi64_mask) \
VECTOR_INTRIN(_mm256_cmp_epi8_mask) \
VECTOR_INTRIN(_mm256_cmp_epu16_mask) \
VECTOR_INTRIN(_mm256_cmp_epu32_mask) \
VECTOR_INTRIN(_mm256_cmp_epu64_mask) \
VECTOR_INTRIN(_mm256_cmp_epu8_mask) \
VECTOR_INTRIN(_mm256_cmpeq_epi16) \
VECTOR_INTRIN(_mm256_cmpeq_epi32) \
VECTOR_INTRIN(_mm256_cmpeq_epi64) \
VECTOR_INTRIN(_mm256_cmpeq_epi8) \
VECTOR_INTRIN(_mm256_cmpgt_epi16) \
VECTOR_INTRIN(_mm256_cmpgt_epi32) \
VECTOR_INTRIN(_mm256_cmpgt_epi64) \
VECTOR_INTRIN(_mm256_cmpgt_epi8) \
VECTOR_INTRIN(_mm256_cmp_pd) \
VECTOR_INTRIN(_mm256_cmp_pd_mask) \
VECTOR_INTRIN(_mm256_cmp_ps) \
VECTOR_INTRIN(_mm256_cmp_ps_mask) \
VECTOR_INTRIN(_mm256_conflict_epi32) \
VECTOR_INTRIN(_mm256_conflict_epi64) \
VECTOR_INTRIN(_mm256_cosd_pd) \
VECTOR_INTRIN(_mm256_cosd_ps) \
VECTOR_INTRIN(_mm256_cosh_pd) \
VECTOR_INTRIN(_mm256_cosh_ps) \
VECTOR_INTRIN(_mm256_cos_pd) \
VECTOR_INTRIN(_mm256_cos_ps) \
VECTOR_INTRIN(_mm256_csqrt_ps) \
VECTOR_INTRIN(_mm256_cvtepi16_epi32) \
VECTOR_INTRIN(_mm256_cvtepi16_epi64) \
VECTOR_INTRIN(_mm256_cvtepi16_epi8) \
VECTOR_INTRIN(_mm256_cvtepi32_epi16) \
VECTOR_INTRIN(_mm256_cvtepi32_epi64) \
VECTOR_INTRIN(_mm256_cvtepi32_epi8) \
VECTOR_INTRIN(_mm256_cvtepi32_pd) \
VECTOR_INTRIN(_mm256_cvtepi32_ps) \
VECTOR_INTRIN(_mm256_cvtepi64_epi16) \
VECTOR_INTRIN(_mm256_cvtepi64_epi32) \
VECTOR_INTRIN(_mm256_cvtepi64_epi8) \
VECTOR_INTRIN(_mm256_cvtepi64_pd) \
VECTOR_INTRIN(_mm256_cvtepi64_ps) \
VECTOR_INTRIN(_mm256_cvtepi8_epi16) \
VECTOR_INTRIN(_mm256_cvtepi8_epi32) \
VECTOR_INTRIN(_mm256_cvtepi8_epi64) \
VECTOR_INTRIN(_mm256_cvtepu16_epi32) \
VECTOR_INTRIN(_mm256_cvtepu16_epi64) \
VECTOR_INTRIN(_mm256_cvtepu32_epi64) \
VECTOR_INTRIN(_mm256_cvtepu32_pd) \
VECTOR_INTRIN(_mm256_cvtepu64_pd) \
VECTOR_INTRIN(_mm256_cvtepu64_ps) \
VECTOR_INTRIN(_mm256_cvtepu8_epi16) \
VECTOR_INTRIN(_mm256_cvtepu8_epi32) \
VECTOR_INTRIN(_mm256_cvtepu8_epi64) \
VECTOR_INTRIN(_mm256_cvtpd_epi32) \
VECTOR_INTRIN(_mm256_cvtpd_epi64) \
VECTOR_INTRIN(_mm256_cvtpd_epu32) \
VECTOR_INTRIN(_mm256_cvtpd_epu64) \
VECTOR_INTRIN(_mm256_cvtpd_ps) \
VECTOR_INTRIN(_mm256_cvtph_ps) \
VECTOR_INTRIN(_mm256_cvtps_epi32) \
VECTOR_INTRIN(_mm256_cvtps_epi64) \
VECTOR_INTRIN(_mm256_cvtps_epu32) \
VECTOR_INTRIN(_mm256_cvtps_epu64) \
VECTOR_INTRIN(_mm256_cvtps_pd) \
VECTOR_INTRIN(_mm256_cvtps_ph) \
VECTOR_INTRIN(_mm256_cvtsd_f64) \
VECTOR_INTRIN(_mm256_cvtsepi16_epi8) \
VECTOR_INTRIN(_mm256_cvtsepi32_epi16) \
VECTOR_INTRIN(_mm256_cvtsepi32_epi8) \
VECTOR_INTRIN(_mm256_cvtsepi64_epi16) \
VECTOR_INTRIN(_mm256_cvtsepi64_epi32) \
VECTOR_INTRIN(_mm256_cvtsepi64_epi8) \
VECTOR_INTRIN(_mm256_cvtsi256_si32) \
VECTOR_INTRIN(_mm256_cvtss_f32) \
VECTOR_INTRIN(_mm256_cvttpd_epi32) \
VECTOR_INTRIN(_mm256_cvttpd_epi64) \
VECTOR_INTRIN(_mm256_cvttpd_epu32) \
VECTOR_INTRIN(_mm256_cvttpd_epu64) \
VECTOR_INTRIN(_mm256_cvttps_epi32) \
VECTOR_INTRIN(_mm256_cvttps_epi64) \
VECTOR_INTRIN(_mm256_cvttps_epu32) \
VECTOR_INTRIN(_mm256_cvttps_epu64) \
VECTOR_INTRIN(_mm256_cvtusepi16_epi8) \
VECTOR_INTRIN(_mm256_cvtusepi32_epi16) \
VECTOR_INTRIN(_mm256_cvtusepi32_epi8) \
VECTOR_INTRIN(_mm256_cvtusepi64_epi16) \
VECTOR_INTRIN(_mm256_cvtusepi64_epi32) \
VECTOR_INTRIN(_mm256_cvtusepi64_epi8) \
VECTOR_INTRIN(_mm256_dbsad_epu8) \
VECTOR_INTRIN(_mm256_div_epi16) \
VECTOR_INTRIN(_mm256_div_epi32) \
VECTOR_INTRIN(_mm256_div_epi64) \
VECTOR_INTRIN(_mm256_div_epi8) \
VECTOR_INTRIN(_mm256_div_epu16) \
VECTOR_INTRIN(_mm256_div_epu32) \
VECTOR_INTRIN(_mm256_div_epu64) \
VECTOR_INTRIN(_mm256_div_epu8) \
VECTOR_INTRIN(_mm256_div_pd) \
VECTOR_INTRIN(_mm256_div_ps) \
VECTOR_INTRIN(_mm256_dp_ps) \
VECTOR_INTRIN(_mm256_erfcinv_pd) \
VECTOR_INTRIN(_mm256_erfcinv_ps) \
VECTOR_INTRIN(_mm256_erfc_pd) \
VECTOR_INTRIN(_mm256_erfc_ps) \
VECTOR_INTRIN(_mm256_erfinv_pd) \
VECTOR_INTRIN(_mm256_erfinv_ps) \
VECTOR_INTRIN(_mm256_erf_pd) \
VECTOR_INTRIN(_mm256_erf_ps) \
VECTOR_INTRIN(_mm256_exp10_pd) \
VECTOR_INTRIN(_mm256_exp10_ps) \
VECTOR_INTRIN(_mm256_exp2_pd) \
VECTOR_INTRIN(_mm256_exp2_ps) \
VECTOR_INTRIN(_mm256_expm1_pd) \
VECTOR_INTRIN(_mm256_expm1_ps) \
VECTOR_INTRIN(_mm256_exp_pd) \
VECTOR_INTRIN(_mm256_exp_ps) \
VECTOR_INTRIN(_mm256_extract_epi16) \
VECTOR_INTRIN(_mm256_extract_epi32) \
VECTOR_INTRIN(_mm256_extract_epi64) \
VECTOR_INTRIN(_mm256_extractf128_pd) \
VECTOR_INTRIN(_mm256_extractf128_ps) \
VECTOR_INTRIN(_mm256_extractf128_si256) \
VECTOR_INTRIN(_mm256_extractf32x4_ps) \
VECTOR_INTRIN(_mm256_extractf64x2_pd) \
VECTOR_INTRIN(_mm256_extracti128_si256) \
VECTOR_INTRIN(_mm256_extracti32x4_epi32) \
VECTOR_INTRIN(_mm256_extracti64x2_epi64) \
VECTOR_INTRIN(_mm256_fixupimm_pd) \
VECTOR_INTRIN(_mm256_fixupimm_ps) \
VECTOR_INTRIN(_mm256_fmadd_pd) \
VECTOR_INTRIN(_mm256_fmadd_ps) \
VECTOR_INTRIN(_mm256_fmaddsub_pd) \
VECTOR_INTRIN(_mm256_fmaddsub_ps) \
VECTOR_INTRIN(_mm256_fmsubadd_pd) \
VECTOR_INTRIN(_mm256_fmsubadd_ps) \
VECTOR_INTRIN(_mm256_fmsub_pd) \
VECTOR_INTRIN(_mm256_fmsub_ps) \
VECTOR_INTRIN(_mm256_fnmadd_pd) \
VECTOR_INTRIN(_mm256_fnmadd_ps) \
VECTOR_INTRIN(_mm256_fnmsub_pd) \
VECTOR_INTRIN(_mm256_fnmsub_ps) \
VECTOR_INTRIN(_mm256_fpclass_pd_mask) \
VECTOR_INTRIN(_mm256_fpclass_ps_mask) \
VECTOR_INTRIN(_mm256_getexp_pd) \
VECTOR_INTRIN(_mm256_getexp_ps) \
VECTOR_INTRIN(_mm256_getmant_pd) \
VECTOR_INTRIN(_mm256_getmant_ps) \
VECTOR_INTRIN(_mm256_hadd_epi16) \
VECTOR_INTRIN(_mm256_hadd_epi32) \
VECTOR_INTRIN(_mm256_hadd_pd) \
VECTOR_INTRIN(_mm256_hadd_ps) \
VECTOR_INTRIN(_mm256_hadds_epi16) \
VECTOR_INTRIN(_mm256_hsub_epi16) \
VECTOR_INTRIN(_mm256_hsub_epi32) \
VECTOR_INTRIN(_mm256_hsub_pd) \
VECTOR_INTRIN(_mm256_hsub_ps) \
VECTOR_INTRIN(_mm256_hsubs_epi16) \
VECTOR_INTRIN(_mm256_hypot_pd) \
VECTOR_INTRIN(_mm256_hypot_ps) \
VECTOR_INTRIN(_mm256_i32gather_epi32) \
VECTOR_INTRIN(_mm256_i32gather_epi64) \
VECTOR_INTRIN(_mm256_i32gather_pd) \
VECTOR_INTRIN(_mm256_i32gather_ps) \
VECTOR_INTRIN(_mm256_i32scatter_epi32) \
VECTOR_INTRIN(_mm256_i32scatter_epi64) \
VECTOR_INTRIN(_mm256_i32scatter_pd) \
VECTOR_INTRIN(_mm256_i32scatter_ps) \
VECTOR_INTRIN(_mm256_i64gather_epi32) \
VECTOR_INTRIN(_mm256_i64gather_epi64) \
VECTOR_INTRIN(_mm256_i64gather_pd) \
VECTOR_INTRIN(_mm256_i64gather_ps) \
VECTOR_INTRIN(_mm256_i64scatter_epi32) \
VECTOR_INTRIN(_mm256_i64scatter_epi64) \
VECTOR_INTRIN(_mm256_i64scatter_pd) \
VECTOR_INTRIN(_mm256_i64scatter_ps) \
VECTOR_INTRIN(_mm256_idivrem_epi32) \
VECTOR_INTRIN(_mm256_insert_epi16) \
VECTOR_INTRIN(_mm256_insert_epi32) \
VECTOR_INTRIN(_mm256_insert_epi64) \
VECTOR_INTRIN(_mm256_insertf128_pd) \
VECTOR_INTRIN(_mm256_insertf128_ps) \
VECTOR_INTRIN(_mm256_insertf128_si256) \
VECTOR_INTRIN(_mm256_insertf32x4) \
VECTOR_INTRIN(_mm256_insertf64x2) \
VECTOR_INTRIN(_mm256_inserti128_si256) \
VECTOR_INTRIN(_mm256_inserti32x4) \
VECTOR_INTRIN(_mm256_inserti64x2) \
VECTOR_INTRIN(_mm256_invcbrt_pd) \
VECTOR_INTRIN(_mm256_invcbrt_ps) \
VECTOR_INTRIN(_mm256_invsqrt_pd) \
VECTOR_INTRIN(_mm256_invsqrt_ps) \
VECTOR_INTRIN(_mm256_lddqu_si256) \
VECTOR_INTRIN(_mm256_load_pd) \
VECTOR_INTRIN(_mm256_load_ps) \
VECTOR_INTRIN(_mm256_load_si256) \
VECTOR_INTRIN(_mm256_loadu_pd) \
VECTOR_INTRIN(_mm256_loadu_ps) \
VECTOR_INTRIN(_mm256_loadu_si256) \
VECTOR_INTRIN(_mm256_log10_pd) \
VECTOR_INTRIN(_mm256_log10_ps) \
VECTOR_INTRIN(_mm256_log1p_pd) \
VECTOR_INTRIN(_mm256_log1p_ps) \
VECTOR_INTRIN(_mm256_log2_pd) \
VECTOR_INTRIN(_mm256_log2_ps) \
VECTOR_INTRIN(_mm256_logb_pd) \
VECTOR_INTRIN(_mm256_logb_ps) \
VECTOR_INTRIN(_mm256_log_pd) \
VECTOR_INTRIN(_mm256_log_ps) \
VECTOR_INTRIN(_mm256_lzcnt_epi32) \
VECTOR_INTRIN(_mm256_lzcnt_epi64) \
VECTOR_INTRIN(_mm256_madd52hi_epu64) \
VECTOR_INTRIN(_mm256_madd52lo_epu64) \
VECTOR_INTRIN(_mm256_madd_epi16) \
VECTOR_INTRIN(_mm256_maddubs_epi16) \
VECTOR_INTRIN(_mm256_mask2_permutex2var_epi16) \
VECTOR_INTRIN(_mm256_mask2_permutex2var_epi32) \
VECTOR_INTRIN(_mm256_mask2_permutex2var_epi64) \
VECTOR_INTRIN(_mm256_mask2_permutex2var_epi8) \
VECTOR_INTRIN(_mm256_mask2_permutex2var_pd) \
VECTOR_INTRIN(_mm256_mask2_permutex2var_ps) \
VECTOR_INTRIN(_mm256_mask3_fmadd_pd) \
VECTOR_INTRIN(_mm256_mask3_fmadd_ps) \
VECTOR_INTRIN(_mm256_mask3_fmaddsub_pd) \
VECTOR_INTRIN(_mm256_mask3_fmaddsub_ps) \
VECTOR_INTRIN(_mm256_mask3_fmsubadd_pd) \
VECTOR_INTRIN(_mm256_mask3_fmsubadd_ps) \
VECTOR_INTRIN(_mm256_mask3_fmsub_pd) \
VECTOR_INTRIN(_mm256_mask3_fmsub_ps) \
VECTOR_INTRIN(_mm256_mask3_fnmadd_pd) \
VECTOR_INTRIN(_mm256_mask3_fnmadd_ps) \
VECTOR_INTRIN(_mm256_mask3_fnmsub_pd) \
VECTOR_INTRIN(_mm256_mask3_fnmsub_ps) \
VECTOR_INTRIN(_mm256_mask_abs_epi16) \
VECTOR_INTRIN(_mm256_mask_abs_epi32) \
VECTOR_INTRIN(_mm256_mask_abs_epi64) \
VECTOR_INTRIN(_mm256_mask_abs_epi8) \
VECTOR_INTRIN(_mm256_mask_add_epi16) \
VECTOR_INTRIN(_mm256_mask_add_epi32) \
VECTOR_INTRIN(_mm256_mask_add_epi64) \
VECTOR_INTRIN(_mm256_mask_add_epi8) \
VECTOR_INTRIN(_mm256_mask_add_pd) \
VECTOR_INTRIN(_mm256_mask_add_ps) \
VECTOR_INTRIN(_mm256_mask_adds_epi16) \
VECTOR_INTRIN(_mm256_mask_adds_epi8) \
VECTOR_INTRIN(_mm256_mask_adds_epu16) \
VECTOR_INTRIN(_mm256_mask_adds_epu8) \
VECTOR_INTRIN(_mm256_mask_alignr_epi32) \
VECTOR_INTRIN(_mm256_mask_alignr_epi64) \
VECTOR_INTRIN(_mm256_mask_alignr_epi8) \
VECTOR_INTRIN(_mm256_mask_and_epi32) \
VECTOR_INTRIN(_mm256_mask_and_epi64) \
VECTOR_INTRIN(_mm256_mask_andnot_epi32) \
VECTOR_INTRIN(_mm256_mask_andnot_epi64) \
VECTOR_INTRIN(_mm256_mask_andnot_pd) \
VECTOR_INTRIN(_mm256_mask_andnot_ps) \
VECTOR_INTRIN(_mm256_mask_and_pd) \
VECTOR_INTRIN(_mm256_mask_and_ps) \
VECTOR_INTRIN(_mm256_mask_avg_epu16) \
VECTOR_INTRIN(_mm256_mask_avg_epu8) \
VECTOR_INTRIN(_mm256_mask_blend_epi16) \
VECTOR_INTRIN(_mm256_mask_blend_epi32) \
VECTOR_INTRIN(_mm256_mask_blend_epi64) \
VECTOR_INTRIN(_mm256_mask_blend_epi8) \
VECTOR_INTRIN(_mm256_mask_blend_pd) \
VECTOR_INTRIN(_mm256_mask_blend_ps) \
VECTOR_INTRIN(_mm256_mask_broadcastb_epi8) \
VECTOR_INTRIN(_mm256_mask_broadcastd_epi32) \
VECTOR_INTRIN(_mm256_mask_broadcast_f32x2) \
VECTOR_INTRIN(_mm256_mask_broadcast_f32x4) \
VECTOR_INTRIN(_mm256_mask_broadcast_f64x2) \
VECTOR_INTRIN(_mm256_mask_broadcast_i32x2) \
VECTOR_INTRIN(_mm256_mask_broadcast_i32x4) \
VECTOR_INTRIN(_mm256_mask_broadcast_i64x2) \
VECTOR_INTRIN(_mm256_mask_broadcastq_epi64) \
VECTOR_INTRIN(_mm256_mask_broadcastsd_pd) \
VECTOR_INTRIN(_mm256_mask_broadcastss_ps) \
VECTOR_INTRIN(_mm256_mask_broadcastw_epi16) \
VECTOR_INTRIN(_mm256_mask_cmp_epi16_mask) \
VECTOR_INTRIN(_mm256_mask_cmp_epi32_mask) \
VECTOR_INTRIN(_mm256_mask_cmp_epi64_mask) \
VECTOR_INTRIN(_mm256_mask_cmp_epi8_mask) \
VECTOR_INTRIN(_mm256_mask_cmp_epu16_mask) \
VECTOR_INTRIN(_mm256_mask_cmp_epu32_mask) \
VECTOR_INTRIN(_mm256_mask_cmp_epu64_mask) \
VECTOR_INTRIN(_mm256_mask_cmp_epu8_mask) \
VECTOR_INTRIN(_mm256_mask_cmp_pd_mask) \
VECTOR_INTRIN(_mm256_mask_cmp_ps_mask) \
VECTOR_INTRIN(_mm256_mask_compress_epi32) \
VECTOR_INTRIN(_mm256_mask_compress_epi64) \
VECTOR_INTRIN(_mm256_mask_compress_pd) \
VECTOR_INTRIN(_mm256_mask_compress_ps) \
VECTOR_INTRIN(_mm256_mask_compressstoreu_epi32) \
VECTOR_INTRIN(_mm256_mask_compressstoreu_epi64) \
VECTOR_INTRIN(_mm256_mask_compressstoreu_pd) \
VECTOR_INTRIN(_mm256_mask_compressstoreu_ps) \
VECTOR_INTRIN(_mm256_mask_conflict_epi32) \
VECTOR_INTRIN(_mm256_mask_conflict_epi64) \
VECTOR_INTRIN(_mm256_mask_cvtepi16_epi32) \
VECTOR_INTRIN(_mm256_mask_cvtepi16_epi64) \
VECTOR_INTRIN(_mm256_mask_cvtepi16_epi8) \
VECTOR_INTRIN(_mm256_mask_cvtepi16_storeu_epi8) \
VECTOR_INTRIN(_mm256_mask_cvtepi32_epi16) \
VECTOR_INTRIN(_mm256_mask_cvtepi32_epi64) \
VECTOR_INTRIN(_mm256_mask_cvtepi32_epi8) \
VECTOR_INTRIN(_mm256_mask_cvtepi32_pd) \
VECTOR_INTRIN(_mm256_mask_cvtepi32_ps) \
VECTOR_INTRIN(_mm256_mask_cvtepi32_storeu_epi16) \
VECTOR_INTRIN(_mm256_mask_cvtepi32_storeu_epi8) \
VECTOR_INTRIN(_mm256_mask_cvtepi64_epi16) \
VECTOR_INTRIN(_mm256_mask_cvtepi64_epi32) \
VECTOR_INTRIN(_mm256_mask_cvtepi64_epi8) \
VECTOR_INTRIN(_mm256_mask_cvtepi64_pd) \
VECTOR_INTRIN(_mm256_mask_cvtepi64_ps) \
VECTOR_INTRIN(_mm256_mask_cvtepi64_storeu_epi16) \
VECTOR_INTRIN(_mm256_mask_cvtepi64_storeu_epi32) \
VECTOR_INTRIN(_mm256_mask_cvtepi64_storeu_epi8) \
VECTOR_INTRIN(_mm256_mask_cvtepi8_epi16) \
VECTOR_INTRIN(_mm256_mask_cvtepi8_epi32) \
VECTOR_INTRIN(_mm256_mask_cvtepi8_epi64) \
VECTOR_INTRIN(_mm256_mask_cvtepu16_epi32) \
VECTOR_INTRIN(_mm256_mask_cvtepu16_epi64) \
VECTOR_INTRIN(_mm256_mask_cvtepu32_epi64) \
VECTOR_INTRIN(_mm256_mask_cvtepu32_pd) \
VECTOR_INTRIN(_mm256_mask_cvtepu64_pd) \
VECTOR_INTRIN(_mm256_mask_cvtepu64_ps) \
VECTOR_INTRIN(_mm256_mask_cvtepu8_epi16) \
VECTOR_INTRIN(_mm256_mask_cvtepu8_epi32) \
VECTOR_INTRIN(_mm256_mask_cvtepu8_epi64) \
VECTOR_INTRIN(_mm256_mask_cvtpd_epi32) \
VECTOR_INTRIN(_mm256_mask_cvtpd_epi64) \
VECTOR_INTRIN(_mm256_mask_cvtpd_epu32) \
VECTOR_INTRIN(_mm256_mask_cvtpd_epu64) \
VECTOR_INTRIN(_mm256_mask_cvtpd_ps) \
VECTOR_INTRIN(_mm256_mask_cvtph_ps) \
VECTOR_INTRIN(_mm256_mask_cvtps_epi32) \
VECTOR_INTRIN(_mm256_mask_cvtps_epi64) \
VECTOR_INTRIN(_mm256_mask_cvtps_epu32) \
VECTOR_INTRIN(_mm256_mask_cvtps_epu64) \
VECTOR_INTRIN(_mm256_mask_cvt_roundps_ph) \
VECTOR_INTRIN(_mm256_mask_cvtsepi16_epi8) \
VECTOR_INTRIN(_mm256_mask_cvtsepi16_storeu_epi8) \
VECTOR_INTRIN(_mm256_mask_cvtsepi32_epi16) \
VECTOR_INTRIN(_mm256_mask_cvtsepi32_epi8) \
VECTOR_INTRIN(_mm256_mask_cvtsepi32_storeu_epi16) \
VECTOR_INTRIN(_mm256_mask_cvtsepi32_storeu_epi8) \
VECTOR_INTRIN(_mm256_mask_cvtsepi64_epi16) \
VECTOR_INTRIN(_mm256_mask_cvtsepi64_epi32) \
VECTOR_INTRIN(_mm256_mask_cvtsepi64_epi8) \
VECTOR_INTRIN(_mm256_mask_cvtsepi64_storeu_epi16) \
VECTOR_INTRIN(_mm256_mask_cvtsepi64_storeu_epi32) \
VECTOR_INTRIN(_mm256_mask_cvtsepi64_storeu_epi8) \
VECTOR_INTRIN(_mm256_mask_cvttpd_epi32) \
VECTOR_INTRIN(_mm256_mask_cvttpd_epi64) \
VECTOR_INTRIN(_mm256_mask_cvttpd_epu32) \
VECTOR_INTRIN(_mm256_mask_cvttpd_epu64) \
VECTOR_INTRIN(_mm256_mask_cvttps_epi32) \
VECTOR_INTRIN(_mm256_mask_cvttps_epi64) \
VECTOR_INTRIN(_mm256_mask_cvttps_epu32) \
VECTOR_INTRIN(_mm256_mask_cvttps_epu64) \
VECTOR_INTRIN(_mm256_mask_cvtusepi16_epi8) \
VECTOR_INTRIN(_mm256_mask_cvtusepi16_storeu_epi8) \
VECTOR_INTRIN(_mm256_mask_cvtusepi32_epi16) \
VECTOR_INTRIN(_mm256_mask_cvtusepi32_epi8) \
VECTOR_INTRIN(_mm256_mask_cvtusepi32_storeu_epi16) \
VECTOR_INTRIN(_mm256_mask_cvtusepi32_storeu_epi8) \
VECTOR_INTRIN(_mm256_mask_cvtusepi64_epi16) \
VECTOR_INTRIN(_mm256_mask_cvtusepi64_epi32) \
VECTOR_INTRIN(_mm256_mask_cvtusepi64_epi8) \
VECTOR_INTRIN(_mm256_mask_cvtusepi64_storeu_epi16) \
VECTOR_INTRIN(_mm256_mask_cvtusepi64_storeu_epi32) \
VECTOR_INTRIN(_mm256_mask_cvtusepi64_storeu_epi8) \
VECTOR_INTRIN(_mm256_mask_dbsad_epu8) \
VECTOR_INTRIN(_mm256_mask_div_pd) \
VECTOR_INTRIN(_mm256_mask_div_ps) \
VECTOR_INTRIN(_mm256_mask_expand_epi32) \
VECTOR_INTRIN(_mm256_mask_expand_epi64) \
VECTOR_INTRIN(_mm256_mask_expandloadu_epi32) \
VECTOR_INTRIN(_mm256_mask_expandloadu_epi64) \
VECTOR_INTRIN(_mm256_mask_expandloadu_pd) \
VECTOR_INTRIN(_mm256_mask_expandloadu_ps) \
VECTOR_INTRIN(_mm256_mask_expand_pd) \
VECTOR_INTRIN(_mm256_mask_expand_ps) \
VECTOR_INTRIN(_mm256_mask_extractf32x4_ps) \
VECTOR_INTRIN(_mm256_mask_extractf64x2_pd) \
VECTOR_INTRIN(_mm256_mask_extracti32x4_epi32) \
VECTOR_INTRIN(_mm256_mask_extracti64x2_epi64) \
VECTOR_INTRIN(_mm256_mask_fixupimm_pd) \
VECTOR_INTRIN(_mm256_mask_fixupimm_ps) \
VECTOR_INTRIN(_mm256_mask_fmadd_pd) \
VECTOR_INTRIN(_mm256_mask_fmadd_ps) \
VECTOR_INTRIN(_mm256_mask_fmaddsub_pd) \
VECTOR_INTRIN(_mm256_mask_fmaddsub_ps) \
VECTOR_INTRIN(_mm256_mask_fmsubadd_pd) \
VECTOR_INTRIN(_mm256_mask_fmsubadd_ps) \
VECTOR_INTRIN(_mm256_mask_fmsub_pd) \
VECTOR_INTRIN(_mm256_mask_fmsub_ps) \
VECTOR_INTRIN(_mm256_mask_fnmadd_pd) \
VECTOR_INTRIN(_mm256_mask_fnmadd_ps) \
VECTOR_INTRIN(_mm256_mask_fnmsub_pd) \
VECTOR_INTRIN(_mm256_mask_fnmsub_ps) \
VECTOR_INTRIN(_mm256_mask_fpclass_pd_mask) \
VECTOR_INTRIN(_mm256_mask_fpclass_ps_mask) \
VECTOR_INTRIN(_mm256_mask_getexp_pd) \
VECTOR_INTRIN(_mm256_mask_getexp_ps) \
VECTOR_INTRIN(_mm256_mask_getmant_pd) \
VECTOR_INTRIN(_mm256_mask_getmant_ps) \
VECTOR_INTRIN(_mm256_mask_i32gather_epi32) \
VECTOR_INTRIN(_mm256_mask_i32gather_epi64) \
VECTOR_INTRIN(_mm256_mask_i32gather_pd) \
VECTOR_INTRIN(_mm256_mask_i32gather_ps) \
VECTOR_INTRIN(_mm256_mask_i32scatter_epi32) \
VECTOR_INTRIN(_mm256_mask_i32scatter_epi64) \
VECTOR_INTRIN(_mm256_mask_i32scatter_pd) \
VECTOR_INTRIN(_mm256_mask_i32scatter_ps) \
VECTOR_INTRIN(_mm256_mask_i64gather_epi32) \
VECTOR_INTRIN(_mm256_mask_i64gather_epi64) \
VECTOR_INTRIN(_mm256_mask_i64gather_pd) \
VECTOR_INTRIN(_mm256_mask_i64gather_ps) \
VECTOR_INTRIN(_mm256_mask_i64scatter_epi32) \
VECTOR_INTRIN(_mm256_mask_i64scatter_epi64) \
VECTOR_INTRIN(_mm256_mask_i64scatter_pd) \
VECTOR_INTRIN(_mm256_mask_i64scatter_ps) \
VECTOR_INTRIN(_mm256_mask_insertf32x4) \
VECTOR_INTRIN(_mm256_mask_insertf64x2) \
VECTOR_INTRIN(_mm256_mask_inserti32x4) \
VECTOR_INTRIN(_mm256_mask_inserti64x2) \
VECTOR_INTRIN(_mm256_mask_load_epi32) \
VECTOR_INTRIN(_mm256_maskload_epi32) \
VECTOR_INTRIN(_mm256_mask_load_epi64) \
VECTOR_INTRIN(_mm256_maskload_epi64) \
VECTOR_INTRIN(_mm256_mask_load_pd) \
VECTOR_INTRIN(_mm256_maskload_pd) \
VECTOR_INTRIN(_mm256_mask_load_ps) \
VECTOR_INTRIN(_mm256_maskload_ps) \
VECTOR_INTRIN(_mm256_mask_loadu_epi16) \
VECTOR_INTRIN(_mm256_mask_loadu_epi32) \
VECTOR_INTRIN(_mm256_mask_loadu_epi64) \
VECTOR_INTRIN(_mm256_mask_loadu_epi8) \
VECTOR_INTRIN(_mm256_mask_loadu_pd) \
VECTOR_INTRIN(_mm256_mask_loadu_ps) \
VECTOR_INTRIN(_mm256_mask_lzcnt_epi32) \
VECTOR_INTRIN(_mm256_mask_lzcnt_epi64) \
VECTOR_INTRIN(_mm256_mask_madd52hi_epu64) \
VECTOR_INTRIN(_mm256_mask_madd52lo_epu64) \
VECTOR_INTRIN(_mm256_mask_madd_epi16) \
VECTOR_INTRIN(_mm256_mask_maddubs_epi16) \
VECTOR_INTRIN(_mm256_mask_max_epi16) \
VECTOR_INTRIN(_mm256_mask_max_epi32) \
VECTOR_INTRIN(_mm256_mask_max_epi64) \
VECTOR_INTRIN(_mm256_mask_max_epi8) \
VECTOR_INTRIN(_mm256_mask_max_epu16) \
VECTOR_INTRIN(_mm256_mask_max_epu32) \
VECTOR_INTRIN(_mm256_mask_max_epu64) \
VECTOR_INTRIN(_mm256_mask_max_epu8) \
VECTOR_INTRIN(_mm256_mask_max_pd) \
VECTOR_INTRIN(_mm256_mask_max_ps) \
VECTOR_INTRIN(_mm256_mask_min_epi16) \
VECTOR_INTRIN(_mm256_mask_min_epi32) \
VECTOR_INTRIN(_mm256_mask_min_epi64) \
VECTOR_INTRIN(_mm256_mask_min_epi8) \
VECTOR_INTRIN(_mm256_mask_min_epu16) \
VECTOR_INTRIN(_mm256_mask_min_epu32) \
VECTOR_INTRIN(_mm256_mask_min_epu64) \
VECTOR_INTRIN(_mm256_mask_min_epu8) \
VECTOR_INTRIN(_mm256_mask_min_pd) \
VECTOR_INTRIN(_mm256_mask_min_ps) \
VECTOR_INTRIN(_mm256_mask_movedup_pd) \
VECTOR_INTRIN(_mm256_mask_movehdup_ps) \
VECTOR_INTRIN(_mm256_mask_moveldup_ps) \
VECTOR_INTRIN(_mm256_mask_mov_epi16) \
VECTOR_INTRIN(_mm256_mask_mov_epi32) \
VECTOR_INTRIN(_mm256_mask_mov_epi64) \
VECTOR_INTRIN(_mm256_mask_mov_epi8) \
VECTOR_INTRIN(_mm256_mask_mov_pd) \
VECTOR_INTRIN(_mm256_mask_mov_ps) \
VECTOR_INTRIN(_mm256_mask_mul_epi32) \
VECTOR_INTRIN(_mm256_mask_mul_epu32) \
VECTOR_INTRIN(_mm256_mask_mulhi_epi16) \
VECTOR_INTRIN(_mm256_mask_mulhi_epu16) \
VECTOR_INTRIN(_mm256_mask_mulhrs_epi16) \
VECTOR_INTRIN(_mm256_mask_mullo_epi16) \
VECTOR_INTRIN(_mm256_mask_mullo_epi32) \
VECTOR_INTRIN(_mm256_mask_mullo_epi64) \
VECTOR_INTRIN(_mm256_mask_mul_pd) \
VECTOR_INTRIN(_mm256_mask_mul_ps) \
VECTOR_INTRIN(_mm256_mask_multishift_epi64_epi8) \
VECTOR_INTRIN(_mm256_mask_or_epi32) \
VECTOR_INTRIN(_mm256_mask_or_epi64) \
VECTOR_INTRIN(_mm256_mask_or_pd) \
VECTOR_INTRIN(_mm256_mask_or_ps) \
VECTOR_INTRIN(_mm256_mask_packs_epi16) \
VECTOR_INTRIN(_mm256_mask_packs_epi32) \
VECTOR_INTRIN(_mm256_mask_packus_epi16) \
VECTOR_INTRIN(_mm256_mask_packus_epi32) \
VECTOR_INTRIN(_mm256_mask_permute_pd) \
VECTOR_INTRIN(_mm256_mask_permute_ps) \
VECTOR_INTRIN(_mm256_mask_permutevar_pd) \
VECTOR_INTRIN(_mm256_mask_permutevar_ps) \
VECTOR_INTRIN(_mm256_mask_permutex2var_epi16) \
VECTOR_INTRIN(_mm256_mask_permutex2var_epi32) \
VECTOR_INTRIN(_mm256_mask_permutex2var_epi64) \
VECTOR_INTRIN(_mm256_mask_permutex2var_epi8) \
VECTOR_INTRIN(_mm256_mask_permutex2var_pd) \
VECTOR_INTRIN(_mm256_mask_permutex2var_ps) \
VECTOR_INTRIN(_mm256_mask_permutex_epi64) \
VECTOR_INTRIN(_mm256_mask_permutex_pd) \
VECTOR_INTRIN(_mm256_mask_permutexvar_epi16) \
VECTOR_INTRIN(_mm256_mask_permutexvar_epi32) \
VECTOR_INTRIN(_mm256_mask_permutexvar_epi64) \
VECTOR_INTRIN(_mm256_mask_permutexvar_epi8) \
VECTOR_INTRIN(_mm256_mask_permutexvar_pd) \
VECTOR_INTRIN(_mm256_mask_permutexvar_ps) \
VECTOR_INTRIN(_mm256_mask_range_pd) \
VECTOR_INTRIN(_mm256_mask_range_ps) \
VECTOR_INTRIN(_mm256_mask_rcp14_pd) \
VECTOR_INTRIN(_mm256_mask_rcp14_ps) \
VECTOR_INTRIN(_mm256_mask_reduce_pd) \
VECTOR_INTRIN(_mm256_mask_reduce_ps) \
VECTOR_INTRIN(_mm256_mask_rol_epi32) \
VECTOR_INTRIN(_mm256_mask_rol_epi64) \
VECTOR_INTRIN(_mm256_mask_rolv_epi32) \
VECTOR_INTRIN(_mm256_mask_rolv_epi64) \
VECTOR_INTRIN(_mm256_mask_ror_epi32) \
VECTOR_INTRIN(_mm256_mask_ror_epi64) \
VECTOR_INTRIN(_mm256_mask_rorv_epi32) \
VECTOR_INTRIN(_mm256_mask_rorv_epi64) \
VECTOR_INTRIN(_mm256_mask_roundscale_pd) \
VECTOR_INTRIN(_mm256_mask_roundscale_ps) \
VECTOR_INTRIN(_mm256_mask_rsqrt14_pd) \
VECTOR_INTRIN(_mm256_mask_rsqrt14_ps) \
VECTOR_INTRIN(_mm256_mask_scalef_pd) \
VECTOR_INTRIN(_mm256_mask_scalef_ps) \
VECTOR_INTRIN(_mm256_mask_set1_epi16) \
VECTOR_INTRIN(_mm256_mask_set1_epi32) \
VECTOR_INTRIN(_mm256_mask_set1_epi64) \
VECTOR_INTRIN(_mm256_mask_set1_epi8) \
VECTOR_INTRIN(_mm256_mask_shuffle_epi32) \
VECTOR_INTRIN(_mm256_mask_shuffle_epi8) \
VECTOR_INTRIN(_mm256_mask_shuffle_f32x4) \
VECTOR_INTRIN(_mm256_mask_shuffle_f64x2) \
VECTOR_INTRIN(_mm256_mask_shufflehi_epi16) \
VECTOR_INTRIN(_mm256_mask_shuffle_i32x4) \
VECTOR_INTRIN(_mm256_mask_shuffle_i64x2) \
VECTOR_INTRIN(_mm256_mask_shufflelo_epi16) \
VECTOR_INTRIN(_mm256_mask_shuffle_pd) \
VECTOR_INTRIN(_mm256_mask_shuffle_ps) \
VECTOR_INTRIN(_mm256_mask_sll_epi16) \
VECTOR_INTRIN(_mm256_mask_sll_epi32) \
VECTOR_INTRIN(_mm256_mask_sll_epi64) \
VECTOR_INTRIN(_mm256_mask_slli_epi16) \
VECTOR_INTRIN(_mm256_mask_slli_epi32) \
VECTOR_INTRIN(_mm256_mask_slli_epi64) \
VECTOR_INTRIN(_mm256_mask_sllv_epi16) \
VECTOR_INTRIN(_mm256_mask_sllv_epi32) \
VECTOR_INTRIN(_mm256_mask_sllv_epi64) \
VECTOR_INTRIN(_mm256_mask_sqrt_pd) \
VECTOR_INTRIN(_mm256_mask_sqrt_ps) \
VECTOR_INTRIN(_mm256_mask_sra_epi16) \
VECTOR_INTRIN(_mm256_mask_sra_epi32) \
VECTOR_INTRIN(_mm256_mask_sra_epi64) \
VECTOR_INTRIN(_mm256_mask_srai_epi16) \
VECTOR_INTRIN(_mm256_mask_srai_epi32) \
VECTOR_INTRIN(_mm256_mask_srai_epi64) \
VECTOR_INTRIN(_mm256_mask_srav_epi16) \
VECTOR_INTRIN(_mm256_mask_srav_epi32) \
VECTOR_INTRIN(_mm256_mask_srav_epi64) \
VECTOR_INTRIN(_mm256_mask_srl_epi16) \
VECTOR_INTRIN(_mm256_mask_srl_epi32) \
VECTOR_INTRIN(_mm256_mask_srl_epi64) \
VECTOR_INTRIN(_mm256_mask_srli_epi16) \
VECTOR_INTRIN(_mm256_mask_srli_epi32) \
VECTOR_INTRIN(_mm256_mask_srli_epi64) \
VECTOR_INTRIN(_mm256_mask_srlv_epi16) \
VECTOR_INTRIN(_mm256_mask_srlv_epi32) \
VECTOR_INTRIN(_mm256_mask_srlv_epi64) \
VECTOR_INTRIN(_mm256_mask_store_epi32) \
VECTOR_INTRIN(_mm256_maskstore_epi32) \
VECTOR_INTRIN(_mm256_mask_store_epi64) \
VECTOR_INTRIN(_mm256_maskstore_epi64) \
VECTOR_INTRIN(_mm256_mask_store_pd) \
VECTOR_INTRIN(_mm256_maskstore_pd) \
VECTOR_INTRIN(_mm256_mask_store_ps) \
VECTOR_INTRIN(_mm256_maskstore_ps) \
VECTOR_INTRIN(_mm256_mask_storeu_epi16) \
VECTOR_INTRIN(_mm256_mask_storeu_epi32) \
VECTOR_INTRIN(_mm256_mask_storeu_epi64) \
VECTOR_INTRIN(_mm256_mask_storeu_epi8) \
VECTOR_INTRIN(_mm256_mask_storeu_pd) \
VECTOR_INTRIN(_mm256_mask_storeu_ps) \
VECTOR_INTRIN(_mm256_mask_sub_epi16) \
VECTOR_INTRIN(_mm256_mask_sub_epi32) \
VECTOR_INTRIN(_mm256_mask_sub_epi64) \
VECTOR_INTRIN(_mm256_mask_sub_epi8) \
VECTOR_INTRIN(_mm256_mask_sub_pd) \
VECTOR_INTRIN(_mm256_mask_sub_ps) \
VECTOR_INTRIN(_mm256_mask_subs_epi16) \
VECTOR_INTRIN(_mm256_mask_subs_epi8) \
VECTOR_INTRIN(_mm256_mask_subs_epu16) \
VECTOR_INTRIN(_mm256_mask_subs_epu8) \
VECTOR_INTRIN(_mm256_mask_ternarylogic_epi32) \
VECTOR_INTRIN(_mm256_mask_ternarylogic_epi64) \
VECTOR_INTRIN(_mm256_mask_test_epi16_mask) \
VECTOR_INTRIN(_mm256_mask_test_epi32_mask) \
VECTOR_INTRIN(_mm256_mask_test_epi64_mask) \
VECTOR_INTRIN(_mm256_mask_test_epi8_mask) \
VECTOR_INTRIN(_mm256_mask_testn_epi16_mask) \
VECTOR_INTRIN(_mm256_mask_testn_epi32_mask) \
VECTOR_INTRIN(_mm256_mask_testn_epi64_mask) \
VECTOR_INTRIN(_mm256_mask_testn_epi8_mask) \
VECTOR_INTRIN(_mm256_mask_unpackhi_epi16) \
VECTOR_INTRIN(_mm256_mask_unpackhi_epi32) \
VECTOR_INTRIN(_mm256_mask_unpackhi_epi64) \
VECTOR_INTRIN(_mm256_mask_unpackhi_epi8) \
VECTOR_INTRIN(_mm256_mask_unpackhi_pd) \
VECTOR_INTRIN(_mm256_mask_unpackhi_ps) \
VECTOR_INTRIN(_mm256_mask_unpacklo_epi16) \
VECTOR_INTRIN(_mm256_mask_unpacklo_epi32) \
VECTOR_INTRIN(_mm256_mask_unpacklo_epi64) \
VECTOR_INTRIN(_mm256_mask_unpacklo_epi8) \
VECTOR_INTRIN(_mm256_mask_unpacklo_pd) \
VECTOR_INTRIN(_mm256_mask_unpacklo_ps) \
VECTOR_INTRIN(_mm256_mask_xor_epi32) \
VECTOR_INTRIN(_mm256_mask_xor_epi64) \
VECTOR_INTRIN(_mm256_mask_xor_pd) \
VECTOR_INTRIN(_mm256_mask_xor_ps) \
VECTOR_INTRIN(_mm256_maskz_abs_epi16) \
VECTOR_INTRIN(_mm256_maskz_abs_epi32) \
VECTOR_INTRIN(_mm256_maskz_abs_epi64) \
VECTOR_INTRIN(_mm256_maskz_abs_epi8) \
VECTOR_INTRIN(_mm256_maskz_add_epi16) \
VECTOR_INTRIN(_mm256_maskz_add_epi32) \
VECTOR_INTRIN(_mm256_maskz_add_epi64) \
VECTOR_INTRIN(_mm256_maskz_add_epi8) \
VECTOR_INTRIN(_mm256_maskz_add_pd) \
VECTOR_INTRIN(_mm256_maskz_add_ps) \
VECTOR_INTRIN(_mm256_maskz_adds_epi16) \
VECTOR_INTRIN(_mm256_maskz_adds_epi8) \
VECTOR_INTRIN(_mm256_maskz_adds_epu16) \
VECTOR_INTRIN(_mm256_maskz_adds_epu8) \
VECTOR_INTRIN(_mm256_maskz_alignr_epi32) \
VECTOR_INTRIN(_mm256_maskz_alignr_epi64) \
VECTOR_INTRIN(_mm256_maskz_alignr_epi8) \
VECTOR_INTRIN(_mm256_maskz_and_epi32) \
VECTOR_INTRIN(_mm256_maskz_and_epi64) \
VECTOR_INTRIN(_mm256_maskz_andnot_epi32) \
VECTOR_INTRIN(_mm256_maskz_andnot_epi64) \
VECTOR_INTRIN(_mm256_maskz_andnot_pd) \
VECTOR_INTRIN(_mm256_maskz_andnot_ps) \
VECTOR_INTRIN(_mm256_maskz_and_pd) \
VECTOR_INTRIN(_mm256_maskz_and_ps) \
VECTOR_INTRIN(_mm256_maskz_avg_epu16) \
VECTOR_INTRIN(_mm256_maskz_avg_epu8) \
VECTOR_INTRIN(_mm256_maskz_broadcastb_epi8) \
VECTOR_INTRIN(_mm256_maskz_broadcastd_epi32) \
VECTOR_INTRIN(_mm256_maskz_broadcast_f32x2) \
VECTOR_INTRIN(_mm256_maskz_broadcast_f32x4) \
VECTOR_INTRIN(_mm256_maskz_broadcast_f64x2) \
VECTOR_INTRIN(_mm256_maskz_broadcast_i32x2) \
VECTOR_INTRIN(_mm256_maskz_broadcast_i32x4) \
VECTOR_INTRIN(_mm256_maskz_broadcast_i64x2) \
VECTOR_INTRIN(_mm256_maskz_broadcastq_epi64) \
VECTOR_INTRIN(_mm256_maskz_broadcastsd_pd) \
VECTOR_INTRIN(_mm256_maskz_broadcastss_ps) \
VECTOR_INTRIN(_mm256_maskz_broadcastw_epi16) \
VECTOR_INTRIN(_mm256_maskz_compress_epi32) \
VECTOR_INTRIN(_mm256_maskz_compress_epi64) \
VECTOR_INTRIN(_mm256_maskz_compress_pd) \
VECTOR_INTRIN(_mm256_maskz_compress_ps) \
VECTOR_INTRIN(_mm256_maskz_conflict_epi32) \
VECTOR_INTRIN(_mm256_maskz_conflict_epi64) \
VECTOR_INTRIN(_mm256_maskz_cvtepi16_epi32) \
VECTOR_INTRIN(_mm256_maskz_cvtepi16_epi64) \
VECTOR_INTRIN(_mm256_maskz_cvtepi16_epi8) \
VECTOR_INTRIN(_mm256_maskz_cvtepi32_epi16) \
VECTOR_INTRIN(_mm256_maskz_cvtepi32_epi64) \
VECTOR_INTRIN(_mm256_maskz_cvtepi32_epi8) \
VECTOR_INTRIN(_mm256_maskz_cvtepi32_pd) \
VECTOR_INTRIN(_mm256_maskz_cvtepi32_ps) \
VECTOR_INTRIN(_mm256_maskz_cvtepi64_epi16) \
VECTOR_INTRIN(_mm256_maskz_cvtepi64_epi32) \
VECTOR_INTRIN(_mm256_maskz_cvtepi64_epi8) \
VECTOR_INTRIN(_mm256_maskz_cvtepi64_pd) \
VECTOR_INTRIN(_mm256_maskz_cvtepi64_ps) \
VECTOR_INTRIN(_mm256_maskz_cvtepi8_epi16) \
VECTOR_INTRIN(_mm256_maskz_cvtepi8_epi32) \
VECTOR_INTRIN(_mm256_maskz_cvtepi8_epi64) \
VECTOR_INTRIN(_mm256_maskz_cvtepu16_epi32) \
VECTOR_INTRIN(_mm256_maskz_cvtepu16_epi64) \
VECTOR_INTRIN(_mm256_maskz_cvtepu32_epi64) \
VECTOR_INTRIN(_mm256_maskz_cvtepu32_pd) \
VECTOR_INTRIN(_mm256_maskz_cvtepu64_pd) \
VECTOR_INTRIN(_mm256_maskz_cvtepu64_ps) \
VECTOR_INTRIN(_mm256_maskz_cvtepu8_epi16) \
VECTOR_INTRIN(_mm256_maskz_cvtepu8_epi32) \
VECTOR_INTRIN(_mm256_maskz_cvtepu8_epi64) \
VECTOR_INTRIN(_mm256_maskz_cvtpd_epi32) \
VECTOR_INTRIN(_mm256_maskz_cvtpd_epi64) \
VECTOR_INTRIN(_mm256_maskz_cvtpd_epu32) \
VECTOR_INTRIN(_mm256_maskz_cvtpd_epu64) \
VECTOR_INTRIN(_mm256_maskz_cvtpd_ps) \
VECTOR_INTRIN(_mm256_maskz_cvtph_ps) \
VECTOR_INTRIN(_mm256_maskz_cvtps_epi32) \
VECTOR_INTRIN(_mm256_maskz_cvtps_epi64) \
VECTOR_INTRIN(_mm256_maskz_cvtps_epu32) \
VECTOR_INTRIN(_mm256_maskz_cvtps_epu64) \
VECTOR_INTRIN(_mm256_maskz_cvt_roundps_ph) \
VECTOR_INTRIN(_mm256_maskz_cvtsepi16_epi8) \
VECTOR_INTRIN(_mm256_maskz_cvtsepi32_epi16) \
VECTOR_INTRIN(_mm256_maskz_cvtsepi32_epi8) \
VECTOR_INTRIN(_mm256_maskz_cvtsepi64_epi16) \
VECTOR_INTRIN(_mm256_maskz_cvtsepi64_epi32) \
VECTOR_INTRIN(_mm256_maskz_cvtsepi64_epi8) \
VECTOR_INTRIN(_mm256_maskz_cvttpd_epi32) \
VECTOR_INTRIN(_mm256_maskz_cvttpd_epi64) \
VECTOR_INTRIN(_mm256_maskz_cvttpd_epu32) \
VECTOR_INTRIN(_mm256_maskz_cvttpd_epu64) \
VECTOR_INTRIN(_mm256_maskz_cvttps_epi32) \
VECTOR_INTRIN(_mm256_maskz_cvttps_epi64) \
VECTOR_INTRIN(_mm256_maskz_cvttps_epu32) \
VECTOR_INTRIN(_mm256_maskz_cvttps_epu64) \
VECTOR_INTRIN(_mm256_maskz_cvtusepi16_epi8) \
VECTOR_INTRIN(_mm256_maskz_cvtusepi32_epi16) \
VECTOR_INTRIN(_mm256_maskz_cvtusepi32_epi8) \
VECTOR_INTRIN(_mm256_maskz_cvtusepi64_epi16) \
VECTOR_INTRIN(_mm256_maskz_cvtusepi64_epi32) \
VECTOR_INTRIN(_mm256_maskz_cvtusepi64_epi8) \
VECTOR_INTRIN(_mm256_maskz_dbsad_epu8) \
VECTOR_INTRIN(_mm256_maskz_div_pd) \
VECTOR_INTRIN(_mm256_maskz_div_ps) \
VECTOR_INTRIN(_mm256_maskz_expand_epi32) \
VECTOR_INTRIN(_mm256_maskz_expand_epi64) \
VECTOR_INTRIN(_mm256_maskz_expandloadu_epi32) \
VECTOR_INTRIN(_mm256_maskz_expandloadu_epi64) \
VECTOR_INTRIN(_mm256_maskz_expandloadu_pd) \
VECTOR_INTRIN(_mm256_maskz_expandloadu_ps) \
VECTOR_INTRIN(_mm256_maskz_expand_pd) \
VECTOR_INTRIN(_mm256_maskz_expand_ps) \
VECTOR_INTRIN(_mm256_maskz_extractf32x4_ps) \
VECTOR_INTRIN(_mm256_maskz_extractf64x2_pd) \
VECTOR_INTRIN(_mm256_maskz_extracti32x4_epi32) \
VECTOR_INTRIN(_mm256_maskz_extracti64x2_epi64) \
VECTOR_INTRIN(_mm256_maskz_fixupimm_pd) \
VECTOR_INTRIN(_mm256_maskz_fixupimm_ps) \
VECTOR_INTRIN(_mm256_maskz_fmadd_pd) \
VECTOR_INTRIN(_mm256_maskz_fmadd_ps) \
VECTOR_INTRIN(_mm256_maskz_fmaddsub_pd) \
VECTOR_INTRIN(_mm256_maskz_fmaddsub_ps) \
VECTOR_INTRIN(_mm256_maskz_fmsubadd_pd) \
VECTOR_INTRIN(_mm256_maskz_fmsubadd_ps) \
VECTOR_INTRIN(_mm256_maskz_fmsub_pd) \
VECTOR_INTRIN(_mm256_maskz_fmsub_ps) \
VECTOR_INTRIN(_mm256_maskz_fnmadd_pd) \
VECTOR_INTRIN(_mm256_maskz_fnmadd_ps) \
VECTOR_INTRIN(_mm256_maskz_fnmsub_pd) \
VECTOR_INTRIN(_mm256_maskz_fnmsub_ps) \
VECTOR_INTRIN(_mm256_maskz_getexp_pd) \
VECTOR_INTRIN(_mm256_maskz_getexp_ps) \
VECTOR_INTRIN(_mm256_maskz_getmant_pd) \
VECTOR_INTRIN(_mm256_maskz_getmant_ps) \
VECTOR_INTRIN(_mm256_maskz_insertf32x4) \
VECTOR_INTRIN(_mm256_maskz_insertf64x2) \
VECTOR_INTRIN(_mm256_maskz_inserti32x4) \
VECTOR_INTRIN(_mm256_maskz_inserti64x2) \
VECTOR_INTRIN(_mm256_maskz_load_epi32) \
VECTOR_INTRIN(_mm256_maskz_load_epi64) \
VECTOR_INTRIN(_mm256_maskz_load_pd) \
VECTOR_INTRIN(_mm256_maskz_load_ps) \
VECTOR_INTRIN(_mm256_maskz_loadu_epi16) \
VECTOR_INTRIN(_mm256_maskz_loadu_epi32) \
VECTOR_INTRIN(_mm256_maskz_loadu_epi64) \
VECTOR_INTRIN(_mm256_maskz_loadu_epi8) \
VECTOR_INTRIN(_mm256_maskz_loadu_pd) \
VECTOR_INTRIN(_mm256_maskz_loadu_ps) \
VECTOR_INTRIN(_mm256_maskz_lzcnt_epi32) \
VECTOR_INTRIN(_mm256_maskz_lzcnt_epi64) \
VECTOR_INTRIN(_mm256_maskz_madd52hi_epu64) \
VECTOR_INTRIN(_mm256_maskz_madd52lo_epu64) \
VECTOR_INTRIN(_mm256_maskz_madd_epi16) \
VECTOR_INTRIN(_mm256_maskz_maddubs_epi16) \
VECTOR_INTRIN(_mm256_maskz_max_epi16) \
VECTOR_INTRIN(_mm256_maskz_max_epi32) \
VECTOR_INTRIN(_mm256_maskz_max_epi64) \
VECTOR_INTRIN(_mm256_maskz_max_epi8) \
VECTOR_INTRIN(_mm256_maskz_max_epu16) \
VECTOR_INTRIN(_mm256_maskz_max_epu32) \
VECTOR_INTRIN(_mm256_maskz_max_epu64) \
VECTOR_INTRIN(_mm256_maskz_max_epu8) \
VECTOR_INTRIN(_mm256_maskz_max_pd) \
VECTOR_INTRIN(_mm256_maskz_max_ps) \
VECTOR_INTRIN(_mm256_maskz_min_epi16) \
VECTOR_INTRIN(_mm256_maskz_min_epi32) \
VECTOR_INTRIN(_mm256_maskz_min_epi64) \
VECTOR_INTRIN(_mm256_maskz_min_epi8) \
VECTOR_INTRIN(_mm256_maskz_min_epu16) \
VECTOR_INTRIN(_mm256_maskz_min_epu32) \
VECTOR_INTRIN(_mm256_maskz_min_epu64) \
VECTOR_INTRIN(_mm256_maskz_min_epu8) \
VECTOR_INTRIN(_mm256_maskz_min_pd) \
VECTOR_INTRIN(_mm256_maskz_min_ps) \
VECTOR_INTRIN(_mm256_maskz_movedup_pd) \
VECTOR_INTRIN(_mm256_maskz_movehdup_ps) \
VECTOR_INTRIN(_mm256_maskz_moveldup_ps) \
VECTOR_INTRIN(_mm256_maskz_mov_epi16) \
VECTOR_INTRIN(_mm256_maskz_mov_epi32) \
VECTOR_INTRIN(_mm256_maskz_mov_epi64) \
VECTOR_INTRIN(_mm256_maskz_mov_epi8) \
VECTOR_INTRIN(_mm256_maskz_mov_pd) \
VECTOR_INTRIN(_mm256_maskz_mov_ps) \
VECTOR_INTRIN(_mm256_maskz_mul_epi32) \
VECTOR_INTRIN(_mm256_maskz_mul_epu32) \
VECTOR_INTRIN(_mm256_maskz_mulhi_epi16) \
VECTOR_INTRIN(_mm256_maskz_mulhi_epu16) \
VECTOR_INTRIN(_mm256_maskz_mulhrs_epi16) \
VECTOR_INTRIN(_mm256_maskz_mullo_epi16) \
VECTOR_INTRIN(_mm256_maskz_mullo_epi32) \
VECTOR_INTRIN(_mm256_maskz_mullo_epi64) \
VECTOR_INTRIN(_mm256_maskz_mul_pd) \
VECTOR_INTRIN(_mm256_maskz_mul_ps) \
VECTOR_INTRIN(_mm256_maskz_multishift_epi64_epi8) \
VECTOR_INTRIN(_mm256_maskz_or_epi32) \
VECTOR_INTRIN(_mm256_maskz_or_epi64) \
VECTOR_INTRIN(_mm256_maskz_or_pd) \
VECTOR_INTRIN(_mm256_maskz_or_ps) \
VECTOR_INTRIN(_mm256_maskz_packs_epi16) \
VECTOR_INTRIN(_mm256_maskz_packs_epi32) \
VECTOR_INTRIN(_mm256_maskz_packus_epi16) \
VECTOR_INTRIN(_mm256_maskz_packus_epi32) \
VECTOR_INTRIN(_mm256_maskz_permute_pd) \
VECTOR_INTRIN(_mm256_maskz_permute_ps) \
VECTOR_INTRIN(_mm256_maskz_permutevar_pd) \
VECTOR_INTRIN(_mm256_maskz_permutevar_ps) \
VECTOR_INTRIN(_mm256_maskz_permutex2var_epi16) \
VECTOR_INTRIN(_mm256_maskz_permutex2var_epi32) \
VECTOR_INTRIN(_mm256_maskz_permutex2var_epi64) \
VECTOR_INTRIN(_mm256_maskz_permutex2var_epi8) \
VECTOR_INTRIN(_mm256_maskz_permutex2var_pd) \
VECTOR_INTRIN(_mm256_maskz_permutex2var_ps) \
VECTOR_INTRIN(_mm256_maskz_permutex_epi64) \
VECTOR_INTRIN(_mm256_maskz_permutex_pd) \
VECTOR_INTRIN(_mm256_maskz_permutexvar_epi16) \
VECTOR_INTRIN(_mm256_maskz_permutexvar_epi32) \
VECTOR_INTRIN(_mm256_maskz_permutexvar_epi64) \
VECTOR_INTRIN(_mm256_maskz_permutexvar_epi8) \
VECTOR_INTRIN(_mm256_maskz_permutexvar_pd) \
VECTOR_INTRIN(_mm256_maskz_permutexvar_ps) \
VECTOR_INTRIN(_mm256_maskz_range_pd) \
VECTOR_INTRIN(_mm256_maskz_range_ps) \
VECTOR_INTRIN(_mm256_maskz_rcp14_pd) \
VECTOR_INTRIN(_mm256_maskz_rcp14_ps) \
VECTOR_INTRIN(_mm256_maskz_reduce_pd) \
VECTOR_INTRIN(_mm256_maskz_reduce_ps) \
VECTOR_INTRIN(_mm256_maskz_rol_epi32) \
VECTOR_INTRIN(_mm256_maskz_rol_epi64) \
VECTOR_INTRIN(_mm256_maskz_rolv_epi32) \
VECTOR_INTRIN(_mm256_maskz_rolv_epi64) \
VECTOR_INTRIN(_mm256_maskz_ror_epi32) \
VECTOR_INTRIN(_mm256_maskz_ror_epi64) \
VECTOR_INTRIN(_mm256_maskz_rorv_epi32) \
VECTOR_INTRIN(_mm256_maskz_rorv_epi64) \
VECTOR_INTRIN(_mm256_maskz_roundscale_pd) \
VECTOR_INTRIN(_mm256_maskz_roundscale_ps) \
VECTOR_INTRIN(_mm256_maskz_rsqrt14_pd) \
VECTOR_INTRIN(_mm256_maskz_rsqrt14_ps) \
VECTOR_INTRIN(_mm256_maskz_scalef_pd) \
VECTOR_INTRIN(_mm256_maskz_scalef_ps) \
VECTOR_INTRIN(_mm256_maskz_set1_epi16) \
VECTOR_INTRIN(_mm256_maskz_set1_epi32) \
VECTOR_INTRIN(_mm256_maskz_set1_epi64) \
VECTOR_INTRIN(_mm256_maskz_set1_epi8) \
VECTOR_INTRIN(_mm256_maskz_shuffle_epi32) \
VECTOR_INTRIN(_mm256_maskz_shuffle_epi8) \
VECTOR_INTRIN(_mm256_maskz_shuffle_f32x4) \
VECTOR_INTRIN(_mm256_maskz_shuffle_f64x2) \
VECTOR_INTRIN(_mm256_maskz_shufflehi_epi16) \
VECTOR_INTRIN(_mm256_maskz_shuffle_i32x4) \
VECTOR_INTRIN(_mm256_maskz_shuffle_i64x2) \
VECTOR_INTRIN(_mm256_maskz_shufflelo_epi16) \
VECTOR_INTRIN(_mm256_maskz_shuffle_pd) \
VECTOR_INTRIN(_mm256_maskz_shuffle_ps) \
VECTOR_INTRIN(_mm256_maskz_sll_epi16) \
VECTOR_INTRIN(_mm256_maskz_sll_epi32) \
VECTOR_INTRIN(_mm256_maskz_sll_epi64) \
VECTOR_INTRIN(_mm256_maskz_slli_epi16) \
VECTOR_INTRIN(_mm256_maskz_slli_epi32) \
VECTOR_INTRIN(_mm256_maskz_slli_epi64) \
VECTOR_INTRIN(_mm256_maskz_sllv_epi16) \
VECTOR_INTRIN(_mm256_maskz_sllv_epi32) \
VECTOR_INTRIN(_mm256_maskz_sllv_epi64) \
VECTOR_INTRIN(_mm256_maskz_sqrt_pd) \
VECTOR_INTRIN(_mm256_maskz_sqrt_ps) \
VECTOR_INTRIN(_mm256_maskz_sra_epi16) \
VECTOR_INTRIN(_mm256_maskz_sra_epi32) \
VECTOR_INTRIN(_mm256_maskz_sra_epi64) \
VECTOR_INTRIN(_mm256_maskz_srai_epi16) \
VECTOR_INTRIN(_mm256_maskz_srai_epi32) \
VECTOR_INTRIN(_mm256_maskz_srai_epi64) \
VECTOR_INTRIN(_mm256_maskz_srav_epi16) \
VECTOR_INTRIN(_mm256_maskz_srav_epi32) \
VECTOR_INTRIN(_mm256_maskz_srav_epi64) \
VECTOR_INTRIN(_mm256_maskz_srl_epi16) \
VECTOR_INTRIN(_mm256_maskz_srl_epi32) \
VECTOR_INTRIN(_mm256_maskz_srl_epi64) \
VECTOR_INTRIN(_mm256_maskz_srli_epi16) \
VECTOR_INTRIN(_mm256_maskz_srli_epi32) \
VECTOR_INTRIN(_mm256_maskz_srli_epi64) \
VECTOR_INTRIN(_mm256_maskz_srlv_epi16) \
VECTOR_INTRIN(_mm256_maskz_srlv_epi32) \
VECTOR_INTRIN(_mm256_maskz_srlv_epi64) \
VECTOR_INTRIN(_mm256_maskz_sub_epi16) \
VECTOR_INTRIN(_mm256_maskz_sub_epi32) \
VECTOR_INTRIN(_mm256_maskz_sub_epi64) \
VECTOR_INTRIN(_mm256_maskz_sub_epi8) \
VECTOR_INTRIN(_mm256_maskz_sub_pd) \
VECTOR_INTRIN(_mm256_maskz_sub_ps) \
VECTOR_INTRIN(_mm256_maskz_subs_epi16) \
VECTOR_INTRIN(_mm256_maskz_subs_epi8) \
VECTOR_INTRIN(_mm256_maskz_subs_epu16) \
VECTOR_INTRIN(_mm256_maskz_subs_epu8) \
VECTOR_INTRIN(_mm256_maskz_ternarylogic_epi32) \
VECTOR_INTRIN(_mm256_maskz_ternarylogic_epi64) \
VECTOR_INTRIN(_mm256_maskz_unpackhi_epi16) \
VECTOR_INTRIN(_mm256_maskz_unpackhi_epi32) \
VECTOR_INTRIN(_mm256_maskz_unpackhi_epi64) \
VECTOR_INTRIN(_mm256_maskz_unpackhi_epi8) \
VECTOR_INTRIN(_mm256_maskz_unpackhi_pd) \
VECTOR_INTRIN(_mm256_maskz_unpackhi_ps) \
VECTOR_INTRIN(_mm256_maskz_unpacklo_epi16) \
VECTOR_INTRIN(_mm256_maskz_unpacklo_epi32) \
VECTOR_INTRIN(_mm256_maskz_unpacklo_epi64) \
VECTOR_INTRIN(_mm256_maskz_unpacklo_epi8) \
VECTOR_INTRIN(_mm256_maskz_unpacklo_pd) \
VECTOR_INTRIN(_mm256_maskz_unpacklo_ps) \
VECTOR_INTRIN(_mm256_maskz_xor_epi32) \
VECTOR_INTRIN(_mm256_maskz_xor_epi64) \
VECTOR_INTRIN(_mm256_maskz_xor_pd) \
VECTOR_INTRIN(_mm256_maskz_xor_ps) \
VECTOR_INTRIN(_mm256_max_epi16) \
VECTOR_INTRIN(_mm256_max_epi32) \
VECTOR_INTRIN(_mm256_max_epi64) \
VECTOR_INTRIN(_mm256_max_epi8) \
VECTOR_INTRIN(_mm256_max_epu16) \
VECTOR_INTRIN(_mm256_max_epu32) \
VECTOR_INTRIN(_mm256_max_epu64) \
VECTOR_INTRIN(_mm256_max_epu8) \
VECTOR_INTRIN(_mm256_max_pd) \
VECTOR_INTRIN(_mm256_max_ps) \
VECTOR_INTRIN(_mm256_min_epi16) \
VECTOR_INTRIN(_mm256_min_epi32) \
VECTOR_INTRIN(_mm256_min_epi64) \
VECTOR_INTRIN(_mm256_min_epi8) \
VECTOR_INTRIN(_mm256_min_epu16) \
VECTOR_INTRIN(_mm256_min_epu32) \
VECTOR_INTRIN(_mm256_min_epu64) \
VECTOR_INTRIN(_mm256_min_epu8) \
VECTOR_INTRIN(_mm256_min_pd) \
VECTOR_INTRIN(_mm256_min_ps) \
VECTOR_INTRIN(_mm256_mmask_i32gather_epi32) \
VECTOR_INTRIN(_mm256_mmask_i32gather_epi64) \
VECTOR_INTRIN(_mm256_mmask_i32gather_pd) \
VECTOR_INTRIN(_mm256_mmask_i32gather_ps) \
VECTOR_INTRIN(_mm256_mmask_i64gather_epi32) \
VECTOR_INTRIN(_mm256_mmask_i64gather_epi64) \
VECTOR_INTRIN(_mm256_mmask_i64gather_pd) \
VECTOR_INTRIN(_mm256_mmask_i64gather_ps) \
VECTOR_INTRIN(_mm256_movedup_pd) \
VECTOR_INTRIN(_mm256_movehdup_ps) \
VECTOR_INTRIN(_mm256_moveldup_ps) \
VECTOR_INTRIN(_mm256_movemask_epi8) \
VECTOR_INTRIN(_mm256_movemask_pd) \
VECTOR_INTRIN(_mm256_movemask_ps) \
VECTOR_INTRIN(_mm256_movepi16_mask) \
VECTOR_INTRIN(_mm256_movepi32_mask) \
VECTOR_INTRIN(_mm256_movepi64_mask) \
VECTOR_INTRIN(_mm256_movepi8_mask) \
VECTOR_INTRIN(_mm256_movm_epi16) \
VECTOR_INTRIN(_mm256_movm_epi32) \
VECTOR_INTRIN(_mm256_movm_epi64) \
VECTOR_INTRIN(_mm256_movm_epi8) \
VECTOR_INTRIN(_mm256_mpsadbw_epu8) \
VECTOR_INTRIN(_mm256_mul_epi32) \
VECTOR_INTRIN(_mm256_mul_epu32) \
VECTOR_INTRIN(_mm256_mulhi_epi16) \
VECTOR_INTRIN(_mm256_mulhi_epu16) \
VECTOR_INTRIN(_mm256_mulhrs_epi16) \
VECTOR_INTRIN(_mm256_mullo_epi16) \
VECTOR_INTRIN(_mm256_mullo_epi32) \
VECTOR_INTRIN(_mm256_mullo_epi64) \
VECTOR_INTRIN(_mm256_mul_pd) \
VECTOR_INTRIN(_mm256_mul_ps) \
VECTOR_INTRIN(_mm256_multishift_epi64_epi8) \
VECTOR_INTRIN(_mm256_or_pd) \
VECTOR_INTRIN(_mm256_or_ps) \
VECTOR_INTRIN(_mm256_or_si256) \
VECTOR_INTRIN(_mm256_packs_epi16) \
VECTOR_INTRIN(_mm256_packs_epi32) \
VECTOR_INTRIN(_mm256_packus_epi16) \
VECTOR_INTRIN(_mm256_packus_epi32) \
VECTOR_INTRIN(_mm256_permute2f128_pd) \
VECTOR_INTRIN(_mm256_permute2f128_ps) \
VECTOR_INTRIN(_mm256_permute2f128_si256) \
VECTOR_INTRIN(_mm256_permute2x128_si256) \
VECTOR_INTRIN(_mm256_permute4x64_epi64) \
VECTOR_INTRIN(_mm256_permute4x64_pd) \
VECTOR_INTRIN(_mm256_permute_pd) \
VECTOR_INTRIN(_mm256_permute_ps) \
VECTOR_INTRIN(_mm256_permutevar8x32_epi32) \
VECTOR_INTRIN(_mm256_permutevar8x32_ps) \
VECTOR_INTRIN(_mm256_permutevar_pd) \
VECTOR_INTRIN(_mm256_permutevar_ps) \
VECTOR_INTRIN(_mm256_permutex2var_epi16) \
VECTOR_INTRIN(_mm256_permutex2var_epi32) \
VECTOR_INTRIN(_mm256_permutex2var_epi64) \
VECTOR_INTRIN(_mm256_permutex2var_epi8) \
VECTOR_INTRIN(_mm256_permutex2var_pd) \
VECTOR_INTRIN(_mm256_permutex2var_ps) \
VECTOR_INTRIN(_mm256_permutex_pd) \
VECTOR_INTRIN(_mm256_permutexvar_epi16) \
VECTOR_INTRIN(_mm256_permutexvar_epi64) \
VECTOR_INTRIN(_mm256_permutexvar_epi8) \
VECTOR_INTRIN(_mm256_permutexvar_pd) \
VECTOR_INTRIN(_mm256_pow_pd) \
VECTOR_INTRIN(_mm256_pow_ps) \
VECTOR_INTRIN(_mm256_range_pd) \
VECTOR_INTRIN(_mm256_range_ps) \
VECTOR_INTRIN(_mm256_rcp14_pd) \
VECTOR_INTRIN(_mm256_rcp14_ps) \
VECTOR_INTRIN(_mm256_rcp_ps) \
VECTOR_INTRIN(_mm256_reduce_pd) \
VECTOR_INTRIN(_mm256_reduce_ps) \
VECTOR_INTRIN(_mm256_rem_epi16) \
VECTOR_INTRIN(_mm256_rem_epi32) \
VECTOR_INTRIN(_mm256_rem_epi64) \
VECTOR_INTRIN(_mm256_rem_epi8) \
VECTOR_INTRIN(_mm256_rem_epu16) \
VECTOR_INTRIN(_mm256_rem_epu32) \
VECTOR_INTRIN(_mm256_rem_epu64) \
VECTOR_INTRIN(_mm256_rem_epu8) \
VECTOR_INTRIN(_mm256_rol_epi32) \
VECTOR_INTRIN(_mm256_rol_epi64) \
VECTOR_INTRIN(_mm256_rolv_epi32) \
VECTOR_INTRIN(_mm256_rolv_epi64) \
VECTOR_INTRIN(_mm256_ror_epi32) \
VECTOR_INTRIN(_mm256_ror_epi64) \
VECTOR_INTRIN(_mm256_rorv_epi32) \
VECTOR_INTRIN(_mm256_rorv_epi64) \
VECTOR_INTRIN(_mm256_round_pd) \
VECTOR_INTRIN(_mm256_round_ps) \
VECTOR_INTRIN(_mm256_roundscale_pd) \
VECTOR_INTRIN(_mm256_roundscale_ps) \
VECTOR_INTRIN(_mm256_rsqrt_ps) \
VECTOR_INTRIN(_mm256_sad_epu8) \
VECTOR_INTRIN(_mm256_scalef_pd) \
VECTOR_INTRIN(_mm256_scalef_ps) \
VECTOR_INTRIN(_mm256_set1_epi16) \
VECTOR_INTRIN(_mm256_set1_epi32) \
VECTOR_INTRIN(_mm256_set1_epi64x) \
VECTOR_INTRIN(_mm256_set1_epi8) \
VECTOR_INTRIN(_mm256_set1_pd) \
VECTOR_INTRIN(_mm256_set1_ps) \
VECTOR_INTRIN(_mm256_set_epi16) \
VECTOR_INTRIN(_mm256_set_epi32) \
VECTOR_INTRIN(_mm256_set_epi64x) \
VECTOR_INTRIN(_mm256_set_epi8) \
VECTOR_INTRIN(_mm256_set_pd) \
VECTOR_INTRIN(_mm256_set_ps) \
VECTOR_INTRIN(_mm256_setr_epi16) \
VECTOR_INTRIN(_mm256_setr_epi32) \
VECTOR_INTRIN(_mm256_setr_epi64x) \
VECTOR_INTRIN(_mm256_setr_epi8) \
VECTOR_INTRIN(_mm256_setr_pd) \
VECTOR_INTRIN(_mm256_setr_ps) \
VECTOR_INTRIN(_mm256_setzero_pd) \
VECTOR_INTRIN(_mm256_setzero_ps) \
VECTOR_INTRIN(_mm256_setzero_si256) \
VECTOR_INTRIN(_mm256_shuffle_epi32) \
VECTOR_INTRIN(_mm256_shuffle_epi8) \
VECTOR_INTRIN(_mm256_shuffle_f32x4) \
VECTOR_INTRIN(_mm256_shuffle_f64x2) \
VECTOR_INTRIN(_mm256_shufflehi_epi16) \
VECTOR_INTRIN(_mm256_shuffle_i32x4) \
VECTOR_INTRIN(_mm256_shuffle_i64x2) \
VECTOR_INTRIN(_mm256_shufflelo_epi16) \
VECTOR_INTRIN(_mm256_shuffle_pd) \
VECTOR_INTRIN(_mm256_shuffle_ps) \
VECTOR_INTRIN(_mm256_sign_epi16) \
VECTOR_INTRIN(_mm256_sign_epi32) \
VECTOR_INTRIN(_mm256_sign_epi8) \
VECTOR_INTRIN(_mm256_sincos_pd) \
VECTOR_INTRIN(_mm256_sincos_ps) \
VECTOR_INTRIN(_mm256_sind_pd) \
VECTOR_INTRIN(_mm256_sind_ps) \
VECTOR_INTRIN(_mm256_sinh_pd) \
VECTOR_INTRIN(_mm256_sinh_ps) \
VECTOR_INTRIN(_mm256_sin_pd) \
VECTOR_INTRIN(_mm256_sin_ps) \
VECTOR_INTRIN(_mm256_sll_epi16) \
VECTOR_INTRIN(_mm256_sll_epi32) \
VECTOR_INTRIN(_mm256_sll_epi64) \
VECTOR_INTRIN(_mm256_slli_epi16) \
VECTOR_INTRIN(_mm256_slli_epi32) \
VECTOR_INTRIN(_mm256_slli_epi64) \
VECTOR_INTRIN(_mm256_slli_si256) \
VECTOR_INTRIN(_mm256_sllv_epi16) \
VECTOR_INTRIN(_mm256_sllv_epi32) \
VECTOR_INTRIN(_mm256_sllv_epi64) \
VECTOR_INTRIN(_mm256_sqrt_pd) \
VECTOR_INTRIN(_mm256_sqrt_ps) \
VECTOR_INTRIN(_mm256_sra_epi16) \
VECTOR_INTRIN(_mm256_sra_epi32) \
VECTOR_INTRIN(_mm256_sra_epi64) \
VECTOR_INTRIN(_mm256_srai_epi16) \
VECTOR_INTRIN(_mm256_srai_epi32) \
VECTOR_INTRIN(_mm256_srai_epi64) \
VECTOR_INTRIN(_mm256_srav_epi16) \
VECTOR_INTRIN(_mm256_srav_epi32) \
VECTOR_INTRIN(_mm256_srav_epi64) \
VECTOR_INTRIN(_mm256_srl_epi16) \
VECTOR_INTRIN(_mm256_srl_epi32) \
VECTOR_INTRIN(_mm256_srl_epi64) \
VECTOR_INTRIN(_mm256_srli_epi16) \
VECTOR_INTRIN(_mm256_srli_epi32) \
VECTOR_INTRIN(_mm256_srli_epi64) \
VECTOR_INTRIN(_mm256_srli_si256) \
VECTOR_INTRIN(_mm256_srlv_epi16) \
VECTOR_INTRIN(_mm256_srlv_epi32) \
VECTOR_INTRIN(_mm256_srlv_epi64) \
VECTOR_INTRIN(_mm256_store_pd) \
VECTOR_INTRIN(_mm256_store_ps) \
VECTOR_INTRIN(_mm256_store_si256) \
VECTOR_INTRIN(_mm256_storeu_pd) \
VECTOR_INTRIN(_mm256_storeu_ps) \
VECTOR_INTRIN(_mm256_storeu_si256) \
VECTOR_INTRIN(_mm256_stream_load_si256) \
VECTOR_INTRIN(_mm256_stream_pd) \
VECTOR_INTRIN(_mm256_stream_ps) \
VECTOR_INTRIN(_mm256_stream_si256) \
VECTOR_INTRIN(_mm256_sub_epi16) \
VECTOR_INTRIN(_mm256_sub_epi32) \
VECTOR_INTRIN(_mm256_sub_epi64) \
VECTOR_INTRIN(_mm256_sub_epi8) \
VECTOR_INTRIN(_mm256_sub_pd) \
VECTOR_INTRIN(_mm256_sub_ps) \
VECTOR_INTRIN(_mm256_subs_epi16) \
VECTOR_INTRIN(_mm256_subs_epi8) \
VECTOR_INTRIN(_mm256_subs_epu16) \
VECTOR_INTRIN(_mm256_subs_epu8) \
VECTOR_INTRIN(_mm256_svml_ceil_pd) \
VECTOR_INTRIN(_mm256_svml_ceil_ps) \
VECTOR_INTRIN(_mm256_svml_floor_pd) \
VECTOR_INTRIN(_mm256_svml_floor_ps) \
VECTOR_INTRIN(_mm256_svml_round_pd) \
VECTOR_INTRIN(_mm256_svml_round_ps) \
VECTOR_INTRIN(_mm256_svml_sqrt_pd) \
VECTOR_INTRIN(_mm256_svml_sqrt_ps) \
VECTOR_INTRIN(_mm256_tand_pd) \
VECTOR_INTRIN(_mm256_tand_ps) \
VECTOR_INTRIN(_mm256_tanh_pd) \
VECTOR_INTRIN(_mm256_tanh_ps) \
VECTOR_INTRIN(_mm256_tan_pd) \
VECTOR_INTRIN(_mm256_tan_ps) \
VECTOR_INTRIN(_mm256_ternarylogic_epi32) \
VECTOR_INTRIN(_mm256_ternarylogic_epi64) \
VECTOR_INTRIN(_mm256_testc_pd) \
VECTOR_INTRIN(_mm256_testc_ps) \
VECTOR_INTRIN(_mm256_testc_si256) \
VECTOR_INTRIN(_mm256_test_epi16_mask) \
VECTOR_INTRIN(_mm256_test_epi32_mask) \
VECTOR_INTRIN(_mm256_test_epi64_mask) \
VECTOR_INTRIN(_mm256_test_epi8_mask) \
VECTOR_INTRIN(_mm256_testn_epi16_mask) \
VECTOR_INTRIN(_mm256_testn_epi32_mask) \
VECTOR_INTRIN(_mm256_testn_epi64_mask) \
VECTOR_INTRIN(_mm256_testn_epi8_mask) \
VECTOR_INTRIN(_mm256_testnzc_pd) \
VECTOR_INTRIN(_mm256_testnzc_ps) \
VECTOR_INTRIN(_mm256_testnzc_si256) \
VECTOR_INTRIN(_mm256_testz_pd) \
VECTOR_INTRIN(_mm256_testz_ps) \
VECTOR_INTRIN(_mm256_testz_si256) \
VECTOR_INTRIN(_mm256_trunc_pd) \
VECTOR_INTRIN(_mm256_trunc_ps) \
VECTOR_INTRIN(_mm256_udivrem_epi32) \
VECTOR_INTRIN(_mm256_undefined_pd) \
VECTOR_INTRIN(_mm256_undefined_ps) \
VECTOR_INTRIN(_mm256_undefined_si256) \
VECTOR_INTRIN(_mm256_unpackhi_epi16) \
VECTOR_INTRIN(_mm256_unpackhi_epi32) \
VECTOR_INTRIN(_mm256_unpackhi_epi64) \
VECTOR_INTRIN(_mm256_unpackhi_epi8) \
VECTOR_INTRIN(_mm256_unpackhi_pd) \
VECTOR_INTRIN(_mm256_unpackhi_ps) \
VECTOR_INTRIN(_mm256_unpacklo_epi16) \
VECTOR_INTRIN(_mm256_unpacklo_epi32) \
VECTOR_INTRIN(_mm256_unpacklo_epi64) \
VECTOR_INTRIN(_mm256_unpacklo_epi8) \
VECTOR_INTRIN(_mm256_unpacklo_pd) \
VECTOR_INTRIN(_mm256_unpacklo_ps) \
VECTOR_INTRIN(_mm256_xor_pd) \
VECTOR_INTRIN(_mm256_xor_ps) \
VECTOR_INTRIN(_mm256_xor_si256) \
VECTOR_INTRIN(_mm256_zeroall) \
VECTOR_INTRIN(_mm256_zeroupper) \
VECTOR_INTRIN(_mm512_abs_epi16) \
VECTOR_INTRIN(_mm512_abs_epi32) \
VECTOR_INTRIN(_mm512_abs_epi64) \
VECTOR_INTRIN(_mm512_abs_epi8) \
VECTOR_INTRIN(_mm512_abs_pd) \
VECTOR_INTRIN(_mm512_abs_ps) \
VECTOR_INTRIN(_mm512_acosh_pd) \
VECTOR_INTRIN(_mm512_acosh_ps) \
VECTOR_INTRIN(_mm512_acos_pd) \
VECTOR_INTRIN(_mm512_acos_ps) \
VECTOR_INTRIN(_mm512_add_epi16) \
VECTOR_INTRIN(_mm512_add_epi32) \
VECTOR_INTRIN(_mm512_add_epi64) \
VECTOR_INTRIN(_mm512_add_epi8) \
VECTOR_INTRIN(_mm512_addn_pd) \
VECTOR_INTRIN(_mm512_addn_ps) \
VECTOR_INTRIN(_mm512_addn_round_pd) \
VECTOR_INTRIN(_mm512_addn_round_ps) \
VECTOR_INTRIN(_mm512_add_pd) \
VECTOR_INTRIN(_mm512_add_ps) \
VECTOR_INTRIN(_mm512_add_round_pd) \
VECTOR_INTRIN(_mm512_add_round_ps) \
VECTOR_INTRIN(_mm512_adds_epi16) \
VECTOR_INTRIN(_mm512_adds_epi8) \
VECTOR_INTRIN(_mm512_adds_epu16) \
VECTOR_INTRIN(_mm512_adds_epu8) \
VECTOR_INTRIN(_mm512_alignr_epi32) \
VECTOR_INTRIN(_mm512_alignr_epi64) \
VECTOR_INTRIN(_mm512_alignr_epi8) \
VECTOR_INTRIN(_mm512_and_epi32) \
VECTOR_INTRIN(_mm512_and_epi64) \
VECTOR_INTRIN(_mm512_andnot_epi32) \
VECTOR_INTRIN(_mm512_andnot_epi64) \
VECTOR_INTRIN(_mm512_andnot_pd) \
VECTOR_INTRIN(_mm512_andnot_ps) \
VECTOR_INTRIN(_mm512_and_pd) \
VECTOR_INTRIN(_mm512_and_ps) \
VECTOR_INTRIN(_mm512_asinh_pd) \
VECTOR_INTRIN(_mm512_asinh_ps) \
VECTOR_INTRIN(_mm512_asin_pd) \
VECTOR_INTRIN(_mm512_asin_ps) \
VECTOR_INTRIN(_mm512_atan2_pd) \
VECTOR_INTRIN(_mm512_atan2_ps) \
VECTOR_INTRIN(_mm512_atanh_pd) \
VECTOR_INTRIN(_mm512_atanh_ps) \
VECTOR_INTRIN(_mm512_atan_pd) \
VECTOR_INTRIN(_mm512_atan_ps) \
VECTOR_INTRIN(_mm512_avg_epu16) \
VECTOR_INTRIN(_mm512_avg_epu8) \
VECTOR_INTRIN(_mm512_broadcastb_epi8) \
VECTOR_INTRIN(_mm512_broadcastd_epi32) \
VECTOR_INTRIN(_mm512_broadcast_f32x2) \
VECTOR_INTRIN(_mm512_broadcast_f32x4) \
VECTOR_INTRIN(_mm512_broadcast_f32x8) \
VECTOR_INTRIN(_mm512_broadcast_f64x2) \
VECTOR_INTRIN(_mm512_broadcast_f64x4) \
VECTOR_INTRIN(_mm512_broadcast_i32x2) \
VECTOR_INTRIN(_mm512_broadcast_i32x4) \
VECTOR_INTRIN(_mm512_broadcast_i32x8) \
VECTOR_INTRIN(_mm512_broadcast_i64x2) \
VECTOR_INTRIN(_mm512_broadcast_i64x4) \
VECTOR_INTRIN(_mm512_broadcastmb_epi64) \
VECTOR_INTRIN(_mm512_broadcastmw_epi32) \
VECTOR_INTRIN(_mm512_broadcastq_epi64) \
VECTOR_INTRIN(_mm512_broadcastsd_pd) \
VECTOR_INTRIN(_mm512_broadcastss_ps) \
VECTOR_INTRIN(_mm512_broadcastw_epi16) \
VECTOR_INTRIN(_mm512_bslli_epi128) \
VECTOR_INTRIN(_mm512_bsrli_epi128) \
VECTOR_INTRIN(_mm512_castpd128_pd512) \
VECTOR_INTRIN(_mm512_castpd256_pd512) \
VECTOR_INTRIN(_mm512_castpd512_pd128) \
VECTOR_INTRIN(_mm512_castpd512_pd256) \
VECTOR_INTRIN(_mm512_castpd_ps) \
VECTOR_INTRIN(_mm512_castpd_si512) \
VECTOR_INTRIN(_mm512_castps128_ps512) \
VECTOR_INTRIN(_mm512_castps256_ps512) \
VECTOR_INTRIN(_mm512_castps512_ps128) \
VECTOR_INTRIN(_mm512_castps512_ps256) \
VECTOR_INTRIN(_mm512_castps_pd) \
VECTOR_INTRIN(_mm512_castps_si512) \
VECTOR_INTRIN(_mm512_castsi128_si512) \
VECTOR_INTRIN(_mm512_castsi256_si512) \
VECTOR_INTRIN(_mm512_castsi512_pd) \
VECTOR_INTRIN(_mm512_castsi512_ps) \
VECTOR_INTRIN(_mm512_castsi512_si128) \
VECTOR_INTRIN(_mm512_castsi512_si256) \
VECTOR_INTRIN(_mm512_cbrt_pd) \
VECTOR_INTRIN(_mm512_cbrt_ps) \
VECTOR_INTRIN(_mm512_cdfnorminv_pd) \
VECTOR_INTRIN(_mm512_cdfnorminv_ps) \
VECTOR_INTRIN(_mm512_cdfnorm_pd) \
VECTOR_INTRIN(_mm512_cdfnorm_ps) \
VECTOR_INTRIN(_mm512_ceil_pd) \
VECTOR_INTRIN(_mm512_ceil_ps) \
VECTOR_INTRIN(_mm512_cmp_epi16_mask) \
VECTOR_INTRIN(_mm512_cmp_epi32_mask) \
VECTOR_INTRIN(_mm512_cmp_epi64_mask) \
VECTOR_INTRIN(_mm512_cmp_epi8_mask) \
VECTOR_INTRIN(_mm512_cmp_epu16_mask) \
VECTOR_INTRIN(_mm512_cmp_epu32_mask) \
VECTOR_INTRIN(_mm512_cmp_epu64_mask) \
VECTOR_INTRIN(_mm512_cmp_epu8_mask) \
VECTOR_INTRIN(_mm512_cmp_pd_mask) \
VECTOR_INTRIN(_mm512_cmp_ps_mask) \
VECTOR_INTRIN(_mm512_cmp_round_pd_mask) \
VECTOR_INTRIN(_mm512_cmp_round_ps_mask) \
VECTOR_INTRIN(_mm512_conflict_epi32) \
VECTOR_INTRIN(_mm512_conflict_epi64) \
VECTOR_INTRIN(_mm512_cosd_pd) \
VECTOR_INTRIN(_mm512_cosd_ps) \
VECTOR_INTRIN(_mm512_cosh_pd) \
VECTOR_INTRIN(_mm512_cosh_ps) \
VECTOR_INTRIN(_mm512_cos_pd) \
VECTOR_INTRIN(_mm512_cos_ps) \
VECTOR_INTRIN(_mm512_cvtepi16_epi32) \
VECTOR_INTRIN(_mm512_cvtepi16_epi64) \
VECTOR_INTRIN(_mm512_cvtepi16_epi8) \
VECTOR_INTRIN(_mm512_cvtepi32_epi16) \
VECTOR_INTRIN(_mm512_cvtepi32_epi64) \
VECTOR_INTRIN(_mm512_cvtepi32_epi8) \
VECTOR_INTRIN(_mm512_cvtepi32lo_pd) \
VECTOR_INTRIN(_mm512_cvtepi32_pd) \
VECTOR_INTRIN(_mm512_cvtepi64_epi16) \
VECTOR_INTRIN(_mm512_cvtepi64_epi32) \
VECTOR_INTRIN(_mm512_cvtepi64_epi8) \
VECTOR_INTRIN(_mm512_cvtepi8_epi16) \
VECTOR_INTRIN(_mm512_cvtepi8_epi32) \
VECTOR_INTRIN(_mm512_cvtepi8_epi64) \
VECTOR_INTRIN(_mm512_cvtepu16_epi32) \
VECTOR_INTRIN(_mm512_cvtepu16_epi64) \
VECTOR_INTRIN(_mm512_cvtepu32_epi64) \
VECTOR_INTRIN(_mm512_cvtepu32lo_pd) \
VECTOR_INTRIN(_mm512_cvtepu32_pd) \
VECTOR_INTRIN(_mm512_cvtepu8_epi16) \
VECTOR_INTRIN(_mm512_cvtepu8_epi32) \
VECTOR_INTRIN(_mm512_cvtepu8_epi64) \
VECTOR_INTRIN(_mm512_cvtfxpnt_round_adjustepi32_ps) \
VECTOR_INTRIN(_mm512_cvtfxpnt_round_adjustepu32_ps) \
VECTOR_INTRIN(_mm512_cvtfxpnt_round_adjustps_epi32) \
VECTOR_INTRIN(_mm512_cvtfxpnt_round_adjustps_epu32) \
VECTOR_INTRIN(_mm512_cvtfxpnt_roundpd_epi32lo) \
VECTOR_INTRIN(_mm512_cvtfxpnt_roundpd_epu32lo) \
VECTOR_INTRIN(_mm512_cvtpslo_pd) \
VECTOR_INTRIN(_mm512_cvt_roundepi32_ps) \
VECTOR_INTRIN(_mm512_cvt_roundepi64_pd) \
VECTOR_INTRIN(_mm512_cvt_roundepi64_ps) \
VECTOR_INTRIN(_mm512_cvt_roundepu32_ps) \
VECTOR_INTRIN(_mm512_cvt_roundepu64_pd) \
VECTOR_INTRIN(_mm512_cvt_roundepu64_ps) \
VECTOR_INTRIN(_mm512_cvt_roundpd_epi32) \
VECTOR_INTRIN(_mm512_cvt_roundpd_epi64) \
VECTOR_INTRIN(_mm512_cvt_roundpd_epu32) \
VECTOR_INTRIN(_mm512_cvt_roundpd_epu64) \
VECTOR_INTRIN(_mm512_cvt_roundpd_ps) \
VECTOR_INTRIN(_mm512_cvt_roundpd_pslo) \
VECTOR_INTRIN(_mm512_cvt_roundph_ps) \
VECTOR_INTRIN(_mm512_cvt_roundps_epi32) \
VECTOR_INTRIN(_mm512_cvt_roundps_epi64) \
VECTOR_INTRIN(_mm512_cvt_roundps_epu32) \
VECTOR_INTRIN(_mm512_cvt_roundps_epu64) \
VECTOR_INTRIN(_mm512_cvt_roundps_pd) \
VECTOR_INTRIN(_mm512_cvt_roundps_ph) \
VECTOR_INTRIN(_mm512_cvtsd_f64) \
VECTOR_INTRIN(_mm512_cvtsepi16_epi8) \
VECTOR_INTRIN(_mm512_cvtsepi32_epi16) \
VECTOR_INTRIN(_mm512_cvtsepi32_epi8) \
VECTOR_INTRIN(_mm512_cvtsepi64_epi16) \
VECTOR_INTRIN(_mm512_cvtsepi64_epi32) \
VECTOR_INTRIN(_mm512_cvtsepi64_epi8) \
VECTOR_INTRIN(_mm512_cvtsi512_si32) \
VECTOR_INTRIN(_mm512_cvtss_f32) \
VECTOR_INTRIN(_mm512_cvtt_roundpd_epi32) \
VECTOR_INTRIN(_mm512_cvtt_roundpd_epi64) \
VECTOR_INTRIN(_mm512_cvtt_roundpd_epu32) \
VECTOR_INTRIN(_mm512_cvtt_roundpd_epu64) \
VECTOR_INTRIN(_mm512_cvtt_roundps_epi32) \
VECTOR_INTRIN(_mm512_cvtt_roundps_epi64) \
VECTOR_INTRIN(_mm512_cvtt_roundps_epu32) \
VECTOR_INTRIN(_mm512_cvtt_roundps_epu64) \
VECTOR_INTRIN(_mm512_cvtusepi16_epi8) \
VECTOR_INTRIN(_mm512_cvtusepi32_epi16) \
VECTOR_INTRIN(_mm512_cvtusepi32_epi8) \
VECTOR_INTRIN(_mm512_cvtusepi64_epi16) \
VECTOR_INTRIN(_mm512_cvtusepi64_epi32) \
VECTOR_INTRIN(_mm512_cvtusepi64_epi8) \
VECTOR_INTRIN(_mm512_dbsad_epu8) \
VECTOR_INTRIN(_mm512_div_epi16) \
VECTOR_INTRIN(_mm512_div_epi32) \
VECTOR_INTRIN(_mm512_div_epi64) \
VECTOR_INTRIN(_mm512_div_epi8) \
VECTOR_INTRIN(_mm512_div_epu16) \
VECTOR_INTRIN(_mm512_div_epu32) \
VECTOR_INTRIN(_mm512_div_epu64) \
VECTOR_INTRIN(_mm512_div_epu8) \
VECTOR_INTRIN(_mm512_div_pd) \
VECTOR_INTRIN(_mm512_div_ps) \
VECTOR_INTRIN(_mm512_div_round_pd) \
VECTOR_INTRIN(_mm512_div_round_ps) \
VECTOR_INTRIN(_mm512_erfcinv_pd) \
VECTOR_INTRIN(_mm512_erfcinv_ps) \
VECTOR_INTRIN(_mm512_erfc_pd) \
VECTOR_INTRIN(_mm512_erfc_ps) \
VECTOR_INTRIN(_mm512_erfinv_pd) \
VECTOR_INTRIN(_mm512_erfinv_ps) \
VECTOR_INTRIN(_mm512_erf_pd) \
VECTOR_INTRIN(_mm512_erf_ps) \
VECTOR_INTRIN(_mm512_exp10_pd) \
VECTOR_INTRIN(_mm512_exp10_ps) \
VECTOR_INTRIN(_mm512_exp2a23_round_pd) \
VECTOR_INTRIN(_mm512_exp2a23_round_ps) \
VECTOR_INTRIN(_mm512_exp2_pd) \
VECTOR_INTRIN(_mm512_exp2_ps) \
VECTOR_INTRIN(_mm512_expm1_pd) \
VECTOR_INTRIN(_mm512_expm1_ps) \
VECTOR_INTRIN(_mm512_exp_pd) \
VECTOR_INTRIN(_mm512_exp_ps) \
VECTOR_INTRIN(_mm512_extload_epi32) \
VECTOR_INTRIN(_mm512_extload_epi64) \
VECTOR_INTRIN(_mm512_extload_pd) \
VECTOR_INTRIN(_mm512_extload_ps) \
VECTOR_INTRIN(_mm512_extloadunpackhi_epi32) \
VECTOR_INTRIN(_mm512_extloadunpackhi_epi64) \
VECTOR_INTRIN(_mm512_extloadunpackhi_pd) \
VECTOR_INTRIN(_mm512_extloadunpackhi_ps) \
VECTOR_INTRIN(_mm512_extloadunpacklo_epi32) \
VECTOR_INTRIN(_mm512_extloadunpacklo_epi64) \
VECTOR_INTRIN(_mm512_extloadunpacklo_pd) \
VECTOR_INTRIN(_mm512_extloadunpacklo_ps) \
VECTOR_INTRIN(_mm512_extpackstorehi_epi32) \
VECTOR_INTRIN(_mm512_extpackstorehi_epi64) \
VECTOR_INTRIN(_mm512_extpackstorehi_pd) \
VECTOR_INTRIN(_mm512_extpackstorehi_ps) \
VECTOR_INTRIN(_mm512_extpackstorelo_epi32) \
VECTOR_INTRIN(_mm512_extpackstorelo_epi64) \
VECTOR_INTRIN(_mm512_extpackstorelo_pd) \
VECTOR_INTRIN(_mm512_extpackstorelo_ps) \
VECTOR_INTRIN(_mm512_extractf32x4_ps) \
VECTOR_INTRIN(_mm512_extractf32x8_ps) \
VECTOR_INTRIN(_mm512_extractf64x2_pd) \
VECTOR_INTRIN(_mm512_extractf64x4_pd) \
VECTOR_INTRIN(_mm512_extracti32x4_epi32) \
VECTOR_INTRIN(_mm512_extracti32x8_epi32) \
VECTOR_INTRIN(_mm512_extracti64x2_epi64) \
VECTOR_INTRIN(_mm512_extracti64x4_epi64) \
VECTOR_INTRIN(_mm512_extstore_epi32) \
VECTOR_INTRIN(_mm512_extstore_epi64) \
VECTOR_INTRIN(_mm512_extstore_pd) \
VECTOR_INTRIN(_mm512_extstore_ps) \
VECTOR_INTRIN(_mm512_fixupimm_pd) \
VECTOR_INTRIN(_mm512_fixupimm_ps) \
VECTOR_INTRIN(_mm512_fixupimm_round_pd) \
VECTOR_INTRIN(_mm512_fixupimm_round_ps) \
VECTOR_INTRIN(_mm512_fixupnan_pd) \
VECTOR_INTRIN(_mm512_fixupnan_ps) \
VECTOR_INTRIN(_mm512_floor_pd) \
VECTOR_INTRIN(_mm512_floor_ps) \
VECTOR_INTRIN(_mm512_fmadd233_epi32) \
VECTOR_INTRIN(_mm512_fmadd233_round_ps) \
VECTOR_INTRIN(_mm512_fmadd_epi32) \
VECTOR_INTRIN(_mm512_fmadd_round_pd) \
VECTOR_INTRIN(_mm512_fmadd_round_ps) \
VECTOR_INTRIN(_mm512_fmaddsub_round_pd) \
VECTOR_INTRIN(_mm512_fmaddsub_round_ps) \
VECTOR_INTRIN(_mm512_fmsubadd_round_pd) \
VECTOR_INTRIN(_mm512_fmsubadd_round_ps) \
VECTOR_INTRIN(_mm512_fmsub_round_pd) \
VECTOR_INTRIN(_mm512_fmsub_round_ps) \
VECTOR_INTRIN(_mm512_fnmadd_round_pd) \
VECTOR_INTRIN(_mm512_fnmadd_round_ps) \
VECTOR_INTRIN(_mm512_fnmsub_round_pd) \
VECTOR_INTRIN(_mm512_fnmsub_round_ps) \
VECTOR_INTRIN(_mm512_fpclass_pd_mask) \
VECTOR_INTRIN(_mm512_fpclass_ps_mask) \
VECTOR_INTRIN(_mm512_getexp_pd) \
VECTOR_INTRIN(_mm512_getexp_ps) \
VECTOR_INTRIN(_mm512_getexp_round_pd) \
VECTOR_INTRIN(_mm512_getexp_round_ps) \
VECTOR_INTRIN(_mm512_getmant_pd) \
VECTOR_INTRIN(_mm512_getmant_ps) \
VECTOR_INTRIN(_mm512_getmant_round_pd) \
VECTOR_INTRIN(_mm512_getmant_round_ps) \
VECTOR_INTRIN(_mm512_gmaxabs_ps) \
VECTOR_INTRIN(_mm512_gmax_pd) \
VECTOR_INTRIN(_mm512_gmax_ps) \
VECTOR_INTRIN(_mm512_gmin_pd) \
VECTOR_INTRIN(_mm512_gmin_ps) \
VECTOR_INTRIN(_mm512_hypot_pd) \
VECTOR_INTRIN(_mm512_hypot_ps) \
VECTOR_INTRIN(_mm512_i32extgather_epi32) \
VECTOR_INTRIN(_mm512_i32extgather_ps) \
VECTOR_INTRIN(_mm512_i32extscatter_epi32) \
VECTOR_INTRIN(_mm512_i32extscatter_ps) \
VECTOR_INTRIN(_mm512_i32loextgather_epi64) \
VECTOR_INTRIN(_mm512_i32loextgather_pd) \
VECTOR_INTRIN(_mm512_i32loextscatter_epi64) \
VECTOR_INTRIN(_mm512_i32loextscatter_pd) \
VECTOR_INTRIN(_mm512_i64extgather_epi32lo) \
VECTOR_INTRIN(_mm512_i64extgather_epi64) \
VECTOR_INTRIN(_mm512_i64extgather_pd) \
VECTOR_INTRIN(_mm512_i64extgather_pslo) \
VECTOR_INTRIN(_mm512_i64extscatter_epi32lo) \
VECTOR_INTRIN(_mm512_i64extscatter_epi64) \
VECTOR_INTRIN(_mm512_i64extscatter_pd) \
VECTOR_INTRIN(_mm512_i64extscatter_pslo) \
VECTOR_INTRIN(_mm512_insertf32x4) \
VECTOR_INTRIN(_mm512_insertf32x8) \
VECTOR_INTRIN(_mm512_insertf64x2) \
VECTOR_INTRIN(_mm512_insertf64x4) \
VECTOR_INTRIN(_mm512_inserti32x4) \
VECTOR_INTRIN(_mm512_inserti32x8) \
VECTOR_INTRIN(_mm512_inserti64x2) \
VECTOR_INTRIN(_mm512_inserti64x4) \
VECTOR_INTRIN(_mm512_invsqrt_pd) \
VECTOR_INTRIN(_mm512_invsqrt_ps) \
VECTOR_INTRIN(_mm512_kconcathi_64) \
VECTOR_INTRIN(_mm512_kconcatlo_64) \
VECTOR_INTRIN(_mm512_kextract_64) \
VECTOR_INTRIN(_mm512_kunpackb) \
VECTOR_INTRIN(_mm512_kunpackd) \
VECTOR_INTRIN(_mm512_kunpackw) \
VECTOR_INTRIN(_mm512_load_epi32) \
VECTOR_INTRIN(_mm512_load_epi64) \
VECTOR_INTRIN(_mm512_load_pd) \
VECTOR_INTRIN(_mm512_load_ps) \
VECTOR_INTRIN(_mm512_loadu_pd) \
VECTOR_INTRIN(_mm512_loadu_ps) \
VECTOR_INTRIN(_mm512_loadu_si512) \
VECTOR_INTRIN(_mm512_log10_pd) \
VECTOR_INTRIN(_mm512_log10_ps) \
VECTOR_INTRIN(_mm512_log1p_pd) \
VECTOR_INTRIN(_mm512_log1p_ps) \
VECTOR_INTRIN(_mm512_log2_pd) \
VECTOR_INTRIN(_mm512_log2_ps) \
VECTOR_INTRIN(_mm512_logb_pd) \
VECTOR_INTRIN(_mm512_logb_ps) \
VECTOR_INTRIN(_mm512_log_pd) \
VECTOR_INTRIN(_mm512_log_ps) \
VECTOR_INTRIN(_mm512_lzcnt_epi32) \
VECTOR_INTRIN(_mm512_lzcnt_epi64) \
VECTOR_INTRIN(_mm512_madd52hi_epu64) \
VECTOR_INTRIN(_mm512_madd52lo_epu64) \
VECTOR_INTRIN(_mm512_madd_epi16) \
VECTOR_INTRIN(_mm512_maddubs_epi16) \
VECTOR_INTRIN(_mm512_mask2_permutex2var_epi16) \
VECTOR_INTRIN(_mm512_mask2_permutex2var_epi32) \
VECTOR_INTRIN(_mm512_mask2_permutex2var_epi64) \
VECTOR_INTRIN(_mm512_mask2_permutex2var_epi8) \
VECTOR_INTRIN(_mm512_mask2_permutex2var_pd) \
VECTOR_INTRIN(_mm512_mask2_permutex2var_ps) \
VECTOR_INTRIN(_mm512_mask3_fmadd_epi32) \
VECTOR_INTRIN(_mm512_mask3_fmadd_round_pd) \
VECTOR_INTRIN(_mm512_mask3_fmadd_round_ps) \
VECTOR_INTRIN(_mm512_mask3_fmaddsub_round_pd) \
VECTOR_INTRIN(_mm512_mask3_fmaddsub_round_ps) \
VECTOR_INTRIN(_mm512_mask3_fmsubadd_round_pd) \
VECTOR_INTRIN(_mm512_mask3_fmsubadd_round_ps) \
VECTOR_INTRIN(_mm512_mask3_fmsub_round_pd) \
VECTOR_INTRIN(_mm512_mask3_fmsub_round_ps) \
VECTOR_INTRIN(_mm512_mask3_fnmadd_round_pd) \
VECTOR_INTRIN(_mm512_mask3_fnmadd_round_ps) \
VECTOR_INTRIN(_mm512_mask3_fnmsub_round_pd) \
VECTOR_INTRIN(_mm512_mask3_fnmsub_round_ps) \
VECTOR_INTRIN(_mm512_mask_abs_epi16) \
VECTOR_INTRIN(_mm512_mask_abs_epi32) \
VECTOR_INTRIN(_mm512_mask_abs_epi64) \
VECTOR_INTRIN(_mm512_mask_abs_epi8) \
VECTOR_INTRIN(_mm512_mask_abs_pd) \
VECTOR_INTRIN(_mm512_mask_abs_ps) \
VECTOR_INTRIN(_mm512_mask_acosh_pd) \
VECTOR_INTRIN(_mm512_mask_acosh_ps) \
VECTOR_INTRIN(_mm512_mask_acos_pd) \
VECTOR_INTRIN(_mm512_mask_acos_ps) \
VECTOR_INTRIN(_mm512_mask_add_epi16) \
VECTOR_INTRIN(_mm512_mask_add_epi32) \
VECTOR_INTRIN(_mm512_mask_add_epi64) \
VECTOR_INTRIN(_mm512_mask_add_epi8) \
VECTOR_INTRIN(_mm512_mask_addn_pd) \
VECTOR_INTRIN(_mm512_mask_addn_ps) \
VECTOR_INTRIN(_mm512_mask_addn_round_pd) \
VECTOR_INTRIN(_mm512_mask_addn_round_ps) \
VECTOR_INTRIN(_mm512_mask_add_pd) \
VECTOR_INTRIN(_mm512_mask_add_ps) \
VECTOR_INTRIN(_mm512_mask_add_round_pd) \
VECTOR_INTRIN(_mm512_mask_add_round_ps) \
VECTOR_INTRIN(_mm512_mask_adds_epi16) \
VECTOR_INTRIN(_mm512_mask_adds_epi8) \
VECTOR_INTRIN(_mm512_mask_adds_epu16) \
VECTOR_INTRIN(_mm512_mask_adds_epu8) \
VECTOR_INTRIN(_mm512_mask_alignr_epi32) \
VECTOR_INTRIN(_mm512_mask_alignr_epi64) \
VECTOR_INTRIN(_mm512_mask_alignr_epi8) \
VECTOR_INTRIN(_mm512_mask_and_epi32) \
VECTOR_INTRIN(_mm512_mask_and_epi64) \
VECTOR_INTRIN(_mm512_mask_andnot_epi32) \
VECTOR_INTRIN(_mm512_mask_andnot_epi64) \
VECTOR_INTRIN(_mm512_mask_andnot_pd) \
VECTOR_INTRIN(_mm512_mask_andnot_ps) \
VECTOR_INTRIN(_mm512_mask_and_pd) \
VECTOR_INTRIN(_mm512_mask_and_ps) \
VECTOR_INTRIN(_mm512_mask_asinh_pd) \
VECTOR_INTRIN(_mm512_mask_asinh_ps) \
VECTOR_INTRIN(_mm512_mask_asin_pd) \
VECTOR_INTRIN(_mm512_mask_asin_ps) \
VECTOR_INTRIN(_mm512_mask_atan2_pd) \
VECTOR_INTRIN(_mm512_mask_atan2_ps) \
VECTOR_INTRIN(_mm512_mask_atanh_pd) \
VECTOR_INTRIN(_mm512_mask_atanh_ps) \
VECTOR_INTRIN(_mm512_mask_atan_pd) \
VECTOR_INTRIN(_mm512_mask_atan_ps) \
VECTOR_INTRIN(_mm512_mask_avg_epu16) \
VECTOR_INTRIN(_mm512_mask_avg_epu8) \
VECTOR_INTRIN(_mm512_mask_blend_epi16) \
VECTOR_INTRIN(_mm512_mask_blend_epi32) \
VECTOR_INTRIN(_mm512_mask_blend_epi64) \
VECTOR_INTRIN(_mm512_mask_blend_epi8) \
VECTOR_INTRIN(_mm512_mask_blend_pd) \
VECTOR_INTRIN(_mm512_mask_blend_ps) \
VECTOR_INTRIN(_mm512_mask_broadcastb_epi8) \
VECTOR_INTRIN(_mm512_mask_broadcastd_epi32) \
VECTOR_INTRIN(_mm512_mask_broadcast_f32x2) \
VECTOR_INTRIN(_mm512_mask_broadcast_f32x4) \
VECTOR_INTRIN(_mm512_mask_broadcast_f32x8) \
VECTOR_INTRIN(_mm512_mask_broadcast_f64x2) \
VECTOR_INTRIN(_mm512_mask_broadcast_f64x4) \
VECTOR_INTRIN(_mm512_mask_broadcast_i32x2) \
VECTOR_INTRIN(_mm512_mask_broadcast_i32x4) \
VECTOR_INTRIN(_mm512_mask_broadcast_i32x8) \
VECTOR_INTRIN(_mm512_mask_broadcast_i64x2) \
VECTOR_INTRIN(_mm512_mask_broadcast_i64x4) \
VECTOR_INTRIN(_mm512_mask_broadcastq_epi64) \
VECTOR_INTRIN(_mm512_mask_broadcastsd_pd) \
VECTOR_INTRIN(_mm512_mask_broadcastss_ps) \
VECTOR_INTRIN(_mm512_mask_broadcastw_epi16) \
VECTOR_INTRIN(_mm512_mask_cbrt_pd) \
VECTOR_INTRIN(_mm512_mask_cbrt_ps) \
VECTOR_INTRIN(_mm512_mask_cdfnorminv_pd) \
VECTOR_INTRIN(_mm512_mask_cdfnorminv_ps) \
VECTOR_INTRIN(_mm512_mask_cdfnorm_pd) \
VECTOR_INTRIN(_mm512_mask_cdfnorm_ps) \
VECTOR_INTRIN(_mm512_mask_ceil_pd) \
VECTOR_INTRIN(_mm512_mask_ceil_ps) \
VECTOR_INTRIN(_mm512_mask_cmp_epi16_mask) \
VECTOR_INTRIN(_mm512_mask_cmp_epi32_mask) \
VECTOR_INTRIN(_mm512_mask_cmp_epi64_mask) \
VECTOR_INTRIN(_mm512_mask_cmp_epi8_mask) \
VECTOR_INTRIN(_mm512_mask_cmp_epu16_mask) \
VECTOR_INTRIN(_mm512_mask_cmp_epu32_mask) \
VECTOR_INTRIN(_mm512_mask_cmp_epu64_mask) \
VECTOR_INTRIN(_mm512_mask_cmp_epu8_mask) \
VECTOR_INTRIN(_mm512_mask_cmp_pd_mask) \
VECTOR_INTRIN(_mm512_mask_cmp_ps_mask) \
VECTOR_INTRIN(_mm512_mask_cmp_round_pd_mask) \
VECTOR_INTRIN(_mm512_mask_cmp_round_ps_mask) \
VECTOR_INTRIN(_mm512_mask_compress_epi32) \
VECTOR_INTRIN(_mm512_mask_compress_epi64) \
VECTOR_INTRIN(_mm512_mask_compress_pd) \
VECTOR_INTRIN(_mm512_mask_compress_ps) \
VECTOR_INTRIN(_mm512_mask_compressstoreu_epi32) \
VECTOR_INTRIN(_mm512_mask_compressstoreu_epi64) \
VECTOR_INTRIN(_mm512_mask_compressstoreu_pd) \
VECTOR_INTRIN(_mm512_mask_compressstoreu_ps) \
VECTOR_INTRIN(_mm512_mask_conflict_epi32) \
VECTOR_INTRIN(_mm512_mask_conflict_epi64) \
VECTOR_INTRIN(_mm512_mask_cosd_pd) \
VECTOR_INTRIN(_mm512_mask_cosd_ps) \
VECTOR_INTRIN(_mm512_mask_cosh_pd) \
VECTOR_INTRIN(_mm512_mask_cosh_ps) \
VECTOR_INTRIN(_mm512_mask_cos_pd) \
VECTOR_INTRIN(_mm512_mask_cos_ps) \
VECTOR_INTRIN(_mm512_mask_cvtepi16_epi32) \
VECTOR_INTRIN(_mm512_mask_cvtepi16_epi64) \
VECTOR_INTRIN(_mm512_mask_cvtepi16_epi8) \
VECTOR_INTRIN(_mm512_mask_cvtepi16_storeu_epi8) \
VECTOR_INTRIN(_mm512_mask_cvtepi32_epi16) \
VECTOR_INTRIN(_mm512_mask_cvtepi32_epi64) \
VECTOR_INTRIN(_mm512_mask_cvtepi32_epi8) \
VECTOR_INTRIN(_mm512_mask_cvtepi32lo_pd) \
VECTOR_INTRIN(_mm512_mask_cvtepi32_pd) \
VECTOR_INTRIN(_mm512_mask_cvtepi32_storeu_epi16) \
VECTOR_INTRIN(_mm512_mask_cvtepi32_storeu_epi8) \
VECTOR_INTRIN(_mm512_mask_cvtepi64_epi16) \
VECTOR_INTRIN(_mm512_mask_cvtepi64_epi32) \
VECTOR_INTRIN(_mm512_mask_cvtepi64_epi8) \
VECTOR_INTRIN(_mm512_mask_cvtepi64_storeu_epi16) \
VECTOR_INTRIN(_mm512_mask_cvtepi64_storeu_epi32) \
VECTOR_INTRIN(_mm512_mask_cvtepi64_storeu_epi8) \
VECTOR_INTRIN(_mm512_mask_cvtepi8_epi16) \
VECTOR_INTRIN(_mm512_mask_cvtepi8_epi32) \
VECTOR_INTRIN(_mm512_mask_cvtepi8_epi64) \
VECTOR_INTRIN(_mm512_mask_cvtepu16_epi32) \
VECTOR_INTRIN(_mm512_mask_cvtepu16_epi64) \
VECTOR_INTRIN(_mm512_mask_cvtepu32_epi64) \
VECTOR_INTRIN(_mm512_mask_cvtepu32lo_pd) \
VECTOR_INTRIN(_mm512_mask_cvtepu32_pd) \
VECTOR_INTRIN(_mm512_mask_cvtepu8_epi16) \
VECTOR_INTRIN(_mm512_mask_cvtepu8_epi32) \
VECTOR_INTRIN(_mm512_mask_cvtepu8_epi64) \
VECTOR_INTRIN(_mm512_mask_cvtfxpnt_round_adjustepi32_ps) \
VECTOR_INTRIN(_mm512_mask_cvtfxpnt_round_adjustepu32_ps) \
VECTOR_INTRIN(_mm512_mask_cvtfxpnt_round_adjustps_epi32) \
VECTOR_INTRIN(_mm512_mask_cvtfxpnt_round_adjustps_epu32) \
VECTOR_INTRIN(_mm512_mask_cvtfxpnt_roundpd_epi32lo) \
VECTOR_INTRIN(_mm512_mask_cvtfxpnt_roundpd_epu32lo) \
VECTOR_INTRIN(_mm512_mask_cvtpslo_pd) \
VECTOR_INTRIN(_mm512_mask_cvt_roundepi32_ps) \
VECTOR_INTRIN(_mm512_mask_cvt_roundepi64_pd) \
VECTOR_INTRIN(_mm512_mask_cvt_roundepi64_ps) \
VECTOR_INTRIN(_mm512_mask_cvt_roundepu32_ps) \
VECTOR_INTRIN(_mm512_mask_cvt_roundepu64_pd) \
VECTOR_INTRIN(_mm512_mask_cvt_roundepu64_ps) \
VECTOR_INTRIN(_mm512_mask_cvt_roundpd_epi32) \
VECTOR_INTRIN(_mm512_mask_cvt_roundpd_epi64) \
VECTOR_INTRIN(_mm512_mask_cvt_roundpd_epu32) \
VECTOR_INTRIN(_mm512_mask_cvt_roundpd_epu64) \
VECTOR_INTRIN(_mm512_mask_cvt_roundpd_ps) \
VECTOR_INTRIN(_mm512_mask_cvt_roundpd_pslo) \
VECTOR_INTRIN(_mm512_mask_cvt_roundph_ps) \
VECTOR_INTRIN(_mm512_mask_cvt_roundps_epi32) \
VECTOR_INTRIN(_mm512_mask_cvt_roundps_epi64) \
VECTOR_INTRIN(_mm512_mask_cvt_roundps_epu32) \
VECTOR_INTRIN(_mm512_mask_cvt_roundps_epu64) \
VECTOR_INTRIN(_mm512_mask_cvt_roundps_pd) \
VECTOR_INTRIN(_mm512_mask_cvt_roundps_ph) \
VECTOR_INTRIN(_mm512_mask_cvtsepi16_epi8) \
VECTOR_INTRIN(_mm512_mask_cvtsepi16_storeu_epi8) \
VECTOR_INTRIN(_mm512_mask_cvtsepi32_epi16) \
VECTOR_INTRIN(_mm512_mask_cvtsepi32_epi8) \
VECTOR_INTRIN(_mm512_mask_cvtsepi32_storeu_epi16) \
VECTOR_INTRIN(_mm512_mask_cvtsepi32_storeu_epi8) \
VECTOR_INTRIN(_mm512_mask_cvtsepi64_epi16) \
VECTOR_INTRIN(_mm512_mask_cvtsepi64_epi32) \
VECTOR_INTRIN(_mm512_mask_cvtsepi64_epi8) \
VECTOR_INTRIN(_mm512_mask_cvtsepi64_storeu_epi16) \
VECTOR_INTRIN(_mm512_mask_cvtsepi64_storeu_epi32) \
VECTOR_INTRIN(_mm512_mask_cvtsepi64_storeu_epi8) \
VECTOR_INTRIN(_mm512_mask_cvtt_roundpd_epi32) \
VECTOR_INTRIN(_mm512_mask_cvtt_roundpd_epi64) \
VECTOR_INTRIN(_mm512_mask_cvtt_roundpd_epu32) \
VECTOR_INTRIN(_mm512_mask_cvtt_roundpd_epu64) \
VECTOR_INTRIN(_mm512_mask_cvtt_roundps_epi32) \
VECTOR_INTRIN(_mm512_mask_cvtt_roundps_epi64) \
VECTOR_INTRIN(_mm512_mask_cvtt_roundps_epu32) \
VECTOR_INTRIN(_mm512_mask_cvtt_roundps_epu64) \
VECTOR_INTRIN(_mm512_mask_cvtusepi16_epi8) \
VECTOR_INTRIN(_mm512_mask_cvtusepi16_storeu_epi8) \
VECTOR_INTRIN(_mm512_mask_cvtusepi32_epi16) \
VECTOR_INTRIN(_mm512_mask_cvtusepi32_epi8) \
VECTOR_INTRIN(_mm512_mask_cvtusepi32_storeu_epi16) \
VECTOR_INTRIN(_mm512_mask_cvtusepi32_storeu_epi8) \
VECTOR_INTRIN(_mm512_mask_cvtusepi64_epi16) \
VECTOR_INTRIN(_mm512_mask_cvtusepi64_epi32) \
VECTOR_INTRIN(_mm512_mask_cvtusepi64_epi8) \
VECTOR_INTRIN(_mm512_mask_cvtusepi64_storeu_epi16) \
VECTOR_INTRIN(_mm512_mask_cvtusepi64_storeu_epi32) \
VECTOR_INTRIN(_mm512_mask_cvtusepi64_storeu_epi8) \
VECTOR_INTRIN(_mm512_mask_dbsad_epu8) \
VECTOR_INTRIN(_mm512_mask_div_epi32) \
VECTOR_INTRIN(_mm512_mask_div_epu32) \
VECTOR_INTRIN(_mm512_mask_div_pd) \
VECTOR_INTRIN(_mm512_mask_div_ps) \
VECTOR_INTRIN(_mm512_mask_div_round_pd) \
VECTOR_INTRIN(_mm512_mask_div_round_ps) \
VECTOR_INTRIN(_mm512_mask_erfcinv_pd) \
VECTOR_INTRIN(_mm512_mask_erfcinv_ps) \
VECTOR_INTRIN(_mm512_mask_erfc_pd) \
VECTOR_INTRIN(_mm512_mask_erfc_ps) \
VECTOR_INTRIN(_mm512_mask_erfinv_pd) \
VECTOR_INTRIN(_mm512_mask_erfinv_ps) \
VECTOR_INTRIN(_mm512_mask_erf_pd) \
VECTOR_INTRIN(_mm512_mask_erf_ps) \
VECTOR_INTRIN(_mm512_mask_exp10_pd) \
VECTOR_INTRIN(_mm512_mask_exp10_ps) \
VECTOR_INTRIN(_mm512_mask_exp2a23_round_pd) \
VECTOR_INTRIN(_mm512_mask_exp2a23_round_ps) \
VECTOR_INTRIN(_mm512_mask_exp2_pd) \
VECTOR_INTRIN(_mm512_mask_exp2_ps) \
VECTOR_INTRIN(_mm512_mask_expand_epi32) \
VECTOR_INTRIN(_mm512_mask_expand_epi64) \
VECTOR_INTRIN(_mm512_mask_expandloadu_epi32) \
VECTOR_INTRIN(_mm512_mask_expandloadu_epi64) \
VECTOR_INTRIN(_mm512_mask_expandloadu_pd) \
VECTOR_INTRIN(_mm512_mask_expandloadu_ps) \
VECTOR_INTRIN(_mm512_mask_expand_pd) \
VECTOR_INTRIN(_mm512_mask_expand_ps) \
VECTOR_INTRIN(_mm512_mask_expm1_pd) \
VECTOR_INTRIN(_mm512_mask_expm1_ps) \
VECTOR_INTRIN(_mm512_mask_exp_pd) \
VECTOR_INTRIN(_mm512_mask_exp_ps) \
VECTOR_INTRIN(_mm512_mask_extload_epi32) \
VECTOR_INTRIN(_mm512_mask_extload_epi64) \
VECTOR_INTRIN(_mm512_mask_extload_pd) \
VECTOR_INTRIN(_mm512_mask_extload_ps) \
VECTOR_INTRIN(_mm512_mask_extloadunpackhi_epi32) \
VECTOR_INTRIN(_mm512_mask_extloadunpackhi_epi64) \
VECTOR_INTRIN(_mm512_mask_extloadunpackhi_pd) \
VECTOR_INTRIN(_mm512_mask_extloadunpackhi_ps) \
VECTOR_INTRIN(_mm512_mask_extloadunpacklo_epi32) \
VECTOR_INTRIN(_mm512_mask_extloadunpacklo_epi64) \
VECTOR_INTRIN(_mm512_mask_extloadunpacklo_pd) \
VECTOR_INTRIN(_mm512_mask_extloadunpacklo_ps) \
VECTOR_INTRIN(_mm512_mask_extpackstorehi_epi32) \
VECTOR_INTRIN(_mm512_mask_extpackstorehi_epi64) \
VECTOR_INTRIN(_mm512_mask_extpackstorehi_pd) \
VECTOR_INTRIN(_mm512_mask_extpackstorehi_ps) \
VECTOR_INTRIN(_mm512_mask_extpackstorelo_epi32) \
VECTOR_INTRIN(_mm512_mask_extpackstorelo_epi64) \
VECTOR_INTRIN(_mm512_mask_extpackstorelo_pd) \
VECTOR_INTRIN(_mm512_mask_extpackstorelo_ps) \
VECTOR_INTRIN(_mm512_mask_extractf32x4_ps) \
VECTOR_INTRIN(_mm512_mask_extractf32x8_ps) \
VECTOR_INTRIN(_mm512_mask_extractf64x2_pd) \
VECTOR_INTRIN(_mm512_mask_extractf64x4_pd) \
VECTOR_INTRIN(_mm512_mask_extracti32x4_epi32) \
VECTOR_INTRIN(_mm512_mask_extracti32x8_epi32) \
VECTOR_INTRIN(_mm512_mask_extracti64x2_epi64) \
VECTOR_INTRIN(_mm512_mask_extracti64x4_epi64) \
VECTOR_INTRIN(_mm512_mask_extstore_epi32) \
VECTOR_INTRIN(_mm512_mask_extstore_epi64) \
VECTOR_INTRIN(_mm512_mask_extstore_pd) \
VECTOR_INTRIN(_mm512_mask_extstore_ps) \
VECTOR_INTRIN(_mm512_mask_fixupimm_pd) \
VECTOR_INTRIN(_mm512_mask_fixupimm_ps) \
VECTOR_INTRIN(_mm512_mask_fixupimm_round_pd) \
VECTOR_INTRIN(_mm512_mask_fixupimm_round_ps) \
VECTOR_INTRIN(_mm512_mask_fixupnan_pd) \
VECTOR_INTRIN(_mm512_mask_fixupnan_ps) \
VECTOR_INTRIN(_mm512_mask_floor_pd) \
VECTOR_INTRIN(_mm512_mask_floor_ps) \
VECTOR_INTRIN(_mm512_mask_fmadd233_epi32) \
VECTOR_INTRIN(_mm512_mask_fmadd233_round_ps) \
VECTOR_INTRIN(_mm512_mask_fmadd_epi32) \
VECTOR_INTRIN(_mm512_mask_fmadd_round_pd) \
VECTOR_INTRIN(_mm512_mask_fmadd_round_ps) \
VECTOR_INTRIN(_mm512_mask_fmaddsub_round_pd) \
VECTOR_INTRIN(_mm512_mask_fmaddsub_round_ps) \
VECTOR_INTRIN(_mm512_mask_fmsubadd_round_pd) \
VECTOR_INTRIN(_mm512_mask_fmsubadd_round_ps) \
VECTOR_INTRIN(_mm512_mask_fmsub_round_pd) \
VECTOR_INTRIN(_mm512_mask_fmsub_round_ps) \
VECTOR_INTRIN(_mm512_mask_fnmadd_round_pd) \
VECTOR_INTRIN(_mm512_mask_fnmadd_round_ps) \
VECTOR_INTRIN(_mm512_mask_fnmsub_round_pd) \
VECTOR_INTRIN(_mm512_mask_fnmsub_round_ps) \
VECTOR_INTRIN(_mm512_mask_fpclass_pd_mask) \
VECTOR_INTRIN(_mm512_mask_fpclass_ps_mask) \
VECTOR_INTRIN(_mm512_mask_getexp_pd) \
VECTOR_INTRIN(_mm512_mask_getexp_ps) \
VECTOR_INTRIN(_mm512_mask_getexp_round_pd) \
VECTOR_INTRIN(_mm512_mask_getexp_round_ps) \
VECTOR_INTRIN(_mm512_mask_getmant_pd) \
VECTOR_INTRIN(_mm512_mask_getmant_ps) \
VECTOR_INTRIN(_mm512_mask_getmant_round_pd) \
VECTOR_INTRIN(_mm512_mask_getmant_round_ps) \
VECTOR_INTRIN(_mm512_mask_gmaxabs_ps) \
VECTOR_INTRIN(_mm512_mask_gmax_pd) \
VECTOR_INTRIN(_mm512_mask_gmax_ps) \
VECTOR_INTRIN(_mm512_mask_gmin_pd) \
VECTOR_INTRIN(_mm512_mask_gmin_ps) \
VECTOR_INTRIN(_mm512_mask_hypot_pd) \
VECTOR_INTRIN(_mm512_mask_hypot_ps) \
VECTOR_INTRIN(_mm512_mask_i32extgather_epi32) \
VECTOR_INTRIN(_mm512_mask_i32extgather_ps) \
VECTOR_INTRIN(_mm512_mask_i32extscatter_epi32) \
VECTOR_INTRIN(_mm512_mask_i32extscatter_ps) \
VECTOR_INTRIN(_mm512_mask_i32loextgather_epi64) \
VECTOR_INTRIN(_mm512_mask_i32loextgather_pd) \
VECTOR_INTRIN(_mm512_mask_i32loextscatter_epi64) \
VECTOR_INTRIN(_mm512_mask_i32loextscatter_pd) \
VECTOR_INTRIN(_mm512_mask_i64extgather_epi32lo) \
VECTOR_INTRIN(_mm512_mask_i64extgather_epi64) \
VECTOR_INTRIN(_mm512_mask_i64extgather_pd) \
VECTOR_INTRIN(_mm512_mask_i64extgather_pslo) \
VECTOR_INTRIN(_mm512_mask_i64extscatter_epi32lo) \
VECTOR_INTRIN(_mm512_mask_i64extscatter_epi64) \
VECTOR_INTRIN(_mm512_mask_i64extscatter_pd) \
VECTOR_INTRIN(_mm512_mask_i64extscatter_pslo) \
VECTOR_INTRIN(_mm512_mask_insertf32x4) \
VECTOR_INTRIN(_mm512_mask_insertf32x8) \
VECTOR_INTRIN(_mm512_mask_insertf64x2) \
VECTOR_INTRIN(_mm512_mask_insertf64x4) \
VECTOR_INTRIN(_mm512_mask_inserti32x4) \
VECTOR_INTRIN(_mm512_mask_inserti32x8) \
VECTOR_INTRIN(_mm512_mask_inserti64x2) \
VECTOR_INTRIN(_mm512_mask_inserti64x4) \
VECTOR_INTRIN(_mm512_mask_invsqrt_pd) \
VECTOR_INTRIN(_mm512_mask_invsqrt_ps) \
VECTOR_INTRIN(_mm512_mask_load_epi32) \
VECTOR_INTRIN(_mm512_mask_load_epi64) \
VECTOR_INTRIN(_mm512_mask_load_pd) \
VECTOR_INTRIN(_mm512_mask_load_ps) \
VECTOR_INTRIN(_mm512_mask_loadu_epi16) \
VECTOR_INTRIN(_mm512_mask_loadu_epi32) \
VECTOR_INTRIN(_mm512_mask_loadu_epi64) \
VECTOR_INTRIN(_mm512_mask_loadu_epi8) \
VECTOR_INTRIN(_mm512_mask_loadu_pd) \
VECTOR_INTRIN(_mm512_mask_loadu_ps) \
VECTOR_INTRIN(_mm512_mask_log10_pd) \
VECTOR_INTRIN(_mm512_mask_log10_ps) \
VECTOR_INTRIN(_mm512_mask_log1p_pd) \
VECTOR_INTRIN(_mm512_mask_log1p_ps) \
VECTOR_INTRIN(_mm512_mask_log2_pd) \
VECTOR_INTRIN(_mm512_mask_log2_ps) \
VECTOR_INTRIN(_mm512_mask_logb_pd) \
VECTOR_INTRIN(_mm512_mask_logb_ps) \
VECTOR_INTRIN(_mm512_mask_log_pd) \
VECTOR_INTRIN(_mm512_mask_log_ps) \
VECTOR_INTRIN(_mm512_mask_lzcnt_epi32) \
VECTOR_INTRIN(_mm512_mask_lzcnt_epi64) \
VECTOR_INTRIN(_mm512_mask_madd52hi_epu64) \
VECTOR_INTRIN(_mm512_mask_madd52lo_epu64) \
VECTOR_INTRIN(_mm512_mask_madd_epi16) \
VECTOR_INTRIN(_mm512_mask_maddubs_epi16) \
VECTOR_INTRIN(_mm512_mask_max_epi16) \
VECTOR_INTRIN(_mm512_mask_max_epi32) \
VECTOR_INTRIN(_mm512_mask_max_epi64) \
VECTOR_INTRIN(_mm512_mask_max_epi8) \
VECTOR_INTRIN(_mm512_mask_max_epu16) \
VECTOR_INTRIN(_mm512_mask_max_epu32) \
VECTOR_INTRIN(_mm512_mask_max_epu64) \
VECTOR_INTRIN(_mm512_mask_max_epu8) \
VECTOR_INTRIN(_mm512_mask_max_pd) \
VECTOR_INTRIN(_mm512_mask_max_ps) \
VECTOR_INTRIN(_mm512_mask_max_round_pd) \
VECTOR_INTRIN(_mm512_mask_max_round_ps) \
VECTOR_INTRIN(_mm512_mask_min_epi16) \
VECTOR_INTRIN(_mm512_mask_min_epi32) \
VECTOR_INTRIN(_mm512_mask_min_epi64) \
VECTOR_INTRIN(_mm512_mask_min_epi8) \
VECTOR_INTRIN(_mm512_mask_min_epu16) \
VECTOR_INTRIN(_mm512_mask_min_epu32) \
VECTOR_INTRIN(_mm512_mask_min_epu64) \
VECTOR_INTRIN(_mm512_mask_min_epu8) \
VECTOR_INTRIN(_mm512_mask_min_pd) \
VECTOR_INTRIN(_mm512_mask_min_ps) \
VECTOR_INTRIN(_mm512_mask_min_round_pd) \
VECTOR_INTRIN(_mm512_mask_min_round_ps) \
VECTOR_INTRIN(_mm512_mask_movedup_pd) \
VECTOR_INTRIN(_mm512_mask_movehdup_ps) \
VECTOR_INTRIN(_mm512_mask_moveldup_ps) \
VECTOR_INTRIN(_mm512_mask_mov_epi16) \
VECTOR_INTRIN(_mm512_mask_mov_epi8) \
VECTOR_INTRIN(_mm512_mask_mov_pd) \
VECTOR_INTRIN(_mm512_mask_mov_ps) \
VECTOR_INTRIN(_mm512_mask_mul_epi32) \
VECTOR_INTRIN(_mm512_mask_mul_epu32) \
VECTOR_INTRIN(_mm512_mask_mulhi_epi16) \
VECTOR_INTRIN(_mm512_mask_mulhi_epi32) \
VECTOR_INTRIN(_mm512_mask_mulhi_epu16) \
VECTOR_INTRIN(_mm512_mask_mulhi_epu32) \
VECTOR_INTRIN(_mm512_mask_mulhrs_epi16) \
VECTOR_INTRIN(_mm512_mask_mullo_epi16) \
VECTOR_INTRIN(_mm512_mask_mullo_epi32) \
VECTOR_INTRIN(_mm512_mask_mullo_epi64) \
VECTOR_INTRIN(_mm512_mask_mullox_epi64) \
VECTOR_INTRIN(_mm512_mask_mul_pd) \
VECTOR_INTRIN(_mm512_mask_mul_ps) \
VECTOR_INTRIN(_mm512_mask_mul_round_pd) \
VECTOR_INTRIN(_mm512_mask_mul_round_ps) \
VECTOR_INTRIN(_mm512_mask_multishift_epi64_epi8) \
VECTOR_INTRIN(_mm512_mask_nearbyint_pd) \
VECTOR_INTRIN(_mm512_mask_nearbyint_ps) \
VECTOR_INTRIN(_mm512_mask_or_epi32) \
VECTOR_INTRIN(_mm512_mask_or_epi64) \
VECTOR_INTRIN(_mm512_mask_or_pd) \
VECTOR_INTRIN(_mm512_mask_or_ps) \
VECTOR_INTRIN(_mm512_mask_packs_epi16) \
VECTOR_INTRIN(_mm512_mask_packs_epi32) \
VECTOR_INTRIN(_mm512_mask_packus_epi16) \
VECTOR_INTRIN(_mm512_mask_packus_epi32) \
VECTOR_INTRIN(_mm512_mask_permute4f128_epi32) \
VECTOR_INTRIN(_mm512_mask_permute4f128_ps) \
VECTOR_INTRIN(_mm512_mask_permute_pd) \
VECTOR_INTRIN(_mm512_mask_permute_ps) \
VECTOR_INTRIN(_mm512_mask_permutevar_epi32) \
VECTOR_INTRIN(_mm512_mask_permutevar_pd) \
VECTOR_INTRIN(_mm512_mask_permutevar_ps) \
VECTOR_INTRIN(_mm512_mask_permutex2var_epi16) \
VECTOR_INTRIN(_mm512_mask_permutex2var_epi32) \
VECTOR_INTRIN(_mm512_mask_permutex2var_epi64) \
VECTOR_INTRIN(_mm512_mask_permutex2var_epi8) \
VECTOR_INTRIN(_mm512_mask_permutex2var_pd) \
VECTOR_INTRIN(_mm512_mask_permutex2var_ps) \
VECTOR_INTRIN(_mm512_mask_permutex_epi64) \
VECTOR_INTRIN(_mm512_mask_permutex_pd) \
VECTOR_INTRIN(_mm512_mask_permutexvar_epi16) \
VECTOR_INTRIN(_mm512_mask_permutexvar_epi32) \
VECTOR_INTRIN(_mm512_mask_permutexvar_epi64) \
VECTOR_INTRIN(_mm512_mask_permutexvar_epi8) \
VECTOR_INTRIN(_mm512_mask_permutexvar_pd) \
VECTOR_INTRIN(_mm512_mask_permutexvar_ps) \
VECTOR_INTRIN(_mm512_mask_pow_pd) \
VECTOR_INTRIN(_mm512_mask_pow_ps) \
VECTOR_INTRIN(_mm512_mask_prefetch_i32extgather_ps) \
VECTOR_INTRIN(_mm512_mask_prefetch_i32extscatter_ps) \
VECTOR_INTRIN(_mm512_mask_prefetch_i32gather_pd) \
VECTOR_INTRIN(_mm512_mask_prefetch_i32scatter_pd) \
VECTOR_INTRIN(_mm512_mask_prefetch_i64gather_pd) \
VECTOR_INTRIN(_mm512_mask_prefetch_i64gather_ps) \
VECTOR_INTRIN(_mm512_mask_prefetch_i64scatter_pd) \
VECTOR_INTRIN(_mm512_mask_prefetch_i64scatter_ps) \
VECTOR_INTRIN(_mm512_mask_range_pd) \
VECTOR_INTRIN(_mm512_mask_range_ps) \
VECTOR_INTRIN(_mm512_mask_range_round_pd) \
VECTOR_INTRIN(_mm512_mask_range_round_ps) \
VECTOR_INTRIN(_mm512_mask_rcp14_pd) \
VECTOR_INTRIN(_mm512_mask_rcp14_ps) \
VECTOR_INTRIN(_mm512_mask_rcp28_round_pd) \
VECTOR_INTRIN(_mm512_mask_rcp28_round_ps) \
VECTOR_INTRIN(_mm512_mask_recip_pd) \
VECTOR_INTRIN(_mm512_mask_recip_ps) \
VECTOR_INTRIN(_mm512_mask_reduce_add_epi32) \
VECTOR_INTRIN(_mm512_mask_reduce_add_epi64) \
VECTOR_INTRIN(_mm512_mask_reduce_add_pd) \
VECTOR_INTRIN(_mm512_mask_reduce_add_ps) \
VECTOR_INTRIN(_mm512_mask_reduce_and_epi32) \
VECTOR_INTRIN(_mm512_mask_reduce_and_epi64) \
VECTOR_INTRIN(_mm512_mask_reduce_gmax_pd) \
VECTOR_INTRIN(_mm512_mask_reduce_gmax_ps) \
VECTOR_INTRIN(_mm512_mask_reduce_gmin_pd) \
VECTOR_INTRIN(_mm512_mask_reduce_gmin_ps) \
VECTOR_INTRIN(_mm512_mask_reduce_max_epi32) \
VECTOR_INTRIN(_mm512_mask_reduce_max_epi64) \
VECTOR_INTRIN(_mm512_mask_reduce_max_epu32) \
VECTOR_INTRIN(_mm512_mask_reduce_max_epu64) \
VECTOR_INTRIN(_mm512_mask_reduce_max_pd) \
VECTOR_INTRIN(_mm512_mask_reduce_max_ps) \
VECTOR_INTRIN(_mm512_mask_reduce_min_epi32) \
VECTOR_INTRIN(_mm512_mask_reduce_min_epi64) \
VECTOR_INTRIN(_mm512_mask_reduce_min_epu32) \
VECTOR_INTRIN(_mm512_mask_reduce_min_epu64) \
VECTOR_INTRIN(_mm512_mask_reduce_min_pd) \
VECTOR_INTRIN(_mm512_mask_reduce_min_ps) \
VECTOR_INTRIN(_mm512_mask_reduce_mul_epi32) \
VECTOR_INTRIN(_mm512_mask_reduce_mul_epi64) \
VECTOR_INTRIN(_mm512_mask_reduce_mul_pd) \
VECTOR_INTRIN(_mm512_mask_reduce_mul_ps) \
VECTOR_INTRIN(_mm512_mask_reduce_or_epi32) \
VECTOR_INTRIN(_mm512_mask_reduce_or_epi64) \
VECTOR_INTRIN(_mm512_mask_reduce_pd) \
VECTOR_INTRIN(_mm512_mask_reduce_ps) \
VECTOR_INTRIN(_mm512_mask_reduce_round_pd) \
VECTOR_INTRIN(_mm512_mask_reduce_round_ps) \
VECTOR_INTRIN(_mm512_mask_rem_epi32) \
VECTOR_INTRIN(_mm512_mask_rem_epu32) \
VECTOR_INTRIN(_mm512_mask_rint_pd) \
VECTOR_INTRIN(_mm512_mask_rint_ps) \
VECTOR_INTRIN(_mm512_mask_rol_epi32) \
VECTOR_INTRIN(_mm512_mask_rol_epi64) \
VECTOR_INTRIN(_mm512_mask_rolv_epi32) \
VECTOR_INTRIN(_mm512_mask_rolv_epi64) \
VECTOR_INTRIN(_mm512_mask_ror_epi32) \
VECTOR_INTRIN(_mm512_mask_ror_epi64) \
VECTOR_INTRIN(_mm512_mask_rorv_epi32) \
VECTOR_INTRIN(_mm512_mask_rorv_epi64) \
VECTOR_INTRIN(_mm512_mask_roundscale_pd) \
VECTOR_INTRIN(_mm512_mask_roundscale_ps) \
VECTOR_INTRIN(_mm512_mask_roundscale_round_pd) \
VECTOR_INTRIN(_mm512_mask_roundscale_round_ps) \
VECTOR_INTRIN(_mm512_mask_rsqrt14_pd) \
VECTOR_INTRIN(_mm512_mask_rsqrt14_ps) \
VECTOR_INTRIN(_mm512_mask_rsqrt28_round_pd) \
VECTOR_INTRIN(_mm512_mask_rsqrt28_round_ps) \
VECTOR_INTRIN(_mm512_mask_scalef_round_pd) \
VECTOR_INTRIN(_mm512_mask_scalef_round_ps) \
VECTOR_INTRIN(_mm512_mask_scale_ps) \
VECTOR_INTRIN(_mm512_mask_scale_round_ps) \
VECTOR_INTRIN(_mm512_mask_set1_epi16) \
VECTOR_INTRIN(_mm512_mask_set1_epi32) \
VECTOR_INTRIN(_mm512_mask_set1_epi64) \
VECTOR_INTRIN(_mm512_mask_set1_epi8) \
VECTOR_INTRIN(_mm512_mask_shuffle_epi32) \
VECTOR_INTRIN(_mm512_mask_shuffle_epi8) \
VECTOR_INTRIN(_mm512_mask_shuffle_f32x4) \
VECTOR_INTRIN(_mm512_mask_shuffle_f64x2) \
VECTOR_INTRIN(_mm512_mask_shufflehi_epi16) \
VECTOR_INTRIN(_mm512_mask_shuffle_i32x4) \
VECTOR_INTRIN(_mm512_mask_shuffle_i64x2) \
VECTOR_INTRIN(_mm512_mask_shufflelo_epi16) \
VECTOR_INTRIN(_mm512_mask_shuffle_pd) \
VECTOR_INTRIN(_mm512_mask_shuffle_ps) \
VECTOR_INTRIN(_mm512_mask_sincos_pd) \
VECTOR_INTRIN(_mm512_mask_sincos_ps) \
VECTOR_INTRIN(_mm512_mask_sind_pd) \
VECTOR_INTRIN(_mm512_mask_sind_ps) \
VECTOR_INTRIN(_mm512_mask_sinh_pd) \
VECTOR_INTRIN(_mm512_mask_sinh_ps) \
VECTOR_INTRIN(_mm512_mask_sin_pd) \
VECTOR_INTRIN(_mm512_mask_sin_ps) \
VECTOR_INTRIN(_mm512_mask_sll_epi16) \
VECTOR_INTRIN(_mm512_mask_sll_epi32) \
VECTOR_INTRIN(_mm512_mask_sll_epi64) \
VECTOR_INTRIN(_mm512_mask_slli_epi16) \
VECTOR_INTRIN(_mm512_mask_slli_epi32) \
VECTOR_INTRIN(_mm512_mask_slli_epi64) \
VECTOR_INTRIN(_mm512_mask_sllv_epi16) \
VECTOR_INTRIN(_mm512_mask_sllv_epi32) \
VECTOR_INTRIN(_mm512_mask_sllv_epi64) \
VECTOR_INTRIN(_mm512_mask_sqrt_pd) \
VECTOR_INTRIN(_mm512_mask_sqrt_ps) \
VECTOR_INTRIN(_mm512_mask_sqrt_round_pd) \
VECTOR_INTRIN(_mm512_mask_sqrt_round_ps) \
VECTOR_INTRIN(_mm512_mask_sra_epi16) \
VECTOR_INTRIN(_mm512_mask_sra_epi32) \
VECTOR_INTRIN(_mm512_mask_sra_epi64) \
VECTOR_INTRIN(_mm512_mask_srai_epi16) \
VECTOR_INTRIN(_mm512_mask_srai_epi32) \
VECTOR_INTRIN(_mm512_mask_srai_epi64) \
VECTOR_INTRIN(_mm512_mask_srav_epi16) \
VECTOR_INTRIN(_mm512_mask_srav_epi32) \
VECTOR_INTRIN(_mm512_mask_srav_epi64) \
VECTOR_INTRIN(_mm512_mask_srl_epi16) \
VECTOR_INTRIN(_mm512_mask_srl_epi32) \
VECTOR_INTRIN(_mm512_mask_srl_epi64) \
VECTOR_INTRIN(_mm512_mask_srli_epi16) \
VECTOR_INTRIN(_mm512_mask_srli_epi32) \
VECTOR_INTRIN(_mm512_mask_srli_epi64) \
VECTOR_INTRIN(_mm512_mask_srlv_epi16) \
VECTOR_INTRIN(_mm512_mask_srlv_epi32) \
VECTOR_INTRIN(_mm512_mask_srlv_epi64) \
VECTOR_INTRIN(_mm512_mask_store_epi32) \
VECTOR_INTRIN(_mm512_mask_store_epi64) \
VECTOR_INTRIN(_mm512_mask_store_pd) \
VECTOR_INTRIN(_mm512_mask_store_ps) \
VECTOR_INTRIN(_mm512_mask_storeu_epi16) \
VECTOR_INTRIN(_mm512_mask_storeu_epi32) \
VECTOR_INTRIN(_mm512_mask_storeu_epi64) \
VECTOR_INTRIN(_mm512_mask_storeu_epi8) \
VECTOR_INTRIN(_mm512_mask_storeu_pd) \
VECTOR_INTRIN(_mm512_mask_storeu_ps) \
VECTOR_INTRIN(_mm512_mask_sub_epi16) \
VECTOR_INTRIN(_mm512_mask_sub_epi32) \
VECTOR_INTRIN(_mm512_mask_sub_epi64) \
VECTOR_INTRIN(_mm512_mask_sub_epi8) \
VECTOR_INTRIN(_mm512_mask_sub_pd) \
VECTOR_INTRIN(_mm512_mask_sub_ps) \
VECTOR_INTRIN(_mm512_mask_subr_epi32) \
VECTOR_INTRIN(_mm512_mask_sub_round_pd) \
VECTOR_INTRIN(_mm512_mask_sub_round_ps) \
VECTOR_INTRIN(_mm512_mask_subr_pd) \
VECTOR_INTRIN(_mm512_mask_subr_ps) \
VECTOR_INTRIN(_mm512_mask_subr_round_pd) \
VECTOR_INTRIN(_mm512_mask_subr_round_ps) \
VECTOR_INTRIN(_mm512_mask_subs_epi16) \
VECTOR_INTRIN(_mm512_mask_subs_epi8) \
VECTOR_INTRIN(_mm512_mask_subs_epu16) \
VECTOR_INTRIN(_mm512_mask_subs_epu8) \
VECTOR_INTRIN(_mm512_mask_svml_round_pd) \
VECTOR_INTRIN(_mm512_mask_swizzle_epi32) \
VECTOR_INTRIN(_mm512_mask_swizzle_epi64) \
VECTOR_INTRIN(_mm512_mask_swizzle_pd) \
VECTOR_INTRIN(_mm512_mask_swizzle_ps) \
VECTOR_INTRIN(_mm512_mask_tand_pd) \
VECTOR_INTRIN(_mm512_mask_tand_ps) \
VECTOR_INTRIN(_mm512_mask_tanh_pd) \
VECTOR_INTRIN(_mm512_mask_tanh_ps) \
VECTOR_INTRIN(_mm512_mask_tan_pd) \
VECTOR_INTRIN(_mm512_mask_tan_ps) \
VECTOR_INTRIN(_mm512_mask_ternarylogic_epi32) \
VECTOR_INTRIN(_mm512_mask_ternarylogic_epi64) \
VECTOR_INTRIN(_mm512_mask_test_epi16_mask) \
VECTOR_INTRIN(_mm512_mask_test_epi32_mask) \
VECTOR_INTRIN(_mm512_mask_test_epi64_mask) \
VECTOR_INTRIN(_mm512_mask_test_epi8_mask) \
VECTOR_INTRIN(_mm512_mask_testn_epi16_mask) \
VECTOR_INTRIN(_mm512_mask_testn_epi32_mask) \
VECTOR_INTRIN(_mm512_mask_testn_epi64_mask) \
VECTOR_INTRIN(_mm512_mask_testn_epi8_mask) \
VECTOR_INTRIN(_mm512_mask_trunc_pd) \
VECTOR_INTRIN(_mm512_mask_trunc_ps) \
VECTOR_INTRIN(_mm512_mask_unpackhi_epi16) \
VECTOR_INTRIN(_mm512_mask_unpackhi_epi32) \
VECTOR_INTRIN(_mm512_mask_unpackhi_epi64) \
VECTOR_INTRIN(_mm512_mask_unpackhi_epi8) \
VECTOR_INTRIN(_mm512_mask_unpackhi_pd) \
VECTOR_INTRIN(_mm512_mask_unpackhi_ps) \
VECTOR_INTRIN(_mm512_mask_unpacklo_epi16) \
VECTOR_INTRIN(_mm512_mask_unpacklo_epi32) \
VECTOR_INTRIN(_mm512_mask_unpacklo_epi64) \
VECTOR_INTRIN(_mm512_mask_unpacklo_epi8) \
VECTOR_INTRIN(_mm512_mask_unpacklo_pd) \
VECTOR_INTRIN(_mm512_mask_unpacklo_ps) \
VECTOR_INTRIN(_mm512_mask_xor_epi32) \
VECTOR_INTRIN(_mm512_mask_xor_epi64) \
VECTOR_INTRIN(_mm512_mask_xor_pd) \
VECTOR_INTRIN(_mm512_mask_xor_ps) \
VECTOR_INTRIN(_mm512_maskz_abs_epi16) \
VECTOR_INTRIN(_mm512_maskz_abs_epi32) \
VECTOR_INTRIN(_mm512_maskz_abs_epi64) \
VECTOR_INTRIN(_mm512_maskz_abs_epi8) \
VECTOR_INTRIN(_mm512_maskz_add_epi16) \
VECTOR_INTRIN(_mm512_maskz_add_epi32) \
VECTOR_INTRIN(_mm512_maskz_add_epi64) \
VECTOR_INTRIN(_mm512_maskz_add_epi8) \
VECTOR_INTRIN(_mm512_maskz_add_round_pd) \
VECTOR_INTRIN(_mm512_maskz_add_round_ps) \
VECTOR_INTRIN(_mm512_maskz_adds_epi16) \
VECTOR_INTRIN(_mm512_maskz_adds_epi8) \
VECTOR_INTRIN(_mm512_maskz_adds_epu16) \
VECTOR_INTRIN(_mm512_maskz_adds_epu8) \
VECTOR_INTRIN(_mm512_maskz_alignr_epi32) \
VECTOR_INTRIN(_mm512_maskz_alignr_epi64) \
VECTOR_INTRIN(_mm512_maskz_alignr_epi8) \
VECTOR_INTRIN(_mm512_maskz_and_epi32) \
VECTOR_INTRIN(_mm512_maskz_and_epi64) \
VECTOR_INTRIN(_mm512_maskz_andnot_epi32) \
VECTOR_INTRIN(_mm512_maskz_andnot_epi64) \
VECTOR_INTRIN(_mm512_maskz_andnot_pd) \
VECTOR_INTRIN(_mm512_maskz_andnot_ps) \
VECTOR_INTRIN(_mm512_maskz_and_pd) \
VECTOR_INTRIN(_mm512_maskz_and_ps) \
VECTOR_INTRIN(_mm512_maskz_avg_epu16) \
VECTOR_INTRIN(_mm512_maskz_avg_epu8) \
VECTOR_INTRIN(_mm512_maskz_broadcastb_epi8) \
VECTOR_INTRIN(_mm512_maskz_broadcastd_epi32) \
VECTOR_INTRIN(_mm512_maskz_broadcast_f32x2) \
VECTOR_INTRIN(_mm512_maskz_broadcast_f32x4) \
VECTOR_INTRIN(_mm512_maskz_broadcast_f32x8) \
VECTOR_INTRIN(_mm512_maskz_broadcast_f64x2) \
VECTOR_INTRIN(_mm512_maskz_broadcast_f64x4) \
VECTOR_INTRIN(_mm512_maskz_broadcast_i32x2) \
VECTOR_INTRIN(_mm512_maskz_broadcast_i32x4) \
VECTOR_INTRIN(_mm512_maskz_broadcast_i32x8) \
VECTOR_INTRIN(_mm512_maskz_broadcast_i64x2) \
VECTOR_INTRIN(_mm512_maskz_broadcast_i64x4) \
VECTOR_INTRIN(_mm512_maskz_broadcastq_epi64) \
VECTOR_INTRIN(_mm512_maskz_broadcastsd_pd) \
VECTOR_INTRIN(_mm512_maskz_broadcastss_ps) \
VECTOR_INTRIN(_mm512_maskz_broadcastw_epi16) \
VECTOR_INTRIN(_mm512_maskz_compress_epi32) \
VECTOR_INTRIN(_mm512_maskz_compress_epi64) \
VECTOR_INTRIN(_mm512_maskz_compress_pd) \
VECTOR_INTRIN(_mm512_maskz_compress_ps) \
VECTOR_INTRIN(_mm512_maskz_conflict_epi32) \
VECTOR_INTRIN(_mm512_maskz_conflict_epi64) \
VECTOR_INTRIN(_mm512_maskz_cvtepi16_epi32) \
VECTOR_INTRIN(_mm512_maskz_cvtepi16_epi64) \
VECTOR_INTRIN(_mm512_maskz_cvtepi16_epi8) \
VECTOR_INTRIN(_mm512_maskz_cvtepi32_epi16) \
VECTOR_INTRIN(_mm512_maskz_cvtepi32_epi64) \
VECTOR_INTRIN(_mm512_maskz_cvtepi32_epi8) \
VECTOR_INTRIN(_mm512_maskz_cvtepi32_pd) \
VECTOR_INTRIN(_mm512_maskz_cvtepi64_epi16) \
VECTOR_INTRIN(_mm512_maskz_cvtepi64_epi32) \
VECTOR_INTRIN(_mm512_maskz_cvtepi64_epi8) \
VECTOR_INTRIN(_mm512_maskz_cvtepi8_epi16) \
VECTOR_INTRIN(_mm512_maskz_cvtepi8_epi32) \
VECTOR_INTRIN(_mm512_maskz_cvtepi8_epi64) \
VECTOR_INTRIN(_mm512_maskz_cvtepu16_epi32) \
VECTOR_INTRIN(_mm512_maskz_cvtepu16_epi64) \
VECTOR_INTRIN(_mm512_maskz_cvtepu32_epi64) \
VECTOR_INTRIN(_mm512_maskz_cvtepu32_pd) \
VECTOR_INTRIN(_mm512_maskz_cvtepu8_epi16) \
VECTOR_INTRIN(_mm512_maskz_cvtepu8_epi32) \
VECTOR_INTRIN(_mm512_maskz_cvtepu8_epi64) \
VECTOR_INTRIN(_mm512_maskz_cvt_roundepi32_ps) \
VECTOR_INTRIN(_mm512_maskz_cvt_roundepi64_pd) \
VECTOR_INTRIN(_mm512_maskz_cvt_roundepi64_ps) \
VECTOR_INTRIN(_mm512_maskz_cvt_roundepu32_ps) \
VECTOR_INTRIN(_mm512_maskz_cvt_roundepu64_pd) \
VECTOR_INTRIN(_mm512_maskz_cvt_roundepu64_ps) \
VECTOR_INTRIN(_mm512_maskz_cvt_roundpd_epi32) \
VECTOR_INTRIN(_mm512_maskz_cvt_roundpd_epi64) \
VECTOR_INTRIN(_mm512_maskz_cvt_roundpd_epu32) \
VECTOR_INTRIN(_mm512_maskz_cvt_roundpd_epu64) \
VECTOR_INTRIN(_mm512_maskz_cvt_roundpd_ps) \
VECTOR_INTRIN(_mm512_maskz_cvt_roundph_ps) \
VECTOR_INTRIN(_mm512_maskz_cvt_roundps_epi32) \
VECTOR_INTRIN(_mm512_maskz_cvt_roundps_epi64) \
VECTOR_INTRIN(_mm512_maskz_cvt_roundps_epu32) \
VECTOR_INTRIN(_mm512_maskz_cvt_roundps_epu64) \
VECTOR_INTRIN(_mm512_maskz_cvt_roundps_pd) \
VECTOR_INTRIN(_mm512_maskz_cvt_roundps_ph) \
VECTOR_INTRIN(_mm512_maskz_cvtsepi16_epi8) \
VECTOR_INTRIN(_mm512_maskz_cvtsepi32_epi16) \
VECTOR_INTRIN(_mm512_maskz_cvtsepi32_epi8) \
VECTOR_INTRIN(_mm512_maskz_cvtsepi64_epi16) \
VECTOR_INTRIN(_mm512_maskz_cvtsepi64_epi32) \
VECTOR_INTRIN(_mm512_maskz_cvtsepi64_epi8) \
VECTOR_INTRIN(_mm512_maskz_cvtt_roundpd_epi32) \
VECTOR_INTRIN(_mm512_maskz_cvtt_roundpd_epi64) \
VECTOR_INTRIN(_mm512_maskz_cvtt_roundpd_epu32) \
VECTOR_INTRIN(_mm512_maskz_cvtt_roundpd_epu64) \
VECTOR_INTRIN(_mm512_maskz_cvtt_roundps_epi32) \
VECTOR_INTRIN(_mm512_maskz_cvtt_roundps_epi64) \
VECTOR_INTRIN(_mm512_maskz_cvtt_roundps_epu32) \
VECTOR_INTRIN(_mm512_maskz_cvtt_roundps_epu64) \
VECTOR_INTRIN(_mm512_maskz_cvtusepi16_epi8) \
VECTOR_INTRIN(_mm512_maskz_cvtusepi32_epi16) \
VECTOR_INTRIN(_mm512_maskz_cvtusepi32_epi8) \
VECTOR_INTRIN(_mm512_maskz_cvtusepi64_epi16) \
VECTOR_INTRIN(_mm512_maskz_cvtusepi64_epi32) \
VECTOR_INTRIN(_mm512_maskz_cvtusepi64_epi8) \
VECTOR_INTRIN(_mm512_maskz_dbsad_epu8) \
VECTOR_INTRIN(_mm512_maskz_div_round_pd) \
VECTOR_INTRIN(_mm512_maskz_div_round_ps) \
VECTOR_INTRIN(_mm512_maskz_exp2a23_round_pd) \
VECTOR_INTRIN(_mm512_maskz_exp2a23_round_ps) \
VECTOR_INTRIN(_mm512_maskz_expand_epi32) \
VECTOR_INTRIN(_mm512_maskz_expand_epi64) \
VECTOR_INTRIN(_mm512_maskz_expandloadu_epi32) \
VECTOR_INTRIN(_mm512_maskz_expandloadu_epi64) \
VECTOR_INTRIN(_mm512_maskz_expandloadu_pd) \
VECTOR_INTRIN(_mm512_maskz_expandloadu_ps) \
VECTOR_INTRIN(_mm512_maskz_expand_pd) \
VECTOR_INTRIN(_mm512_maskz_expand_ps) \
VECTOR_INTRIN(_mm512_maskz_extractf32x4_ps) \
VECTOR_INTRIN(_mm512_maskz_extractf32x8_ps) \
VECTOR_INTRIN(_mm512_maskz_extractf64x2_pd) \
VECTOR_INTRIN(_mm512_maskz_extractf64x4_pd) \
VECTOR_INTRIN(_mm512_maskz_extracti32x4_epi32) \
VECTOR_INTRIN(_mm512_maskz_extracti32x8_epi32) \
VECTOR_INTRIN(_mm512_maskz_extracti64x2_epi64) \
VECTOR_INTRIN(_mm512_maskz_extracti64x4_epi64) \
VECTOR_INTRIN(_mm512_maskz_fixupimm_pd) \
VECTOR_INTRIN(_mm512_maskz_fixupimm_ps) \
VECTOR_INTRIN(_mm512_maskz_fixupimm_round_pd) \
VECTOR_INTRIN(_mm512_maskz_fixupimm_round_ps) \
VECTOR_INTRIN(_mm512_maskz_fmadd_round_pd) \
VECTOR_INTRIN(_mm512_maskz_fmadd_round_ps) \
VECTOR_INTRIN(_mm512_maskz_fmaddsub_round_pd) \
VECTOR_INTRIN(_mm512_maskz_fmaddsub_round_ps) \
VECTOR_INTRIN(_mm512_maskz_fmsubadd_round_pd) \
VECTOR_INTRIN(_mm512_maskz_fmsubadd_round_ps) \
VECTOR_INTRIN(_mm512_maskz_fmsub_round_pd) \
VECTOR_INTRIN(_mm512_maskz_fmsub_round_ps) \
VECTOR_INTRIN(_mm512_maskz_fnmadd_round_pd) \
VECTOR_INTRIN(_mm512_maskz_fnmadd_round_ps) \
VECTOR_INTRIN(_mm512_maskz_fnmsub_round_pd) \
VECTOR_INTRIN(_mm512_maskz_fnmsub_round_ps) \
VECTOR_INTRIN(_mm512_maskz_getexp_round_pd) \
VECTOR_INTRIN(_mm512_maskz_getexp_round_ps) \
VECTOR_INTRIN(_mm512_maskz_getmant_round_pd) \
VECTOR_INTRIN(_mm512_maskz_getmant_round_ps) \
VECTOR_INTRIN(_mm512_maskz_insertf32x4) \
VECTOR_INTRIN(_mm512_maskz_insertf32x8) \
VECTOR_INTRIN(_mm512_maskz_insertf64x2) \
VECTOR_INTRIN(_mm512_maskz_insertf64x4) \
VECTOR_INTRIN(_mm512_maskz_inserti32x4) \
VECTOR_INTRIN(_mm512_maskz_inserti32x8) \
VECTOR_INTRIN(_mm512_maskz_inserti64x2) \
VECTOR_INTRIN(_mm512_maskz_inserti64x4) \
VECTOR_INTRIN(_mm512_maskz_load_epi32) \
VECTOR_INTRIN(_mm512_maskz_load_epi64) \
VECTOR_INTRIN(_mm512_maskz_load_pd) \
VECTOR_INTRIN(_mm512_maskz_load_ps) \
VECTOR_INTRIN(_mm512_maskz_loadu_epi16) \
VECTOR_INTRIN(_mm512_maskz_loadu_epi32) \
VECTOR_INTRIN(_mm512_maskz_loadu_epi64) \
VECTOR_INTRIN(_mm512_maskz_loadu_epi8) \
VECTOR_INTRIN(_mm512_maskz_loadu_pd) \
VECTOR_INTRIN(_mm512_maskz_loadu_ps) \
VECTOR_INTRIN(_mm512_maskz_lzcnt_epi32) \
VECTOR_INTRIN(_mm512_maskz_lzcnt_epi64) \
VECTOR_INTRIN(_mm512_maskz_madd52hi_epu64) \
VECTOR_INTRIN(_mm512_maskz_madd52lo_epu64) \
VECTOR_INTRIN(_mm512_maskz_madd_epi16) \
VECTOR_INTRIN(_mm512_maskz_maddubs_epi16) \
VECTOR_INTRIN(_mm512_maskz_max_epi16) \
VECTOR_INTRIN(_mm512_maskz_max_epi32) \
VECTOR_INTRIN(_mm512_maskz_max_epi64) \
VECTOR_INTRIN(_mm512_maskz_max_epi8) \
VECTOR_INTRIN(_mm512_maskz_max_epu16) \
VECTOR_INTRIN(_mm512_maskz_max_epu32) \
VECTOR_INTRIN(_mm512_maskz_max_epu64) \
VECTOR_INTRIN(_mm512_maskz_max_epu8) \
VECTOR_INTRIN(_mm512_maskz_max_round_pd) \
VECTOR_INTRIN(_mm512_maskz_max_round_ps) \
VECTOR_INTRIN(_mm512_maskz_min_epi16) \
VECTOR_INTRIN(_mm512_maskz_min_epi32) \
VECTOR_INTRIN(_mm512_maskz_min_epi64) \
VECTOR_INTRIN(_mm512_maskz_min_epi8) \
VECTOR_INTRIN(_mm512_maskz_min_epu16) \
VECTOR_INTRIN(_mm512_maskz_min_epu32) \
VECTOR_INTRIN(_mm512_maskz_min_epu64) \
VECTOR_INTRIN(_mm512_maskz_min_epu8) \
VECTOR_INTRIN(_mm512_maskz_min_round_pd) \
VECTOR_INTRIN(_mm512_maskz_min_round_ps) \
VECTOR_INTRIN(_mm512_maskz_movedup_pd) \
VECTOR_INTRIN(_mm512_maskz_movehdup_ps) \
VECTOR_INTRIN(_mm512_maskz_moveldup_ps) \
VECTOR_INTRIN(_mm512_maskz_mov_epi16) \
VECTOR_INTRIN(_mm512_maskz_mov_epi32) \
VECTOR_INTRIN(_mm512_maskz_mov_epi64) \
VECTOR_INTRIN(_mm512_maskz_mov_epi8) \
VECTOR_INTRIN(_mm512_maskz_mov_pd) \
VECTOR_INTRIN(_mm512_maskz_mov_ps) \
VECTOR_INTRIN(_mm512_maskz_mul_epi32) \
VECTOR_INTRIN(_mm512_maskz_mul_epu32) \
VECTOR_INTRIN(_mm512_maskz_mulhi_epi16) \
VECTOR_INTRIN(_mm512_maskz_mulhi_epu16) \
VECTOR_INTRIN(_mm512_maskz_mulhrs_epi16) \
VECTOR_INTRIN(_mm512_maskz_mullo_epi16) \
VECTOR_INTRIN(_mm512_maskz_mullo_epi32) \
VECTOR_INTRIN(_mm512_maskz_mullo_epi64) \
VECTOR_INTRIN(_mm512_maskz_mul_round_pd) \
VECTOR_INTRIN(_mm512_maskz_mul_round_ps) \
VECTOR_INTRIN(_mm512_maskz_multishift_epi64_epi8) \
VECTOR_INTRIN(_mm512_maskz_or_epi32) \
VECTOR_INTRIN(_mm512_maskz_or_epi64) \
VECTOR_INTRIN(_mm512_maskz_or_pd) \
VECTOR_INTRIN(_mm512_maskz_or_ps) \
VECTOR_INTRIN(_mm512_maskz_packs_epi16) \
VECTOR_INTRIN(_mm512_maskz_packs_epi32) \
VECTOR_INTRIN(_mm512_maskz_packus_epi16) \
VECTOR_INTRIN(_mm512_maskz_packus_epi32) \
VECTOR_INTRIN(_mm512_maskz_permute_pd) \
VECTOR_INTRIN(_mm512_maskz_permute_ps) \
VECTOR_INTRIN(_mm512_maskz_permutevar_pd) \
VECTOR_INTRIN(_mm512_maskz_permutevar_ps) \
VECTOR_INTRIN(_mm512_maskz_permutex2var_epi16) \
VECTOR_INTRIN(_mm512_maskz_permutex2var_epi32) \
VECTOR_INTRIN(_mm512_maskz_permutex2var_epi64) \
VECTOR_INTRIN(_mm512_maskz_permutex2var_epi8) \
VECTOR_INTRIN(_mm512_maskz_permutex2var_pd) \
VECTOR_INTRIN(_mm512_maskz_permutex2var_ps) \
VECTOR_INTRIN(_mm512_maskz_permutex_epi64) \
VECTOR_INTRIN(_mm512_maskz_permutex_pd) \
VECTOR_INTRIN(_mm512_maskz_permutexvar_epi16) \
VECTOR_INTRIN(_mm512_maskz_permutexvar_epi32) \
VECTOR_INTRIN(_mm512_maskz_permutexvar_epi64) \
VECTOR_INTRIN(_mm512_maskz_permutexvar_epi8) \
VECTOR_INTRIN(_mm512_maskz_permutexvar_pd) \
VECTOR_INTRIN(_mm512_maskz_permutexvar_ps) \
VECTOR_INTRIN(_mm512_maskz_range_pd) \
VECTOR_INTRIN(_mm512_maskz_range_ps) \
VECTOR_INTRIN(_mm512_maskz_range_round_pd) \
VECTOR_INTRIN(_mm512_maskz_range_round_ps) \
VECTOR_INTRIN(_mm512_maskz_rcp14_pd) \
VECTOR_INTRIN(_mm512_maskz_rcp14_ps) \
VECTOR_INTRIN(_mm512_maskz_rcp28_round_pd) \
VECTOR_INTRIN(_mm512_maskz_rcp28_round_ps) \
VECTOR_INTRIN(_mm512_maskz_reduce_pd) \
VECTOR_INTRIN(_mm512_maskz_reduce_ps) \
VECTOR_INTRIN(_mm512_maskz_reduce_round_pd) \
VECTOR_INTRIN(_mm512_maskz_reduce_round_ps) \
VECTOR_INTRIN(_mm512_maskz_rol_epi32) \
VECTOR_INTRIN(_mm512_maskz_rol_epi64) \
VECTOR_INTRIN(_mm512_maskz_rolv_epi32) \
VECTOR_INTRIN(_mm512_maskz_rolv_epi64) \
VECTOR_INTRIN(_mm512_maskz_ror_epi32) \
VECTOR_INTRIN(_mm512_maskz_ror_epi64) \
VECTOR_INTRIN(_mm512_maskz_rorv_epi32) \
VECTOR_INTRIN(_mm512_maskz_rorv_epi64) \
VECTOR_INTRIN(_mm512_maskz_roundscale_pd) \
VECTOR_INTRIN(_mm512_maskz_roundscale_ps) \
VECTOR_INTRIN(_mm512_maskz_roundscale_round_pd) \
VECTOR_INTRIN(_mm512_maskz_roundscale_round_ps) \
VECTOR_INTRIN(_mm512_maskz_rsqrt14_pd) \
VECTOR_INTRIN(_mm512_maskz_rsqrt14_ps) \
VECTOR_INTRIN(_mm512_maskz_rsqrt28_round_pd) \
VECTOR_INTRIN(_mm512_maskz_rsqrt28_round_ps) \
VECTOR_INTRIN(_mm512_maskz_scalef_round_pd) \
VECTOR_INTRIN(_mm512_maskz_scalef_round_ps) \
VECTOR_INTRIN(_mm512_maskz_set1_epi16) \
VECTOR_INTRIN(_mm512_maskz_set1_epi32) \
VECTOR_INTRIN(_mm512_maskz_set1_epi64) \
VECTOR_INTRIN(_mm512_maskz_set1_epi8) \
VECTOR_INTRIN(_mm512_maskz_shuffle_epi32) \
VECTOR_INTRIN(_mm512_maskz_shuffle_epi8) \
VECTOR_INTRIN(_mm512_maskz_shuffle_f32x4) \
VECTOR_INTRIN(_mm512_maskz_shuffle_f64x2) \
VECTOR_INTRIN(_mm512_maskz_shufflehi_epi16) \
VECTOR_INTRIN(_mm512_maskz_shuffle_i32x4) \
VECTOR_INTRIN(_mm512_maskz_shuffle_i64x2) \
VECTOR_INTRIN(_mm512_maskz_shufflelo_epi16) \
VECTOR_INTRIN(_mm512_maskz_shuffle_pd) \
VECTOR_INTRIN(_mm512_maskz_shuffle_ps) \
VECTOR_INTRIN(_mm512_maskz_sll_epi16) \
VECTOR_INTRIN(_mm512_maskz_sll_epi32) \
VECTOR_INTRIN(_mm512_maskz_sll_epi64) \
VECTOR_INTRIN(_mm512_maskz_slli_epi16) \
VECTOR_INTRIN(_mm512_maskz_slli_epi32) \
VECTOR_INTRIN(_mm512_maskz_slli_epi64) \
VECTOR_INTRIN(_mm512_maskz_sllv_epi16) \
VECTOR_INTRIN(_mm512_maskz_sllv_epi32) \
VECTOR_INTRIN(_mm512_maskz_sllv_epi64) \
VECTOR_INTRIN(_mm512_maskz_sqrt_round_pd) \
VECTOR_INTRIN(_mm512_maskz_sqrt_round_ps) \
VECTOR_INTRIN(_mm512_maskz_sra_epi16) \
VECTOR_INTRIN(_mm512_maskz_sra_epi32) \
VECTOR_INTRIN(_mm512_maskz_sra_epi64) \
VECTOR_INTRIN(_mm512_maskz_srai_epi16) \
VECTOR_INTRIN(_mm512_maskz_srai_epi32) \
VECTOR_INTRIN(_mm512_maskz_srai_epi64) \
VECTOR_INTRIN(_mm512_maskz_srav_epi16) \
VECTOR_INTRIN(_mm512_maskz_srav_epi32) \
VECTOR_INTRIN(_mm512_maskz_srav_epi64) \
VECTOR_INTRIN(_mm512_maskz_srl_epi16) \
VECTOR_INTRIN(_mm512_maskz_srl_epi32) \
VECTOR_INTRIN(_mm512_maskz_srl_epi64) \
VECTOR_INTRIN(_mm512_maskz_srli_epi16) \
VECTOR_INTRIN(_mm512_maskz_srli_epi32) \
VECTOR_INTRIN(_mm512_maskz_srli_epi64) \
VECTOR_INTRIN(_mm512_maskz_srlv_epi16) \
VECTOR_INTRIN(_mm512_maskz_srlv_epi32) \
VECTOR_INTRIN(_mm512_maskz_srlv_epi64) \
VECTOR_INTRIN(_mm512_maskz_sub_epi16) \
VECTOR_INTRIN(_mm512_maskz_sub_epi32) \
VECTOR_INTRIN(_mm512_maskz_sub_epi64) \
VECTOR_INTRIN(_mm512_maskz_sub_epi8) \
VECTOR_INTRIN(_mm512_maskz_sub_round_pd) \
VECTOR_INTRIN(_mm512_maskz_sub_round_ps) \
VECTOR_INTRIN(_mm512_maskz_subs_epi16) \
VECTOR_INTRIN(_mm512_maskz_subs_epi8) \
VECTOR_INTRIN(_mm512_maskz_subs_epu16) \
VECTOR_INTRIN(_mm512_maskz_subs_epu8) \
VECTOR_INTRIN(_mm512_maskz_ternarylogic_epi32) \
VECTOR_INTRIN(_mm512_maskz_ternarylogic_epi64) \
VECTOR_INTRIN(_mm512_maskz_unpackhi_epi16) \
VECTOR_INTRIN(_mm512_maskz_unpackhi_epi32) \
VECTOR_INTRIN(_mm512_maskz_unpackhi_epi64) \
VECTOR_INTRIN(_mm512_maskz_unpackhi_epi8) \
VECTOR_INTRIN(_mm512_maskz_unpackhi_pd) \
VECTOR_INTRIN(_mm512_maskz_unpackhi_ps) \
VECTOR_INTRIN(_mm512_maskz_unpacklo_epi16) \
VECTOR_INTRIN(_mm512_maskz_unpacklo_epi32) \
VECTOR_INTRIN(_mm512_maskz_unpacklo_epi64) \
VECTOR_INTRIN(_mm512_maskz_unpacklo_epi8) \
VECTOR_INTRIN(_mm512_maskz_unpacklo_pd) \
VECTOR_INTRIN(_mm512_maskz_unpacklo_ps) \
VECTOR_INTRIN(_mm512_maskz_xor_epi32) \
VECTOR_INTRIN(_mm512_maskz_xor_epi64) \
VECTOR_INTRIN(_mm512_maskz_xor_pd) \
VECTOR_INTRIN(_mm512_maskz_xor_ps) \
VECTOR_INTRIN(_mm512_max_epi16) \
VECTOR_INTRIN(_mm512_max_epi32) \
VECTOR_INTRIN(_mm512_max_epi64) \
VECTOR_INTRIN(_mm512_max_epi8) \
VECTOR_INTRIN(_mm512_max_epu16) \
VECTOR_INTRIN(_mm512_max_epu32) \
VECTOR_INTRIN(_mm512_max_epu64) \
VECTOR_INTRIN(_mm512_max_epu8) \
VECTOR_INTRIN(_mm512_max_pd) \
VECTOR_INTRIN(_mm512_max_ps) \
VECTOR_INTRIN(_mm512_max_round_pd) \
VECTOR_INTRIN(_mm512_max_round_ps) \
VECTOR_INTRIN(_mm512_min_epi16) \
VECTOR_INTRIN(_mm512_min_epi32) \
VECTOR_INTRIN(_mm512_min_epi64) \
VECTOR_INTRIN(_mm512_min_epi8) \
VECTOR_INTRIN(_mm512_min_epu16) \
VECTOR_INTRIN(_mm512_min_epu32) \
VECTOR_INTRIN(_mm512_min_epu64) \
VECTOR_INTRIN(_mm512_min_epu8) \
VECTOR_INTRIN(_mm512_min_pd) \
VECTOR_INTRIN(_mm512_min_ps) \
VECTOR_INTRIN(_mm512_min_round_pd) \
VECTOR_INTRIN(_mm512_min_round_ps) \
VECTOR_INTRIN(_mm512_movedup_pd) \
VECTOR_INTRIN(_mm512_movehdup_ps) \
VECTOR_INTRIN(_mm512_moveldup_ps) \
VECTOR_INTRIN(_mm512_movepi16_mask) \
VECTOR_INTRIN(_mm512_movepi32_mask) \
VECTOR_INTRIN(_mm512_movepi64_mask) \
VECTOR_INTRIN(_mm512_movepi8_mask) \
VECTOR_INTRIN(_mm512_movm_epi16) \
VECTOR_INTRIN(_mm512_movm_epi32) \
VECTOR_INTRIN(_mm512_movm_epi64) \
VECTOR_INTRIN(_mm512_movm_epi8) \
VECTOR_INTRIN(_mm512_mul_epi32) \
VECTOR_INTRIN(_mm512_mul_epu32) \
VECTOR_INTRIN(_mm512_mulhi_epi16) \
VECTOR_INTRIN(_mm512_mulhi_epi32) \
VECTOR_INTRIN(_mm512_mulhi_epu16) \
VECTOR_INTRIN(_mm512_mulhi_epu32) \
VECTOR_INTRIN(_mm512_mulhrs_epi16) \
VECTOR_INTRIN(_mm512_mullo_epi16) \
VECTOR_INTRIN(_mm512_mullo_epi32) \
VECTOR_INTRIN(_mm512_mullo_epi64) \
VECTOR_INTRIN(_mm512_mullox_epi64) \
VECTOR_INTRIN(_mm512_mul_pd) \
VECTOR_INTRIN(_mm512_mul_ps) \
VECTOR_INTRIN(_mm512_mul_round_pd) \
VECTOR_INTRIN(_mm512_mul_round_ps) \
VECTOR_INTRIN(_mm512_multishift_epi64_epi8) \
VECTOR_INTRIN(_mm512_nearbyint_pd) \
VECTOR_INTRIN(_mm512_nearbyint_ps) \
VECTOR_INTRIN(_mm512_or_epi32) \
VECTOR_INTRIN(_mm512_or_epi64) \
VECTOR_INTRIN(_mm512_or_pd) \
VECTOR_INTRIN(_mm512_or_ps) \
VECTOR_INTRIN(_mm512_packs_epi16) \
VECTOR_INTRIN(_mm512_packs_epi32) \
VECTOR_INTRIN(_mm512_packus_epi16) \
VECTOR_INTRIN(_mm512_packus_epi32) \
VECTOR_INTRIN(_mm512_permute4f128_epi32) \
VECTOR_INTRIN(_mm512_permute4f128_ps) \
VECTOR_INTRIN(_mm512_permute_pd) \
VECTOR_INTRIN(_mm512_permute_ps) \
VECTOR_INTRIN(_mm512_permutevar_epi32) \
VECTOR_INTRIN(_mm512_permutevar_pd) \
VECTOR_INTRIN(_mm512_permutevar_ps) \
VECTOR_INTRIN(_mm512_permutex2var_epi16) \
VECTOR_INTRIN(_mm512_permutex2var_epi32) \
VECTOR_INTRIN(_mm512_permutex2var_epi64) \
VECTOR_INTRIN(_mm512_permutex2var_epi8) \
VECTOR_INTRIN(_mm512_permutex2var_pd) \
VECTOR_INTRIN(_mm512_permutex2var_ps) \
VECTOR_INTRIN(_mm512_permutex_epi64) \
VECTOR_INTRIN(_mm512_permutex_pd) \
VECTOR_INTRIN(_mm512_permutexvar_epi16) \
VECTOR_INTRIN(_mm512_permutexvar_epi32) \
VECTOR_INTRIN(_mm512_permutexvar_epi64) \
VECTOR_INTRIN(_mm512_permutexvar_epi8) \
VECTOR_INTRIN(_mm512_permutexvar_pd) \
VECTOR_INTRIN(_mm512_permutexvar_ps) \
VECTOR_INTRIN(_mm512_pow_pd) \
VECTOR_INTRIN(_mm512_pow_ps) \
VECTOR_INTRIN(_mm512_prefetch_i32extgather_ps) \
VECTOR_INTRIN(_mm512_prefetch_i32extscatter_ps) \
VECTOR_INTRIN(_mm512_prefetch_i32gather_pd) \
VECTOR_INTRIN(_mm512_prefetch_i32scatter_pd) \
VECTOR_INTRIN(_mm512_prefetch_i64gather_pd) \
VECTOR_INTRIN(_mm512_prefetch_i64gather_ps) \
VECTOR_INTRIN(_mm512_prefetch_i64scatter_pd) \
VECTOR_INTRIN(_mm512_prefetch_i64scatter_ps) \
VECTOR_INTRIN(_mm512_range_pd) \
VECTOR_INTRIN(_mm512_range_ps) \
VECTOR_INTRIN(_mm512_range_round_pd) \
VECTOR_INTRIN(_mm512_range_round_ps) \
VECTOR_INTRIN(_mm512_rcp14_pd) \
VECTOR_INTRIN(_mm512_rcp14_ps) \
VECTOR_INTRIN(_mm512_rcp28_round_pd) \
VECTOR_INTRIN(_mm512_rcp28_round_ps) \
VECTOR_INTRIN(_mm512_recip_pd) \
VECTOR_INTRIN(_mm512_recip_ps) \
VECTOR_INTRIN(_mm512_reduce_add_epi32) \
VECTOR_INTRIN(_mm512_reduce_add_epi64) \
VECTOR_INTRIN(_mm512_reduce_add_pd) \
VECTOR_INTRIN(_mm512_reduce_add_ps) \
VECTOR_INTRIN(_mm512_reduce_and_epi32) \
VECTOR_INTRIN(_mm512_reduce_and_epi64) \
VECTOR_INTRIN(_mm512_reduce_gmax_pd) \
VECTOR_INTRIN(_mm512_reduce_gmax_ps) \
VECTOR_INTRIN(_mm512_reduce_gmin_pd) \
VECTOR_INTRIN(_mm512_reduce_gmin_ps) \
VECTOR_INTRIN(_mm512_reduce_max_epi32) \
VECTOR_INTRIN(_mm512_reduce_max_epi64) \
VECTOR_INTRIN(_mm512_reduce_max_epu32) \
VECTOR_INTRIN(_mm512_reduce_max_epu64) \
VECTOR_INTRIN(_mm512_reduce_max_pd) \
VECTOR_INTRIN(_mm512_reduce_max_ps) \
VECTOR_INTRIN(_mm512_reduce_min_epi32) \
VECTOR_INTRIN(_mm512_reduce_min_epi64) \
VECTOR_INTRIN(_mm512_reduce_min_epu32) \
VECTOR_INTRIN(_mm512_reduce_min_epu64) \
VECTOR_INTRIN(_mm512_reduce_min_pd) \
VECTOR_INTRIN(_mm512_reduce_min_ps) \
VECTOR_INTRIN(_mm512_reduce_mul_epi32) \
VECTOR_INTRIN(_mm512_reduce_mul_epi64) \
VECTOR_INTRIN(_mm512_reduce_mul_pd) \
VECTOR_INTRIN(_mm512_reduce_mul_ps) \
VECTOR_INTRIN(_mm512_reduce_or_epi32) \
VECTOR_INTRIN(_mm512_reduce_or_epi64) \
VECTOR_INTRIN(_mm512_reduce_pd) \
VECTOR_INTRIN(_mm512_reduce_ps) \
VECTOR_INTRIN(_mm512_reduce_round_pd) \
VECTOR_INTRIN(_mm512_reduce_round_ps) \
VECTOR_INTRIN(_mm512_rem_epi16) \
VECTOR_INTRIN(_mm512_rem_epi32) \
VECTOR_INTRIN(_mm512_rem_epi64) \
VECTOR_INTRIN(_mm512_rem_epi8) \
VECTOR_INTRIN(_mm512_rem_epu16) \
VECTOR_INTRIN(_mm512_rem_epu32) \
VECTOR_INTRIN(_mm512_rem_epu64) \
VECTOR_INTRIN(_mm512_rem_epu8) \
VECTOR_INTRIN(_mm512_rint_pd) \
VECTOR_INTRIN(_mm512_rint_ps) \
VECTOR_INTRIN(_mm512_rol_epi32) \
VECTOR_INTRIN(_mm512_rol_epi64) \
VECTOR_INTRIN(_mm512_rolv_epi32) \
VECTOR_INTRIN(_mm512_rolv_epi64) \
VECTOR_INTRIN(_mm512_ror_epi32) \
VECTOR_INTRIN(_mm512_ror_epi64) \
VECTOR_INTRIN(_mm512_rorv_epi32) \
VECTOR_INTRIN(_mm512_rorv_epi64) \
VECTOR_INTRIN(_mm512_roundscale_pd) \
VECTOR_INTRIN(_mm512_roundscale_ps) \
VECTOR_INTRIN(_mm512_roundscale_round_pd) \
VECTOR_INTRIN(_mm512_roundscale_round_ps) \
VECTOR_INTRIN(_mm512_rsqrt14_pd) \
VECTOR_INTRIN(_mm512_rsqrt14_ps) \
VECTOR_INTRIN(_mm512_rsqrt28_round_pd) \
VECTOR_INTRIN(_mm512_rsqrt28_round_ps) \
VECTOR_INTRIN(_mm512_sad_epu8) \
VECTOR_INTRIN(_mm512_scalef_round_pd) \
VECTOR_INTRIN(_mm512_scalef_round_ps) \
VECTOR_INTRIN(_mm512_scale_ps) \
VECTOR_INTRIN(_mm512_scale_round_ps) \
VECTOR_INTRIN(_mm512_set1_epi16) \
VECTOR_INTRIN(_mm512_set1_epi32) \
VECTOR_INTRIN(_mm512_set1_epi64) \
VECTOR_INTRIN(_mm512_set1_epi8) \
VECTOR_INTRIN(_mm512_set1_pd) \
VECTOR_INTRIN(_mm512_set1_ps) \
VECTOR_INTRIN(_mm512_set4_epi32) \
VECTOR_INTRIN(_mm512_set4_epi64) \
VECTOR_INTRIN(_mm512_set4_pd) \
VECTOR_INTRIN(_mm512_set4_ps) \
VECTOR_INTRIN(_mm512_set_epi16) \
VECTOR_INTRIN(_mm512_set_epi32) \
VECTOR_INTRIN(_mm512_set_epi64) \
VECTOR_INTRIN(_mm512_set_epi8) \
VECTOR_INTRIN(_mm512_set_pd) \
VECTOR_INTRIN(_mm512_set_ps) \
VECTOR_INTRIN(_mm512_setzero) \
VECTOR_INTRIN(_mm512_shuffle_epi32) \
VECTOR_INTRIN(_mm512_shuffle_epi8) \
VECTOR_INTRIN(_mm512_shuffle_f32x4) \
VECTOR_INTRIN(_mm512_shuffle_f64x2) \
VECTOR_INTRIN(_mm512_shufflehi_epi16) \
VECTOR_INTRIN(_mm512_shuffle_i32x4) \
VECTOR_INTRIN(_mm512_shuffle_i64x2) \
VECTOR_INTRIN(_mm512_shufflelo_epi16) \
VECTOR_INTRIN(_mm512_shuffle_pd) \
VECTOR_INTRIN(_mm512_shuffle_ps) \
VECTOR_INTRIN(_mm512_sincos_pd) \
VECTOR_INTRIN(_mm512_sincos_ps) \
VECTOR_INTRIN(_mm512_sind_pd) \
VECTOR_INTRIN(_mm512_sind_ps) \
VECTOR_INTRIN(_mm512_sinh_pd) \
VECTOR_INTRIN(_mm512_sinh_ps) \
VECTOR_INTRIN(_mm512_sin_pd) \
VECTOR_INTRIN(_mm512_sin_ps) \
VECTOR_INTRIN(_mm512_sll_epi16) \
VECTOR_INTRIN(_mm512_sll_epi32) \
VECTOR_INTRIN(_mm512_sll_epi64) \
VECTOR_INTRIN(_mm512_slli_epi16) \
VECTOR_INTRIN(_mm512_slli_epi32) \
VECTOR_INTRIN(_mm512_slli_epi64) \
VECTOR_INTRIN(_mm512_sllv_epi16) \
VECTOR_INTRIN(_mm512_sllv_epi32) \
VECTOR_INTRIN(_mm512_sllv_epi64) \
VECTOR_INTRIN(_mm512_sqrt_pd) \
VECTOR_INTRIN(_mm512_sqrt_ps) \
VECTOR_INTRIN(_mm512_sqrt_round_pd) \
VECTOR_INTRIN(_mm512_sqrt_round_ps) \
VECTOR_INTRIN(_mm512_sra_epi16) \
VECTOR_INTRIN(_mm512_sra_epi32) \
VECTOR_INTRIN(_mm512_sra_epi64) \
VECTOR_INTRIN(_mm512_srai_epi16) \
VECTOR_INTRIN(_mm512_srai_epi32) \
VECTOR_INTRIN(_mm512_srai_epi64) \
VECTOR_INTRIN(_mm512_srav_epi16) \
VECTOR_INTRIN(_mm512_srav_epi32) \
VECTOR_INTRIN(_mm512_srav_epi64) \
VECTOR_INTRIN(_mm512_srl_epi16) \
VECTOR_INTRIN(_mm512_srl_epi32) \
VECTOR_INTRIN(_mm512_srl_epi64) \
VECTOR_INTRIN(_mm512_srli_epi16) \
VECTOR_INTRIN(_mm512_srli_epi32) \
VECTOR_INTRIN(_mm512_srli_epi64) \
VECTOR_INTRIN(_mm512_srlv_epi16) \
VECTOR_INTRIN(_mm512_srlv_epi32) \
VECTOR_INTRIN(_mm512_srlv_epi64) \
VECTOR_INTRIN(_mm512_store_epi32) \
VECTOR_INTRIN(_mm512_store_epi64) \
VECTOR_INTRIN(_mm512_store_pd) \
VECTOR_INTRIN(_mm512_store_ps) \
VECTOR_INTRIN(_mm512_storeu_pd) \
VECTOR_INTRIN(_mm512_storeu_ps) \
VECTOR_INTRIN(_mm512_storeu_si512) \
VECTOR_INTRIN(_mm512_stream_load_si512) \
VECTOR_INTRIN(_mm512_stream_pd) \
VECTOR_INTRIN(_mm512_stream_ps) \
VECTOR_INTRIN(_mm512_stream_si512) \
VECTOR_INTRIN(_mm512_sub_epi16) \
VECTOR_INTRIN(_mm512_sub_epi32) \
VECTOR_INTRIN(_mm512_sub_epi64) \
VECTOR_INTRIN(_mm512_sub_epi8) \
VECTOR_INTRIN(_mm512_sub_pd) \
VECTOR_INTRIN(_mm512_sub_ps) \
VECTOR_INTRIN(_mm512_subr_epi32) \
VECTOR_INTRIN(_mm512_sub_round_pd) \
VECTOR_INTRIN(_mm512_sub_round_ps) \
VECTOR_INTRIN(_mm512_subr_pd) \
VECTOR_INTRIN(_mm512_subr_ps) \
VECTOR_INTRIN(_mm512_subr_round_pd) \
VECTOR_INTRIN(_mm512_subr_round_ps) \
VECTOR_INTRIN(_mm512_subs_epi16) \
VECTOR_INTRIN(_mm512_subs_epi8) \
VECTOR_INTRIN(_mm512_subs_epu16) \
VECTOR_INTRIN(_mm512_subs_epu8) \
VECTOR_INTRIN(_mm512_svml_round_pd) \
VECTOR_INTRIN(_mm512_swizzle_epi32) \
VECTOR_INTRIN(_mm512_swizzle_epi64) \
VECTOR_INTRIN(_mm512_swizzle_pd) \
VECTOR_INTRIN(_mm512_swizzle_ps) \
VECTOR_INTRIN(_mm512_tand_pd) \
VECTOR_INTRIN(_mm512_tand_ps) \
VECTOR_INTRIN(_mm512_tanh_pd) \
VECTOR_INTRIN(_mm512_tanh_ps) \
VECTOR_INTRIN(_mm512_tan_pd) \
VECTOR_INTRIN(_mm512_tan_ps) \
VECTOR_INTRIN(_mm512_ternarylogic_epi32) \
VECTOR_INTRIN(_mm512_ternarylogic_epi64) \
VECTOR_INTRIN(_mm512_test_epi16_mask) \
VECTOR_INTRIN(_mm512_test_epi32_mask) \
VECTOR_INTRIN(_mm512_test_epi64_mask) \
VECTOR_INTRIN(_mm512_test_epi8_mask) \
VECTOR_INTRIN(_mm512_testn_epi16_mask) \
VECTOR_INTRIN(_mm512_testn_epi32_mask) \
VECTOR_INTRIN(_mm512_testn_epi64_mask) \
VECTOR_INTRIN(_mm512_testn_epi8_mask) \
VECTOR_INTRIN(_mm512_trunc_pd) \
VECTOR_INTRIN(_mm512_trunc_ps) \
VECTOR_INTRIN(_mm512_undefined) \
VECTOR_INTRIN(_mm512_unpackhi_epi16) \
VECTOR_INTRIN(_mm512_unpackhi_epi32) \
VECTOR_INTRIN(_mm512_unpackhi_epi64) \
VECTOR_INTRIN(_mm512_unpackhi_epi8) \
VECTOR_INTRIN(_mm512_unpackhi_pd) \
VECTOR_INTRIN(_mm512_unpackhi_ps) \
VECTOR_INTRIN(_mm512_unpacklo_epi16) \
VECTOR_INTRIN(_mm512_unpacklo_epi32) \
VECTOR_INTRIN(_mm512_unpacklo_epi64) \
VECTOR_INTRIN(_mm512_unpacklo_epi8) \
VECTOR_INTRIN(_mm512_unpacklo_pd) \
VECTOR_INTRIN(_mm512_unpacklo_ps) \
VECTOR_INTRIN(_mm512_xor_epi32) \
VECTOR_INTRIN(_mm512_xor_epi64) \
VECTOR_INTRIN(_mm512_xor_pd) \
VECTOR_INTRIN(_mm512_xor_ps) \
VECTOR_INTRIN(_mm_abs_epi64) \
VECTOR_INTRIN(_mm_acosh_pd) \
VECTOR_INTRIN(_mm_acosh_ps) \
VECTOR_INTRIN(_mm_acos_pd) \
VECTOR_INTRIN(_mm_acos_ps) \
VECTOR_INTRIN(_mm_add_epi16) \
VECTOR_INTRIN(_mm_add_epi32) \
VECTOR_INTRIN(_mm_add_epi64) \
VECTOR_INTRIN(_mm_add_epi8) \
VECTOR_INTRIN(_mm_add_pd) \
VECTOR_INTRIN(_mm_add_ps) \
VECTOR_INTRIN(_mm_add_round_sd) \
VECTOR_INTRIN(_mm_add_round_ss) \
VECTOR_INTRIN(_mm_add_sd) \
VECTOR_INTRIN(_mm_adds_epi16) \
VECTOR_INTRIN(_mm_adds_epi8) \
VECTOR_INTRIN(_mm_adds_epu16) \
VECTOR_INTRIN(_mm_adds_epu8) \
VECTOR_INTRIN(_mm_add_si64) \
VECTOR_INTRIN(_mm_add_ss) \
VECTOR_INTRIN(_mm_addsub_pd) \
VECTOR_INTRIN(_mm_addsub_ps) \
VECTOR_INTRIN(_mm_aesdeclast_si128) \
VECTOR_INTRIN(_mm_aesdec_si128) \
VECTOR_INTRIN(_mm_aesenclast_si128) \
VECTOR_INTRIN(_mm_aesenc_si128) \
VECTOR_INTRIN(_mm_aesimc_si128) \
VECTOR_INTRIN(_mm_aeskeygenassist_si128) \
VECTOR_INTRIN(_mm_alignr_epi32) \
VECTOR_INTRIN(_mm_alignr_epi64) \
VECTOR_INTRIN(_mm_andnot_pd) \
VECTOR_INTRIN(_mm_andnot_ps) \
VECTOR_INTRIN(_mm_andnot_si128) \
VECTOR_INTRIN(_mm_and_pd) \
VECTOR_INTRIN(_mm_and_ps) \
VECTOR_INTRIN(_mm_and_si128) \
VECTOR_INTRIN(_mm_asinh_pd) \
VECTOR_INTRIN(_mm_asinh_ps) \
VECTOR_INTRIN(_mm_asin_pd) \
VECTOR_INTRIN(_mm_asin_ps) \
VECTOR_INTRIN(_m_maskmovq) \
VECTOR_INTRIN(_mm_atan2_pd) \
VECTOR_INTRIN(_mm_atan2_ps) \
VECTOR_INTRIN(_mm_atanh_pd) \
VECTOR_INTRIN(_mm_atanh_ps) \
VECTOR_INTRIN(_mm_atan_pd) \
VECTOR_INTRIN(_mm_atan_ps) \
VECTOR_INTRIN(_mm_avg_epu16) \
VECTOR_INTRIN(_mm_avg_epu8) \
VECTOR_INTRIN(_mm_blend_epi32) \
VECTOR_INTRIN(_mm_blendv_epi8) \
VECTOR_INTRIN(_mm_blendv_pd) \
VECTOR_INTRIN(_mm_blendv_ps) \
VECTOR_INTRIN(_mm_broadcastb_epi8) \
VECTOR_INTRIN(_mm_broadcastd_epi32) \
VECTOR_INTRIN(_mm_broadcast_i32x2) \
VECTOR_INTRIN(_mm_broadcastmb_epi64) \
VECTOR_INTRIN(_mm_broadcastmw_epi32) \
VECTOR_INTRIN(_mm_broadcastq_epi64) \
VECTOR_INTRIN(_mm_broadcastsd_pd) \
VECTOR_INTRIN(_mm_broadcast_ss) \
VECTOR_INTRIN(_mm_broadcastss_ps) \
VECTOR_INTRIN(_mm_broadcastw_epi16) \
VECTOR_INTRIN(_mm_bslli_si128) \
VECTOR_INTRIN(_mm_bsrli_si128) \
VECTOR_INTRIN(_mm_castpd_ps) \
VECTOR_INTRIN(_mm_castpd_si128) \
VECTOR_INTRIN(_mm_castps_pd) \
VECTOR_INTRIN(_mm_castps_si128) \
VECTOR_INTRIN(_mm_castsi128_pd) \
VECTOR_INTRIN(_mm_castsi128_ps) \
VECTOR_INTRIN(_mm_cbrt_pd) \
VECTOR_INTRIN(_mm_cbrt_ps) \
VECTOR_INTRIN(_mm_cdfnorminv_pd) \
VECTOR_INTRIN(_mm_cdfnorminv_ps) \
VECTOR_INTRIN(_mm_cdfnorm_pd) \
VECTOR_INTRIN(_mm_cdfnorm_ps) \
VECTOR_INTRIN(_mm_cexp_ps) \
VECTOR_INTRIN(_mm_clflush) \
VECTOR_INTRIN(_mm_clflushopt) \
VECTOR_INTRIN(_mm_clmulepi64_si128) \
VECTOR_INTRIN(_mm_clog_ps) \
VECTOR_INTRIN(_mm_clwb) \
VECTOR_INTRIN(_mm_cmp_epi16_mask) \
VECTOR_INTRIN(_mm_cmp_epi32_mask) \
VECTOR_INTRIN(_mm_cmp_epi64_mask) \
VECTOR_INTRIN(_mm_cmp_epi8_mask) \
VECTOR_INTRIN(_mm_cmp_epu16_mask) \
VECTOR_INTRIN(_mm_cmp_epu32_mask) \
VECTOR_INTRIN(_mm_cmp_epu64_mask) \
VECTOR_INTRIN(_mm_cmp_epu8_mask) \
VECTOR_INTRIN(_mm_cmpeq_epi16) \
VECTOR_INTRIN(_mm_cmpeq_epi32) \
VECTOR_INTRIN(_mm_cmpeq_epi64) \
VECTOR_INTRIN(_mm_cmpeq_epi8) \
VECTOR_INTRIN(_mm_cmpeq_pd) \
VECTOR_INTRIN(_mm_cmpeq_ps) \
VECTOR_INTRIN(_mm_cmpeq_sd) \
VECTOR_INTRIN(_mm_cmpeq_ss) \
VECTOR_INTRIN(_mm_cmpge_pd) \
VECTOR_INTRIN(_mm_cmpge_ps) \
VECTOR_INTRIN(_mm_cmpge_sd) \
VECTOR_INTRIN(_mm_cmpge_ss) \
VECTOR_INTRIN(_mm_cmpgt_epi16) \
VECTOR_INTRIN(_mm_cmpgt_epi32) \
VECTOR_INTRIN(_mm_cmpgt_epi64) \
VECTOR_INTRIN(_mm_cmpgt_epi8) \
VECTOR_INTRIN(_mm_cmpgt_pd) \
VECTOR_INTRIN(_mm_cmpgt_ps) \
VECTOR_INTRIN(_mm_cmpgt_sd) \
VECTOR_INTRIN(_mm_cmpgt_ss) \
VECTOR_INTRIN(_mm_cmple_pd) \
VECTOR_INTRIN(_mm_cmple_ps) \
VECTOR_INTRIN(_mm_cmple_sd) \
VECTOR_INTRIN(_mm_cmple_ss) \
VECTOR_INTRIN(_mm_cmplt_epi16) \
VECTOR_INTRIN(_mm_cmplt_epi32) \
VECTOR_INTRIN(_mm_cmplt_epi8) \
VECTOR_INTRIN(_mm_cmplt_pd) \
VECTOR_INTRIN(_mm_cmplt_ps) \
VECTOR_INTRIN(_mm_cmplt_sd) \
VECTOR_INTRIN(_mm_cmplt_ss) \
VECTOR_INTRIN(_mm_cmpneq_pd) \
VECTOR_INTRIN(_mm_cmpneq_ps) \
VECTOR_INTRIN(_mm_cmpneq_sd) \
VECTOR_INTRIN(_mm_cmpneq_ss) \
VECTOR_INTRIN(_mm_cmpnge_pd) \
VECTOR_INTRIN(_mm_cmpnge_ps) \
VECTOR_INTRIN(_mm_cmpnge_sd) \
VECTOR_INTRIN(_mm_cmpnge_ss) \
VECTOR_INTRIN(_mm_cmpngt_pd) \
VECTOR_INTRIN(_mm_cmpngt_ps) \
VECTOR_INTRIN(_mm_cmpngt_sd) \
VECTOR_INTRIN(_mm_cmpngt_ss) \
VECTOR_INTRIN(_mm_cmpnle_pd) \
VECTOR_INTRIN(_mm_cmpnle_ps) \
VECTOR_INTRIN(_mm_cmpnle_sd) \
VECTOR_INTRIN(_mm_cmpnle_ss) \
VECTOR_INTRIN(_mm_cmpnlt_pd) \
VECTOR_INTRIN(_mm_cmpnlt_ps) \
VECTOR_INTRIN(_mm_cmpnlt_sd) \
VECTOR_INTRIN(_mm_cmpnlt_ss) \
VECTOR_INTRIN(_mm_cmpord_pd) \
VECTOR_INTRIN(_mm_cmpord_ps) \
VECTOR_INTRIN(_mm_cmpord_sd) \
VECTOR_INTRIN(_mm_cmpord_ss) \
VECTOR_INTRIN(_mm_cmp_pd) \
VECTOR_INTRIN(_mm_cmp_pd_mask) \
VECTOR_INTRIN(_mm_cmp_ps) \
VECTOR_INTRIN(_mm_cmp_ps_mask) \
VECTOR_INTRIN(_mm_cmp_round_sd_mask) \
VECTOR_INTRIN(_mm_cmp_round_ss_mask) \
VECTOR_INTRIN(_mm_cmp_sd) \
VECTOR_INTRIN(_mm_cmp_ss) \
VECTOR_INTRIN(_mm_cmpunord_pd) \
VECTOR_INTRIN(_mm_cmpunord_ps) \
VECTOR_INTRIN(_mm_cmpunord_sd) \
VECTOR_INTRIN(_mm_cmpunord_ss) \
VECTOR_INTRIN(_mm_comieq_sd) \
VECTOR_INTRIN(_mm_comieq_ss) \
VECTOR_INTRIN(_mm_comige_sd) \
VECTOR_INTRIN(_mm_comige_ss) \
VECTOR_INTRIN(_mm_comigt_sd) \
VECTOR_INTRIN(_mm_comigt_ss) \
VECTOR_INTRIN(_mm_comile_sd) \
VECTOR_INTRIN(_mm_comile_ss) \
VECTOR_INTRIN(_mm_comilt_sd) \
VECTOR_INTRIN(_mm_comilt_ss) \
VECTOR_INTRIN(_mm_comineq_sd) \
VECTOR_INTRIN(_mm_comineq_ss) \
VECTOR_INTRIN(_mm_comi_round_sd) \
VECTOR_INTRIN(_mm_comi_round_ss) \
VECTOR_INTRIN(_mm_conflict_epi32) \
VECTOR_INTRIN(_mm_conflict_epi64) \
VECTOR_INTRIN(_mm_cosd_pd) \
VECTOR_INTRIN(_mm_cosd_ps) \
VECTOR_INTRIN(_mm_cosh_pd) \
VECTOR_INTRIN(_mm_cosh_ps) \
VECTOR_INTRIN(_mm_cos_pd) \
VECTOR_INTRIN(_mm_cos_ps) \
VECTOR_INTRIN(_mm_countbits_32) \
VECTOR_INTRIN(_mm_countbits_64) \
VECTOR_INTRIN(_mm_crc32_u16) \
VECTOR_INTRIN(_mm_crc32_u32) \
VECTOR_INTRIN(_mm_crc32_u64) \
VECTOR_INTRIN(_mm_crc32_u8 ) \
VECTOR_INTRIN(_mm_csqrt_ps) \
VECTOR_INTRIN(_mm_cvtepi16_epi32) \
VECTOR_INTRIN(_mm_cvtepi16_epi64) \
VECTOR_INTRIN(_mm_cvtepi16_epi8) \
VECTOR_INTRIN(_mm_cvtepi32_epi16) \
VECTOR_INTRIN(_mm_cvtepi32_epi64) \
VECTOR_INTRIN(_mm_cvtepi32_epi8) \
VECTOR_INTRIN(_mm_cvtepi32_pd) \
VECTOR_INTRIN(_mm_cvtepi32_ps) \
VECTOR_INTRIN(_mm_cvtepi64_epi16) \
VECTOR_INTRIN(_mm_cvtepi64_epi32) \
VECTOR_INTRIN(_mm_cvtepi64_epi8) \
VECTOR_INTRIN(_mm_cvtepi64_pd) \
VECTOR_INTRIN(_mm_cvtepi64_ps) \
VECTOR_INTRIN(_mm_cvtepu16_epi32) \
VECTOR_INTRIN(_mm_cvtepu16_epi64) \
VECTOR_INTRIN(_mm_cvtepu32_epi64) \
VECTOR_INTRIN(_mm_cvtepu32_pd) \
VECTOR_INTRIN(_mm_cvtepu64_pd) \
VECTOR_INTRIN(_mm_cvtepu64_ps) \
VECTOR_INTRIN(_mm_cvtpd_epi32) \
VECTOR_INTRIN(_mm_cvtpd_epi64) \
VECTOR_INTRIN(_mm_cvtpd_epu32) \
VECTOR_INTRIN(_mm_cvtpd_epu64) \
VECTOR_INTRIN(_mm_cvtpd_pi32) \
VECTOR_INTRIN(_mm_cvtpd_ps) \
VECTOR_INTRIN(_mm_cvtph_ps) \
VECTOR_INTRIN(_mm_cvt_pi2ps) \
VECTOR_INTRIN(_mm_cvtpi32_pd) \
VECTOR_INTRIN(_mm_cvt_ps2pi) \
VECTOR_INTRIN(_mm_cvtps_epi32) \
VECTOR_INTRIN(_mm_cvtps_epi64) \
VECTOR_INTRIN(_mm_cvtps_epu32) \
VECTOR_INTRIN(_mm_cvtps_epu64) \
VECTOR_INTRIN(_mm_cvtps_pd) \
VECTOR_INTRIN(_mm_cvtps_ph) \
VECTOR_INTRIN(_mm_cvt_roundi32_ss) \
VECTOR_INTRIN(_mm_cvt_roundi64_sd) \
VECTOR_INTRIN(_mm_cvt_roundi64_ss) \
VECTOR_INTRIN(_mm_cvt_roundsd_i32) \
VECTOR_INTRIN(_mm_cvt_roundsd_i64) \
VECTOR_INTRIN(_mm_cvt_roundsd_ss) \
VECTOR_INTRIN(_mm_cvt_roundsd_u32) \
VECTOR_INTRIN(_mm_cvt_roundsd_u64) \
VECTOR_INTRIN(_mm_cvt_roundss_i32) \
VECTOR_INTRIN(_mm_cvt_roundss_i64) \
VECTOR_INTRIN(_mm_cvt_roundss_sd) \
VECTOR_INTRIN(_mm_cvt_roundss_u32) \
VECTOR_INTRIN(_mm_cvt_roundss_u64) \
VECTOR_INTRIN(_mm_cvt_roundu32_ss) \
VECTOR_INTRIN(_mm_cvt_roundu64_sd) \
VECTOR_INTRIN(_mm_cvt_roundu64_ss) \
VECTOR_INTRIN(_mm_cvtsd_f64) \
VECTOR_INTRIN(_mm_cvtsd_si32) \
VECTOR_INTRIN(_mm_cvtsd_si64) \
VECTOR_INTRIN(_mm_cvtsd_ss) \
VECTOR_INTRIN(_mm_cvtsepi16_epi8) \
VECTOR_INTRIN(_mm_cvtsepi32_epi16) \
VECTOR_INTRIN(_mm_cvtsepi32_epi8) \
VECTOR_INTRIN(_mm_cvtsepi64_epi16) \
VECTOR_INTRIN(_mm_cvtsepi64_epi32) \
VECTOR_INTRIN(_mm_cvtsepi64_epi8) \
VECTOR_INTRIN(_mm_cvtsi128_si32) \
VECTOR_INTRIN(_mm_cvtsi128_si64) \
VECTOR_INTRIN(_mm_cvt_si2ss) \
VECTOR_INTRIN(_mm_cvtsi32_sd) \
VECTOR_INTRIN(_mm_cvtsi32_si128) \
VECTOR_INTRIN(_mm_cvtsi64_sd) \
VECTOR_INTRIN(_mm_cvtsi64_si128) \
VECTOR_INTRIN(_mm_cvtsi64_ss) \
VECTOR_INTRIN(_mm_cvt_ss2si) \
VECTOR_INTRIN(_mm_cvtss_f32) \
VECTOR_INTRIN(_mm_cvtss_sd) \
VECTOR_INTRIN(_mm_cvtss_si64) \
VECTOR_INTRIN(_mm_cvttpd_epi32) \
VECTOR_INTRIN(_mm_cvttpd_epi64) \
VECTOR_INTRIN(_mm_cvttpd_epu32) \
VECTOR_INTRIN(_mm_cvttpd_epu64) \
VECTOR_INTRIN(_mm_cvttpd_pi32) \
VECTOR_INTRIN(_mm_cvtt_ps2pi) \
VECTOR_INTRIN(_mm_cvttps_epi32) \
VECTOR_INTRIN(_mm_cvttps_epi64) \
VECTOR_INTRIN(_mm_cvttps_epu32) \
VECTOR_INTRIN(_mm_cvttps_epu64) \
VECTOR_INTRIN(_mm_cvtt_roundsd_i32) \
VECTOR_INTRIN(_mm_cvtt_roundsd_i64) \
VECTOR_INTRIN(_mm_cvtt_roundsd_u32) \
VECTOR_INTRIN(_mm_cvtt_roundsd_u64) \
VECTOR_INTRIN(_mm_cvtt_roundss_i32) \
VECTOR_INTRIN(_mm_cvtt_roundss_i64) \
VECTOR_INTRIN(_mm_cvtt_roundss_u32) \
VECTOR_INTRIN(_mm_cvtt_roundss_u64) \
VECTOR_INTRIN(_mm_cvttsd_si32) \
VECTOR_INTRIN(_mm_cvttsd_si64) \
VECTOR_INTRIN(_mm_cvtt_ss2si) \
VECTOR_INTRIN(_mm_cvttss_si64) \
VECTOR_INTRIN(_mm_cvtu32_sd) \
VECTOR_INTRIN(_mm_cvtusepi16_epi8) \
VECTOR_INTRIN(_mm_cvtusepi32_epi16) \
VECTOR_INTRIN(_mm_cvtusepi32_epi8) \
VECTOR_INTRIN(_mm_cvtusepi64_epi16) \
VECTOR_INTRIN(_mm_cvtusepi64_epi32) \
VECTOR_INTRIN(_mm_cvtusepi64_epi8) \
VECTOR_INTRIN(_mm_dbsad_epu8) \
VECTOR_INTRIN(_mm_delay_32) \
VECTOR_INTRIN(_mm_delay_64) \
VECTOR_INTRIN(_mm_div_epi16) \
VECTOR_INTRIN(_mm_div_epi32) \
VECTOR_INTRIN(_mm_div_epi64) \
VECTOR_INTRIN(_mm_div_epi8) \
VECTOR_INTRIN(_mm_div_epu16) \
VECTOR_INTRIN(_mm_div_epu32) \
VECTOR_INTRIN(_mm_div_epu64) \
VECTOR_INTRIN(_mm_div_epu8) \
VECTOR_INTRIN(_mm_div_pd) \
VECTOR_INTRIN(_mm_div_ps) \
VECTOR_INTRIN(_mm_div_round_sd) \
VECTOR_INTRIN(_mm_div_round_ss) \
VECTOR_INTRIN(_mm_div_sd) \
VECTOR_INTRIN(_mm_div_ss) \
VECTOR_INTRIN(_mm_dp_pd) \
VECTOR_INTRIN(_mm_dp_ps) \
VECTOR_INTRIN(_mm_erfcinv_pd) \
VECTOR_INTRIN(_mm_erfcinv_ps) \
VECTOR_INTRIN(_mm_erfc_pd) \
VECTOR_INTRIN(_mm_erfc_ps) \
VECTOR_INTRIN(_mm_erfinv_pd) \
VECTOR_INTRIN(_mm_erfinv_ps) \
VECTOR_INTRIN(_mm_erf_pd) \
VECTOR_INTRIN(_mm_erf_ps) \
VECTOR_INTRIN(_mm_exp10_pd) \
VECTOR_INTRIN(_mm_exp10_ps) \
VECTOR_INTRIN(_mm_exp2_pd) \
VECTOR_INTRIN(_mm_exp2_ps) \
VECTOR_INTRIN(_mm_expm1_pd) \
VECTOR_INTRIN(_mm_expm1_ps) \
VECTOR_INTRIN(_mm_exp_pd) \
VECTOR_INTRIN(_mm_exp_ps) \
VECTOR_INTRIN(_mm_extract_epi16) \
VECTOR_INTRIN(_mm_extract_epi32) \
VECTOR_INTRIN(_mm_extract_epi64) \
VECTOR_INTRIN(_mm_extract_ps) \
VECTOR_INTRIN(_mm_fixupimm_pd) \
VECTOR_INTRIN(_mm_fixupimm_ps) \
VECTOR_INTRIN(_mm_fixupimm_round_sd) \
VECTOR_INTRIN(_mm_fixupimm_round_ss) \
VECTOR_INTRIN(_mm_fmadd_pd) \
VECTOR_INTRIN(_mm_fmadd_ps) \
VECTOR_INTRIN(_mm_fmadd_sd) \
VECTOR_INTRIN(_mm_fmadd_ss) \
VECTOR_INTRIN(_mm_fmaddsub_pd) \
VECTOR_INTRIN(_mm_fmaddsub_ps) \
VECTOR_INTRIN(_mm_fmsubadd_pd) \
VECTOR_INTRIN(_mm_fmsubadd_ps) \
VECTOR_INTRIN(_mm_fmsub_pd) \
VECTOR_INTRIN(_mm_fmsub_ps) \
VECTOR_INTRIN(_mm_fmsub_sd) \
VECTOR_INTRIN(_mm_fmsub_ss) \
VECTOR_INTRIN(_mm_fnmadd_pd) \
VECTOR_INTRIN(_mm_fnmadd_ps) \
VECTOR_INTRIN(_mm_fnmadd_sd) \
VECTOR_INTRIN(_mm_fnmadd_ss) \
VECTOR_INTRIN(_mm_fnmsub_pd) \
VECTOR_INTRIN(_mm_fnmsub_ps) \
VECTOR_INTRIN(_mm_fnmsub_sd) \
VECTOR_INTRIN(_mm_fnmsub_ss) \
VECTOR_INTRIN(_mm_fpclass_pd_mask) \
VECTOR_INTRIN(_mm_fpclass_ps_mask) \
VECTOR_INTRIN(_mm_fpclass_sd_mask) \
VECTOR_INTRIN(_mm_fpclass_ss_mask) \
VECTOR_INTRIN(_mm_getcsr) \
VECTOR_INTRIN(_mm_getexp_pd) \
VECTOR_INTRIN(_mm_getexp_ps) \
VECTOR_INTRIN(_mm_getexp_round_sd) \
VECTOR_INTRIN(_mm_getexp_round_ss) \
VECTOR_INTRIN(_mm_getmant_pd) \
VECTOR_INTRIN(_mm_getmant_ps) \
VECTOR_INTRIN(_mm_getmant_round_sd) \
VECTOR_INTRIN(_mm_getmant_round_ss) \
VECTOR_INTRIN(_mm_hadd_epi16) \
VECTOR_INTRIN(_mm_hadd_epi32) \
VECTOR_INTRIN(_mm_hadd_pd) \
VECTOR_INTRIN(_mm_hadd_pi16) \
VECTOR_INTRIN(_mm_hadd_pi32) \
VECTOR_INTRIN(_mm_hadd_ps) \
VECTOR_INTRIN(_mm_hadds_epi16) \
VECTOR_INTRIN(_mm_hadds_pi16) \
VECTOR_INTRIN(_mm_hsub_pd) \
VECTOR_INTRIN(_mm_hsub_ps) \
VECTOR_INTRIN(_mm_hypot_pd) \
VECTOR_INTRIN(_mm_hypot_ps) \
VECTOR_INTRIN(_mm_i32gather_epi32) \
VECTOR_INTRIN(_mm_i32gather_epi64) \
VECTOR_INTRIN(_mm_i32gather_pd) \
VECTOR_INTRIN(_mm_i32gather_ps) \
VECTOR_INTRIN(_mm_i32scatter_epi32) \
VECTOR_INTRIN(_mm_i32scatter_epi64) \
VECTOR_INTRIN(_mm_i32scatter_pd) \
VECTOR_INTRIN(_mm_i32scatter_ps) \
VECTOR_INTRIN(_mm_i64gather_epi32) \
VECTOR_INTRIN(_mm_i64gather_epi64) \
VECTOR_INTRIN(_mm_i64gather_pd) \
VECTOR_INTRIN(_mm_i64gather_ps) \
VECTOR_INTRIN(_mm_i64scatter_epi32) \
VECTOR_INTRIN(_mm_i64scatter_epi64) \
VECTOR_INTRIN(_mm_i64scatter_pd) \
VECTOR_INTRIN(_mm_i64scatter_ps) \
VECTOR_INTRIN(_mm_idivrem_epi32) \
VECTOR_INTRIN(_mm_insert_epi16) \
VECTOR_INTRIN(_mm_insert_epi32) \
VECTOR_INTRIN(_mm_insert_epi64) \
VECTOR_INTRIN(_mm_insert_ps) \
VECTOR_INTRIN(_mm_invcbrt_pd) \
VECTOR_INTRIN(_mm_invcbrt_ps) \
VECTOR_INTRIN(_mm_invsqrt_pd) \
VECTOR_INTRIN(_mm_invsqrt_ps) \
VECTOR_INTRIN(_mm_lddqu_si128) \
VECTOR_INTRIN(_mm_lfence) \
VECTOR_INTRIN(_mm_load1_pd) \
VECTOR_INTRIN(_mm_loaddup_pd) \
VECTOR_INTRIN(_mm_loadh_pd) \
VECTOR_INTRIN(_mm_loadh_pi) \
VECTOR_INTRIN(_mm_loadl_epi64) \
VECTOR_INTRIN(_mm_loadl_pd) \
VECTOR_INTRIN(_mm_loadl_pi) \
VECTOR_INTRIN(_mm_load_pd) \
VECTOR_INTRIN(_mm_load_ps) \
VECTOR_INTRIN(_mm_load_ps1) \
VECTOR_INTRIN(_mm_loadr_pd) \
VECTOR_INTRIN(_mm_loadr_ps) \
VECTOR_INTRIN(_mm_load_sd) \
VECTOR_INTRIN(_mm_load_si128) \
VECTOR_INTRIN(_mm_load_ss) \
VECTOR_INTRIN(_mm_loadu_pd) \
VECTOR_INTRIN(_mm_loadu_ps) \
VECTOR_INTRIN(_mm_loadu_si128) \
VECTOR_INTRIN(_mm_loadu_si16) \
VECTOR_INTRIN(_mm_loadu_si32) \
VECTOR_INTRIN(_mm_loadu_si64) \
VECTOR_INTRIN(_mm_log10_pd) \
VECTOR_INTRIN(_mm_log10_ps) \
VECTOR_INTRIN(_mm_log1p_pd) \
VECTOR_INTRIN(_mm_log1p_ps) \
VECTOR_INTRIN(_mm_log2_pd) \
VECTOR_INTRIN(_mm_log2_ps) \
VECTOR_INTRIN(_mm_logb_pd) \
VECTOR_INTRIN(_mm_logb_ps) \
VECTOR_INTRIN(_mm_log_pd) \
VECTOR_INTRIN(_mm_log_ps) \
VECTOR_INTRIN(_mm_lzcnt_epi32) \
VECTOR_INTRIN(_mm_lzcnt_epi64) \
VECTOR_INTRIN(_mm_madd52hi_epu64) \
VECTOR_INTRIN(_mm_madd52lo_epu64) \
VECTOR_INTRIN(_mm_madd_epi16) \
VECTOR_INTRIN(_mm_mask2_permutex2var_epi16) \
VECTOR_INTRIN(_mm_mask2_permutex2var_epi32) \
VECTOR_INTRIN(_mm_mask2_permutex2var_epi64) \
VECTOR_INTRIN(_mm_mask2_permutex2var_epi8) \
VECTOR_INTRIN(_mm_mask2_permutex2var_pd) \
VECTOR_INTRIN(_mm_mask2_permutex2var_ps) \
VECTOR_INTRIN(_mm_mask3_fmadd_pd) \
VECTOR_INTRIN(_mm_mask3_fmadd_ps) \
VECTOR_INTRIN(_mm_mask3_fmadd_round_sd) \
VECTOR_INTRIN(_mm_mask3_fmadd_round_ss) \
VECTOR_INTRIN(_mm_mask3_fmaddsub_pd) \
VECTOR_INTRIN(_mm_mask3_fmaddsub_ps) \
VECTOR_INTRIN(_mm_mask3_fmsubadd_pd) \
VECTOR_INTRIN(_mm_mask3_fmsubadd_ps) \
VECTOR_INTRIN(_mm_mask3_fmsub_pd) \
VECTOR_INTRIN(_mm_mask3_fmsub_ps) \
VECTOR_INTRIN(_mm_mask3_fmsub_round_sd) \
VECTOR_INTRIN(_mm_mask3_fmsub_round_ss) \
VECTOR_INTRIN(_mm_mask3_fnmadd_pd) \
VECTOR_INTRIN(_mm_mask3_fnmadd_ps) \
VECTOR_INTRIN(_mm_mask3_fnmadd_round_sd) \
VECTOR_INTRIN(_mm_mask3_fnmadd_round_ss) \
VECTOR_INTRIN(_mm_mask3_fnmsub_pd) \
VECTOR_INTRIN(_mm_mask3_fnmsub_ps) \
VECTOR_INTRIN(_mm_mask3_fnmsub_round_sd) \
VECTOR_INTRIN(_mm_mask3_fnmsub_round_ss) \
VECTOR_INTRIN(_mm_mask_abs_epi16) \
VECTOR_INTRIN(_mm_mask_abs_epi32) \
VECTOR_INTRIN(_mm_mask_abs_epi64) \
VECTOR_INTRIN(_mm_mask_abs_epi8) \
VECTOR_INTRIN(_mm_mask_add_epi16) \
VECTOR_INTRIN(_mm_mask_add_epi32) \
VECTOR_INTRIN(_mm_mask_add_epi64) \
VECTOR_INTRIN(_mm_mask_add_epi8) \
VECTOR_INTRIN(_mm_mask_add_pd) \
VECTOR_INTRIN(_mm_mask_add_ps) \
VECTOR_INTRIN(_mm_mask_add_round_sd) \
VECTOR_INTRIN(_mm_mask_add_round_ss) \
VECTOR_INTRIN(_mm_mask_adds_epi16) \
VECTOR_INTRIN(_mm_mask_adds_epi8) \
VECTOR_INTRIN(_mm_mask_adds_epu16) \
VECTOR_INTRIN(_mm_mask_adds_epu8) \
VECTOR_INTRIN(_mm_mask_alignr_epi32) \
VECTOR_INTRIN(_mm_mask_alignr_epi64) \
VECTOR_INTRIN(_mm_mask_alignr_epi8) \
VECTOR_INTRIN(_mm_mask_and_epi32) \
VECTOR_INTRIN(_mm_mask_and_epi64) \
VECTOR_INTRIN(_mm_mask_andnot_epi32) \
VECTOR_INTRIN(_mm_mask_andnot_epi64) \
VECTOR_INTRIN(_mm_mask_andnot_pd) \
VECTOR_INTRIN(_mm_mask_andnot_ps) \
VECTOR_INTRIN(_mm_mask_and_pd) \
VECTOR_INTRIN(_mm_mask_and_ps) \
VECTOR_INTRIN(_mm_mask_avg_epu16) \
VECTOR_INTRIN(_mm_mask_avg_epu8) \
VECTOR_INTRIN(_mm_mask_blend_epi16) \
VECTOR_INTRIN(_mm_mask_blend_epi32) \
VECTOR_INTRIN(_mm_mask_blend_epi64) \
VECTOR_INTRIN(_mm_mask_blend_epi8) \
VECTOR_INTRIN(_mm_mask_blend_pd) \
VECTOR_INTRIN(_mm_mask_blend_ps) \
VECTOR_INTRIN(_mm_mask_broadcastb_epi8) \
VECTOR_INTRIN(_mm_mask_broadcastd_epi32) \
VECTOR_INTRIN(_mm_mask_broadcast_i32x2) \
VECTOR_INTRIN(_mm_mask_broadcastq_epi64) \
VECTOR_INTRIN(_mm_mask_broadcastss_ps) \
VECTOR_INTRIN(_mm_mask_broadcastw_epi16) \
VECTOR_INTRIN(_mm_mask_cmp_epi16_mask) \
VECTOR_INTRIN(_mm_mask_cmp_epi32_mask) \
VECTOR_INTRIN(_mm_mask_cmp_epi64_mask) \
VECTOR_INTRIN(_mm_mask_cmp_epi8_mask) \
VECTOR_INTRIN(_mm_mask_cmp_epu16_mask) \
VECTOR_INTRIN(_mm_mask_cmp_epu32_mask) \
VECTOR_INTRIN(_mm_mask_cmp_epu64_mask) \
VECTOR_INTRIN(_mm_mask_cmp_epu8_mask) \
VECTOR_INTRIN(_mm_mask_cmp_pd_mask) \
VECTOR_INTRIN(_mm_mask_cmp_ps_mask) \
VECTOR_INTRIN(_mm_mask_cmp_round_sd_mask) \
VECTOR_INTRIN(_mm_mask_cmp_round_ss_mask) \
VECTOR_INTRIN(_mm_mask_compress_epi32) \
VECTOR_INTRIN(_mm_mask_compress_epi64) \
VECTOR_INTRIN(_mm_mask_compress_pd) \
VECTOR_INTRIN(_mm_mask_compress_ps) \
VECTOR_INTRIN(_mm_mask_compressstoreu_epi32) \
VECTOR_INTRIN(_mm_mask_compressstoreu_epi64) \
VECTOR_INTRIN(_mm_mask_compressstoreu_pd) \
VECTOR_INTRIN(_mm_mask_compressstoreu_ps) \
VECTOR_INTRIN(_mm_mask_conflict_epi32) \
VECTOR_INTRIN(_mm_mask_conflict_epi64) \
VECTOR_INTRIN(_mm_mask_cvtepi16_epi32) \
VECTOR_INTRIN(_mm_mask_cvtepi16_epi64) \
VECTOR_INTRIN(_mm_mask_cvtepi16_epi8) \
VECTOR_INTRIN(_mm_mask_cvtepi16_storeu_epi8) \
VECTOR_INTRIN(_mm_mask_cvtepi32_epi16) \
VECTOR_INTRIN(_mm_mask_cvtepi32_epi64) \
VECTOR_INTRIN(_mm_mask_cvtepi32_epi8) \
VECTOR_INTRIN(_mm_mask_cvtepi32_pd) \
VECTOR_INTRIN(_mm_mask_cvtepi32_ps) \
VECTOR_INTRIN(_mm_mask_cvtepi32_storeu_epi16) \
VECTOR_INTRIN(_mm_mask_cvtepi32_storeu_epi8) \
VECTOR_INTRIN(_mm_mask_cvtepi64_epi16) \
VECTOR_INTRIN(_mm_mask_cvtepi64_epi32) \
VECTOR_INTRIN(_mm_mask_cvtepi64_epi8) \
VECTOR_INTRIN(_mm_mask_cvtepi64_pd) \
VECTOR_INTRIN(_mm_mask_cvtepi64_ps) \
VECTOR_INTRIN(_mm_mask_cvtepi64_storeu_epi16) \
VECTOR_INTRIN(_mm_mask_cvtepi64_storeu_epi32) \
VECTOR_INTRIN(_mm_mask_cvtepi64_storeu_epi8) \
VECTOR_INTRIN(_mm_mask_cvtepi8_epi16) \
VECTOR_INTRIN(_mm_mask_cvtepi8_epi32) \
VECTOR_INTRIN(_mm_mask_cvtepi8_epi64) \
VECTOR_INTRIN(_mm_mask_cvtepu16_epi32) \
VECTOR_INTRIN(_mm_mask_cvtepu16_epi64) \
VECTOR_INTRIN(_mm_mask_cvtepu32_epi64) \
VECTOR_INTRIN(_mm_mask_cvtepu32_pd) \
VECTOR_INTRIN(_mm_mask_cvtepu64_pd) \
VECTOR_INTRIN(_mm_mask_cvtepu64_ps) \
VECTOR_INTRIN(_mm_mask_cvtepu8_epi16) \
VECTOR_INTRIN(_mm_mask_cvtepu8_epi32) \
VECTOR_INTRIN(_mm_mask_cvtepu8_epi64) \
VECTOR_INTRIN(_mm_mask_cvtpd_epi32) \
VECTOR_INTRIN(_mm_mask_cvtpd_epi64) \
VECTOR_INTRIN(_mm_mask_cvtpd_epu32) \
VECTOR_INTRIN(_mm_mask_cvtpd_epu64) \
VECTOR_INTRIN(_mm_mask_cvtpd_ps) \
VECTOR_INTRIN(_mm_mask_cvtph_ps) \
VECTOR_INTRIN(_mm_mask_cvtps_epi32) \
VECTOR_INTRIN(_mm_mask_cvtps_epi64) \
VECTOR_INTRIN(_mm_mask_cvtps_epu32) \
VECTOR_INTRIN(_mm_mask_cvtps_epu64) \
VECTOR_INTRIN(_mm_mask_cvt_roundps_ph) \
VECTOR_INTRIN(_mm_mask_cvt_roundsd_ss) \
VECTOR_INTRIN(_mm_mask_cvt_roundss_sd) \
VECTOR_INTRIN(_mm_mask_cvtsepi16_epi8) \
VECTOR_INTRIN(_mm_mask_cvtsepi16_storeu_epi8) \
VECTOR_INTRIN(_mm_mask_cvtsepi32_epi16) \
VECTOR_INTRIN(_mm_mask_cvtsepi32_epi8) \
VECTOR_INTRIN(_mm_mask_cvtsepi32_storeu_epi16) \
VECTOR_INTRIN(_mm_mask_cvtsepi32_storeu_epi8) \
VECTOR_INTRIN(_mm_mask_cvtsepi64_epi16) \
VECTOR_INTRIN(_mm_mask_cvtsepi64_epi32) \
VECTOR_INTRIN(_mm_mask_cvtsepi64_epi8) \
VECTOR_INTRIN(_mm_mask_cvtsepi64_storeu_epi16) \
VECTOR_INTRIN(_mm_mask_cvtsepi64_storeu_epi32) \
VECTOR_INTRIN(_mm_mask_cvtsepi64_storeu_epi8) \
VECTOR_INTRIN(_mm_mask_cvttpd_epi32) \
VECTOR_INTRIN(_mm_mask_cvttpd_epi64) \
VECTOR_INTRIN(_mm_mask_cvttpd_epu32) \
VECTOR_INTRIN(_mm_mask_cvttpd_epu64) \
VECTOR_INTRIN(_mm_mask_cvttps_epi32) \
VECTOR_INTRIN(_mm_mask_cvttps_epi64) \
VECTOR_INTRIN(_mm_mask_cvttps_epu32) \
VECTOR_INTRIN(_mm_mask_cvttps_epu64) \
VECTOR_INTRIN(_mm_mask_cvtusepi16_epi8) \
VECTOR_INTRIN(_mm_mask_cvtusepi16_storeu_epi8) \
VECTOR_INTRIN(_mm_mask_cvtusepi32_epi16) \
VECTOR_INTRIN(_mm_mask_cvtusepi32_epi8) \
VECTOR_INTRIN(_mm_mask_cvtusepi32_storeu_epi16) \
VECTOR_INTRIN(_mm_mask_cvtusepi32_storeu_epi8) \
VECTOR_INTRIN(_mm_mask_cvtusepi64_epi16) \
VECTOR_INTRIN(_mm_mask_cvtusepi64_epi32) \
VECTOR_INTRIN(_mm_mask_cvtusepi64_epi8) \
VECTOR_INTRIN(_mm_mask_cvtusepi64_storeu_epi16) \
VECTOR_INTRIN(_mm_mask_cvtusepi64_storeu_epi32) \
VECTOR_INTRIN(_mm_mask_cvtusepi64_storeu_epi8) \
VECTOR_INTRIN(_mm_mask_dbsad_epu8) \
VECTOR_INTRIN(_mm_mask_div_pd) \
VECTOR_INTRIN(_mm_mask_div_ps) \
VECTOR_INTRIN(_mm_mask_div_round_sd) \
VECTOR_INTRIN(_mm_mask_div_round_ss) \
VECTOR_INTRIN(_mm_mask_expand_epi32) \
VECTOR_INTRIN(_mm_mask_expand_epi64) \
VECTOR_INTRIN(_mm_mask_expandloadu_epi32) \
VECTOR_INTRIN(_mm_mask_expandloadu_epi64) \
VECTOR_INTRIN(_mm_mask_expandloadu_pd) \
VECTOR_INTRIN(_mm_mask_expandloadu_ps) \
VECTOR_INTRIN(_mm_mask_expand_pd) \
VECTOR_INTRIN(_mm_mask_expand_ps) \
VECTOR_INTRIN(_mm_mask_fixupimm_pd) \
VECTOR_INTRIN(_mm_mask_fixupimm_ps) \
VECTOR_INTRIN(_mm_mask_fixupimm_round_sd) \
VECTOR_INTRIN(_mm_mask_fixupimm_round_ss) \
VECTOR_INTRIN(_mm_mask_fmadd_pd) \
VECTOR_INTRIN(_mm_mask_fmadd_ps) \
VECTOR_INTRIN(_mm_mask_fmadd_round_sd) \
VECTOR_INTRIN(_mm_mask_fmadd_round_ss) \
VECTOR_INTRIN(_mm_mask_fmaddsub_pd) \
VECTOR_INTRIN(_mm_mask_fmaddsub_ps) \
VECTOR_INTRIN(_mm_mask_fmsubadd_pd) \
VECTOR_INTRIN(_mm_mask_fmsubadd_ps) \
VECTOR_INTRIN(_mm_mask_fmsub_pd) \
VECTOR_INTRIN(_mm_mask_fmsub_ps) \
VECTOR_INTRIN(_mm_mask_fmsub_round_sd) \
VECTOR_INTRIN(_mm_mask_fmsub_round_ss) \
VECTOR_INTRIN(_mm_mask_fnmadd_pd) \
VECTOR_INTRIN(_mm_mask_fnmadd_ps) \
VECTOR_INTRIN(_mm_mask_fnmadd_round_sd) \
VECTOR_INTRIN(_mm_mask_fnmadd_round_ss) \
VECTOR_INTRIN(_mm_mask_fnmsub_pd) \
VECTOR_INTRIN(_mm_mask_fnmsub_ps) \
VECTOR_INTRIN(_mm_mask_fnmsub_round_sd) \
VECTOR_INTRIN(_mm_mask_fnmsub_round_ss) \
VECTOR_INTRIN(_mm_mask_fpclass_pd_mask) \
VECTOR_INTRIN(_mm_mask_fpclass_ps_mask) \
VECTOR_INTRIN(_mm_mask_fpclass_sd_mask) \
VECTOR_INTRIN(_mm_mask_fpclass_ss_mask) \
VECTOR_INTRIN(_mm_mask_getexp_pd) \
VECTOR_INTRIN(_mm_mask_getexp_ps) \
VECTOR_INTRIN(_mm_mask_getexp_round_sd) \
VECTOR_INTRIN(_mm_mask_getexp_round_ss) \
VECTOR_INTRIN(_mm_mask_getmant_pd) \
VECTOR_INTRIN(_mm_mask_getmant_ps) \
VECTOR_INTRIN(_mm_mask_getmant_round_sd) \
VECTOR_INTRIN(_mm_mask_getmant_round_ss) \
VECTOR_INTRIN(_mm_mask_i32gather_epi32) \
VECTOR_INTRIN(_mm_mask_i32gather_epi64) \
VECTOR_INTRIN(_mm_mask_i32gather_pd) \
VECTOR_INTRIN(_mm_mask_i32gather_ps) \
VECTOR_INTRIN(_mm_mask_i32scatter_epi32) \
VECTOR_INTRIN(_mm_mask_i32scatter_epi64) \
VECTOR_INTRIN(_mm_mask_i32scatter_pd) \
VECTOR_INTRIN(_mm_mask_i32scatter_ps) \
VECTOR_INTRIN(_mm_mask_i64gather_epi32) \
VECTOR_INTRIN(_mm_mask_i64gather_epi64) \
VECTOR_INTRIN(_mm_mask_i64gather_pd) \
VECTOR_INTRIN(_mm_mask_i64gather_ps) \
VECTOR_INTRIN(_mm_mask_i64scatter_epi32) \
VECTOR_INTRIN(_mm_mask_i64scatter_epi64) \
VECTOR_INTRIN(_mm_mask_i64scatter_pd) \
VECTOR_INTRIN(_mm_mask_i64scatter_ps) \
VECTOR_INTRIN(_mm_mask_load_epi32) \
VECTOR_INTRIN(_mm_maskload_epi32) \
VECTOR_INTRIN(_mm_mask_load_epi64) \
VECTOR_INTRIN(_mm_maskload_epi64) \
VECTOR_INTRIN(_mm_mask_load_pd) \
VECTOR_INTRIN(_mm_maskload_pd) \
VECTOR_INTRIN(_mm_mask_load_ps) \
VECTOR_INTRIN(_mm_maskload_ps) \
VECTOR_INTRIN(_mm_mask_load_sd) \
VECTOR_INTRIN(_mm_mask_load_ss) \
VECTOR_INTRIN(_mm_mask_loadu_epi16) \
VECTOR_INTRIN(_mm_mask_loadu_epi32) \
VECTOR_INTRIN(_mm_mask_loadu_epi64) \
VECTOR_INTRIN(_mm_mask_loadu_epi8) \
VECTOR_INTRIN(_mm_mask_loadu_pd) \
VECTOR_INTRIN(_mm_mask_loadu_ps) \
VECTOR_INTRIN(_mm_mask_lzcnt_epi32) \
VECTOR_INTRIN(_mm_mask_lzcnt_epi64) \
VECTOR_INTRIN(_mm_mask_madd52hi_epu64) \
VECTOR_INTRIN(_mm_mask_madd52lo_epu64) \
VECTOR_INTRIN(_mm_mask_madd_epi16) \
VECTOR_INTRIN(_mm_mask_maddubs_epi16) \
VECTOR_INTRIN(_mm_mask_max_epi16) \
VECTOR_INTRIN(_mm_mask_max_epi32) \
VECTOR_INTRIN(_mm_mask_max_epi64) \
VECTOR_INTRIN(_mm_mask_max_epi8) \
VECTOR_INTRIN(_mm_mask_max_epu16) \
VECTOR_INTRIN(_mm_mask_max_epu32) \
VECTOR_INTRIN(_mm_mask_max_epu64) \
VECTOR_INTRIN(_mm_mask_max_epu8) \
VECTOR_INTRIN(_mm_mask_max_pd) \
VECTOR_INTRIN(_mm_mask_max_ps) \
VECTOR_INTRIN(_mm_mask_max_round_sd) \
VECTOR_INTRIN(_mm_mask_max_round_ss) \
VECTOR_INTRIN(_mm_mask_min_epi16) \
VECTOR_INTRIN(_mm_mask_min_epi32) \
VECTOR_INTRIN(_mm_mask_min_epi64) \
VECTOR_INTRIN(_mm_mask_min_epi8) \
VECTOR_INTRIN(_mm_mask_min_epu16) \
VECTOR_INTRIN(_mm_mask_min_epu32) \
VECTOR_INTRIN(_mm_mask_min_epu64) \
VECTOR_INTRIN(_mm_mask_min_epu8) \
VECTOR_INTRIN(_mm_mask_min_pd) \
VECTOR_INTRIN(_mm_mask_min_ps) \
VECTOR_INTRIN(_mm_mask_min_round_sd) \
VECTOR_INTRIN(_mm_mask_min_round_ss) \
VECTOR_INTRIN(_mm_mask_movedup_pd) \
VECTOR_INTRIN(_mm_mask_movehdup_ps) \
VECTOR_INTRIN(_mm_mask_moveldup_ps) \
VECTOR_INTRIN(_mm_mask_mov_epi16) \
VECTOR_INTRIN(_mm_mask_mov_epi32) \
VECTOR_INTRIN(_mm_mask_mov_epi64) \
VECTOR_INTRIN(_mm_mask_mov_epi8) \
VECTOR_INTRIN(_mm_mask_move_sd) \
VECTOR_INTRIN(_mm_mask_move_ss) \
VECTOR_INTRIN(_mm_maskmoveu_si128) \
VECTOR_INTRIN(_mm_mask_mov_pd) \
VECTOR_INTRIN(_mm_mask_mov_ps) \
VECTOR_INTRIN(_mm_mask_mul_epi32) \
VECTOR_INTRIN(_mm_mask_mul_epu32) \
VECTOR_INTRIN(_mm_mask_mulhi_epi16) \
VECTOR_INTRIN(_mm_mask_mulhi_epu16) \
VECTOR_INTRIN(_mm_mask_mulhrs_epi16) \
VECTOR_INTRIN(_mm_mask_mullo_epi16) \
VECTOR_INTRIN(_mm_mask_mullo_epi32) \
VECTOR_INTRIN(_mm_mask_mullo_epi64) \
VECTOR_INTRIN(_mm_mask_mul_pd) \
VECTOR_INTRIN(_mm_mask_mul_ps) \
VECTOR_INTRIN(_mm_mask_mul_round_sd) \
VECTOR_INTRIN(_mm_mask_mul_round_ss) \
VECTOR_INTRIN(_mm_mask_multishift_epi64_epi8) \
VECTOR_INTRIN(_mm_mask_or_epi32) \
VECTOR_INTRIN(_mm_mask_or_epi64) \
VECTOR_INTRIN(_mm_mask_or_pd) \
VECTOR_INTRIN(_mm_mask_or_ps) \
VECTOR_INTRIN(_mm_mask_packs_epi16) \
VECTOR_INTRIN(_mm_mask_packs_epi32) \
VECTOR_INTRIN(_mm_mask_packus_epi16) \
VECTOR_INTRIN(_mm_mask_packus_epi32) \
VECTOR_INTRIN(_mm_mask_permute_pd) \
VECTOR_INTRIN(_mm_mask_permute_ps) \
VECTOR_INTRIN(_mm_mask_permutevar_pd) \
VECTOR_INTRIN(_mm_mask_permutevar_ps) \
VECTOR_INTRIN(_mm_mask_permutex2var_epi16) \
VECTOR_INTRIN(_mm_mask_permutex2var_epi32) \
VECTOR_INTRIN(_mm_mask_permutex2var_epi64) \
VECTOR_INTRIN(_mm_mask_permutex2var_epi8) \
VECTOR_INTRIN(_mm_mask_permutex2var_pd) \
VECTOR_INTRIN(_mm_mask_permutex2var_ps) \
VECTOR_INTRIN(_mm_mask_permutexvar_epi16) \
VECTOR_INTRIN(_mm_mask_permutexvar_epi8) \
VECTOR_INTRIN(_mm_mask_range_pd) \
VECTOR_INTRIN(_mm_mask_range_ps) \
VECTOR_INTRIN(_mm_mask_range_round_sd) \
VECTOR_INTRIN(_mm_mask_range_round_ss) \
VECTOR_INTRIN(_mm_mask_rcp14_pd) \
VECTOR_INTRIN(_mm_mask_rcp14_ps) \
VECTOR_INTRIN(_mm_mask_rcp14_sd) \
VECTOR_INTRIN(_mm_mask_rcp14_ss) \
VECTOR_INTRIN(_mm_mask_rcp28_round_sd) \
VECTOR_INTRIN(_mm_mask_rcp28_round_ss) \
VECTOR_INTRIN(_mm_mask_reduce_pd) \
VECTOR_INTRIN(_mm_mask_reduce_ps) \
VECTOR_INTRIN(_mm_mask_reduce_round_sd) \
VECTOR_INTRIN(_mm_mask_reduce_round_ss) \
VECTOR_INTRIN(_mm_mask_reduce_sd) \
VECTOR_INTRIN(_mm_mask_reduce_ss) \
VECTOR_INTRIN(_mm_mask_rol_epi32) \
VECTOR_INTRIN(_mm_mask_rol_epi64) \
VECTOR_INTRIN(_mm_mask_rolv_epi32) \
VECTOR_INTRIN(_mm_mask_rolv_epi64) \
VECTOR_INTRIN(_mm_mask_ror_epi32) \
VECTOR_INTRIN(_mm_mask_ror_epi64) \
VECTOR_INTRIN(_mm_mask_rorv_epi32) \
VECTOR_INTRIN(_mm_mask_rorv_epi64) \
VECTOR_INTRIN(_mm_mask_roundscale_pd) \
VECTOR_INTRIN(_mm_mask_roundscale_ps) \
VECTOR_INTRIN(_mm_mask_roundscale_round_sd) \
VECTOR_INTRIN(_mm_mask_roundscale_round_ss) \
VECTOR_INTRIN(_mm_mask_rsqrt14_pd) \
VECTOR_INTRIN(_mm_mask_rsqrt14_ps) \
VECTOR_INTRIN(_mm_mask_rsqrt14_sd) \
VECTOR_INTRIN(_mm_mask_rsqrt14_ss) \
VECTOR_INTRIN(_mm_mask_rsqrt28_round_sd) \
VECTOR_INTRIN(_mm_mask_rsqrt28_round_ss) \
VECTOR_INTRIN(_mm_mask_scalef_pd) \
VECTOR_INTRIN(_mm_mask_scalef_ps) \
VECTOR_INTRIN(_mm_mask_scalef_round_sd) \
VECTOR_INTRIN(_mm_mask_scalef_round_ss) \
VECTOR_INTRIN(_mm_mask_set1_epi16) \
VECTOR_INTRIN(_mm_mask_set1_epi32) \
VECTOR_INTRIN(_mm_mask_set1_epi64) \
VECTOR_INTRIN(_mm_mask_set1_epi8) \
VECTOR_INTRIN(_mm_mask_shuffle_epi32) \
VECTOR_INTRIN(_mm_mask_shuffle_epi8) \
VECTOR_INTRIN(_mm_mask_shufflehi_epi16) \
VECTOR_INTRIN(_mm_mask_shufflelo_epi16) \
VECTOR_INTRIN(_mm_mask_shuffle_pd) \
VECTOR_INTRIN(_mm_mask_shuffle_ps) \
VECTOR_INTRIN(_mm_mask_sll_epi16) \
VECTOR_INTRIN(_mm_mask_sll_epi32) \
VECTOR_INTRIN(_mm_mask_sll_epi64) \
VECTOR_INTRIN(_mm_mask_slli_epi16) \
VECTOR_INTRIN(_mm_mask_slli_epi32) \
VECTOR_INTRIN(_mm_mask_slli_epi64) \
VECTOR_INTRIN(_mm_mask_sllv_epi16) \
VECTOR_INTRIN(_mm_mask_sllv_epi32) \
VECTOR_INTRIN(_mm_mask_sllv_epi64) \
VECTOR_INTRIN(_mm_mask_sqrt_pd) \
VECTOR_INTRIN(_mm_mask_sqrt_ps) \
VECTOR_INTRIN(_mm_mask_sqrt_round_sd) \
VECTOR_INTRIN(_mm_mask_sqrt_round_ss) \
VECTOR_INTRIN(_mm_mask_sra_epi16) \
VECTOR_INTRIN(_mm_mask_sra_epi32) \
VECTOR_INTRIN(_mm_mask_sra_epi64) \
VECTOR_INTRIN(_mm_mask_srai_epi16) \
VECTOR_INTRIN(_mm_mask_srai_epi32) \
VECTOR_INTRIN(_mm_mask_srai_epi64) \
VECTOR_INTRIN(_mm_mask_srav_epi16) \
VECTOR_INTRIN(_mm_mask_srav_epi32) \
VECTOR_INTRIN(_mm_mask_srav_epi64) \
VECTOR_INTRIN(_mm_mask_srl_epi16) \
VECTOR_INTRIN(_mm_mask_srl_epi32) \
VECTOR_INTRIN(_mm_mask_srl_epi64) \
VECTOR_INTRIN(_mm_mask_srli_epi16) \
VECTOR_INTRIN(_mm_mask_srli_epi32) \
VECTOR_INTRIN(_mm_mask_srli_epi64) \
VECTOR_INTRIN(_mm_mask_srlv_epi16) \
VECTOR_INTRIN(_mm_mask_srlv_epi32) \
VECTOR_INTRIN(_mm_mask_srlv_epi64) \
VECTOR_INTRIN(_mm_mask_store_epi32) \
VECTOR_INTRIN(_mm_maskstore_epi32) \
VECTOR_INTRIN(_mm_mask_store_epi64) \
VECTOR_INTRIN(_mm_maskstore_epi64) \
VECTOR_INTRIN(_mm_mask_store_pd) \
VECTOR_INTRIN(_mm_maskstore_pd) \
VECTOR_INTRIN(_mm_mask_store_ps) \
VECTOR_INTRIN(_mm_maskstore_ps) \
VECTOR_INTRIN(_mm_mask_store_sd) \
VECTOR_INTRIN(_mm_mask_store_ss) \
VECTOR_INTRIN(_mm_mask_storeu_epi16) \
VECTOR_INTRIN(_mm_mask_storeu_epi32) \
VECTOR_INTRIN(_mm_mask_storeu_epi64) \
VECTOR_INTRIN(_mm_mask_storeu_epi8) \
VECTOR_INTRIN(_mm_mask_storeu_pd) \
VECTOR_INTRIN(_mm_mask_storeu_ps) \
VECTOR_INTRIN(_mm_mask_sub_epi16) \
VECTOR_INTRIN(_mm_mask_sub_epi32) \
VECTOR_INTRIN(_mm_mask_sub_epi64) \
VECTOR_INTRIN(_mm_mask_sub_epi8) \
VECTOR_INTRIN(_mm_mask_sub_pd) \
VECTOR_INTRIN(_mm_mask_sub_ps) \
VECTOR_INTRIN(_mm_mask_sub_round_sd) \
VECTOR_INTRIN(_mm_mask_sub_round_ss) \
VECTOR_INTRIN(_mm_mask_subs_epi16) \
VECTOR_INTRIN(_mm_mask_subs_epi8) \
VECTOR_INTRIN(_mm_mask_subs_epu16) \
VECTOR_INTRIN(_mm_mask_subs_epu8) \
VECTOR_INTRIN(_mm_mask_ternarylogic_epi32) \
VECTOR_INTRIN(_mm_mask_ternarylogic_epi64) \
VECTOR_INTRIN(_mm_mask_test_epi16_mask) \
VECTOR_INTRIN(_mm_mask_test_epi32_mask) \
VECTOR_INTRIN(_mm_mask_test_epi64_mask) \
VECTOR_INTRIN(_mm_mask_test_epi8_mask) \
VECTOR_INTRIN(_mm_mask_testn_epi16_mask) \
VECTOR_INTRIN(_mm_mask_testn_epi32_mask) \
VECTOR_INTRIN(_mm_mask_testn_epi64_mask) \
VECTOR_INTRIN(_mm_mask_testn_epi8_mask) \
VECTOR_INTRIN(_mm_mask_unpackhi_epi16) \
VECTOR_INTRIN(_mm_mask_unpackhi_epi32) \
VECTOR_INTRIN(_mm_mask_unpackhi_epi64) \
VECTOR_INTRIN(_mm_mask_unpackhi_epi8) \
VECTOR_INTRIN(_mm_mask_unpackhi_pd) \
VECTOR_INTRIN(_mm_mask_unpackhi_ps) \
VECTOR_INTRIN(_mm_mask_unpacklo_epi16) \
VECTOR_INTRIN(_mm_mask_unpacklo_epi32) \
VECTOR_INTRIN(_mm_mask_unpacklo_epi64) \
VECTOR_INTRIN(_mm_mask_unpacklo_epi8) \
VECTOR_INTRIN(_mm_mask_unpacklo_pd) \
VECTOR_INTRIN(_mm_mask_unpacklo_ps) \
VECTOR_INTRIN(_mm_mask_xor_epi32) \
VECTOR_INTRIN(_mm_mask_xor_epi64) \
VECTOR_INTRIN(_mm_mask_xor_pd) \
VECTOR_INTRIN(_mm_mask_xor_ps) \
VECTOR_INTRIN(_mm_maskz_abs_epi16) \
VECTOR_INTRIN(_mm_maskz_abs_epi32) \
VECTOR_INTRIN(_mm_maskz_abs_epi64) \
VECTOR_INTRIN(_mm_maskz_abs_epi8) \
VECTOR_INTRIN(_mm_maskz_add_epi16) \
VECTOR_INTRIN(_mm_maskz_add_epi32) \
VECTOR_INTRIN(_mm_maskz_add_epi64) \
VECTOR_INTRIN(_mm_maskz_add_epi8) \
VECTOR_INTRIN(_mm_maskz_add_pd) \
VECTOR_INTRIN(_mm_maskz_add_ps) \
VECTOR_INTRIN(_mm_maskz_add_round_sd) \
VECTOR_INTRIN(_mm_maskz_add_round_ss) \
VECTOR_INTRIN(_mm_maskz_adds_epi16) \
VECTOR_INTRIN(_mm_maskz_adds_epi8) \
VECTOR_INTRIN(_mm_maskz_adds_epu16) \
VECTOR_INTRIN(_mm_maskz_adds_epu8) \
VECTOR_INTRIN(_mm_maskz_alignr_epi32) \
VECTOR_INTRIN(_mm_maskz_alignr_epi64) \
VECTOR_INTRIN(_mm_maskz_alignr_epi8) \
VECTOR_INTRIN(_mm_maskz_and_epi32) \
VECTOR_INTRIN(_mm_maskz_and_epi64) \
VECTOR_INTRIN(_mm_maskz_andnot_epi32) \
VECTOR_INTRIN(_mm_maskz_andnot_epi64) \
VECTOR_INTRIN(_mm_maskz_andnot_pd) \
VECTOR_INTRIN(_mm_maskz_andnot_ps) \
VECTOR_INTRIN(_mm_maskz_and_pd) \
VECTOR_INTRIN(_mm_maskz_and_ps) \
VECTOR_INTRIN(_mm_maskz_avg_epu16) \
VECTOR_INTRIN(_mm_maskz_avg_epu8) \
VECTOR_INTRIN(_mm_maskz_broadcastb_epi8) \
VECTOR_INTRIN(_mm_maskz_broadcastd_epi32) \
VECTOR_INTRIN(_mm_maskz_broadcast_i32x2) \
VECTOR_INTRIN(_mm_maskz_broadcastq_epi64) \
VECTOR_INTRIN(_mm_maskz_broadcastss_ps) \
VECTOR_INTRIN(_mm_maskz_broadcastw_epi16) \
VECTOR_INTRIN(_mm_maskz_compress_epi32) \
VECTOR_INTRIN(_mm_maskz_compress_epi64) \
VECTOR_INTRIN(_mm_maskz_compress_pd) \
VECTOR_INTRIN(_mm_maskz_compress_ps) \
VECTOR_INTRIN(_mm_maskz_conflict_epi32) \
VECTOR_INTRIN(_mm_maskz_conflict_epi64) \
VECTOR_INTRIN(_mm_maskz_cvtepi16_epi32) \
VECTOR_INTRIN(_mm_maskz_cvtepi16_epi64) \
VECTOR_INTRIN(_mm_maskz_cvtepi16_epi8) \
VECTOR_INTRIN(_mm_maskz_cvtepi32_epi16) \
VECTOR_INTRIN(_mm_maskz_cvtepi32_epi64) \
VECTOR_INTRIN(_mm_maskz_cvtepi32_epi8) \
VECTOR_INTRIN(_mm_maskz_cvtepi32_pd) \
VECTOR_INTRIN(_mm_maskz_cvtepi32_ps) \
VECTOR_INTRIN(_mm_maskz_cvtepi64_epi16) \
VECTOR_INTRIN(_mm_maskz_cvtepi64_epi32) \
VECTOR_INTRIN(_mm_maskz_cvtepi64_epi8) \
VECTOR_INTRIN(_mm_maskz_cvtepi64_pd) \
VECTOR_INTRIN(_mm_maskz_cvtepi64_ps) \
VECTOR_INTRIN(_mm_maskz_cvtepi8_epi16) \
VECTOR_INTRIN(_mm_maskz_cvtepi8_epi32) \
VECTOR_INTRIN(_mm_maskz_cvtepi8_epi64) \
VECTOR_INTRIN(_mm_maskz_cvtepu16_epi32) \
VECTOR_INTRIN(_mm_maskz_cvtepu16_epi64) \
VECTOR_INTRIN(_mm_maskz_cvtepu32_epi64) \
VECTOR_INTRIN(_mm_maskz_cvtepu32_pd) \
VECTOR_INTRIN(_mm_maskz_cvtepu64_pd) \
VECTOR_INTRIN(_mm_maskz_cvtepu64_ps) \
VECTOR_INTRIN(_mm_maskz_cvtepu8_epi16) \
VECTOR_INTRIN(_mm_maskz_cvtepu8_epi32) \
VECTOR_INTRIN(_mm_maskz_cvtepu8_epi64) \
VECTOR_INTRIN(_mm_maskz_cvtpd_epi32) \
VECTOR_INTRIN(_mm_maskz_cvtpd_epi64) \
VECTOR_INTRIN(_mm_maskz_cvtpd_epu32) \
VECTOR_INTRIN(_mm_maskz_cvtpd_epu64) \
VECTOR_INTRIN(_mm_maskz_cvtpd_ps) \
VECTOR_INTRIN(_mm_maskz_cvtph_ps) \
VECTOR_INTRIN(_mm_maskz_cvtps_epi32) \
VECTOR_INTRIN(_mm_maskz_cvtps_epi64) \
VECTOR_INTRIN(_mm_maskz_cvtps_epu32) \
VECTOR_INTRIN(_mm_maskz_cvtps_epu64) \
VECTOR_INTRIN(_mm_maskz_cvt_roundps_ph) \
VECTOR_INTRIN(_mm_maskz_cvt_roundsd_ss) \
VECTOR_INTRIN(_mm_maskz_cvt_roundss_sd) \
VECTOR_INTRIN(_mm_maskz_cvtsepi16_epi8) \
VECTOR_INTRIN(_mm_maskz_cvtsepi32_epi16) \
VECTOR_INTRIN(_mm_maskz_cvtsepi32_epi8) \
VECTOR_INTRIN(_mm_maskz_cvtsepi64_epi16) \
VECTOR_INTRIN(_mm_maskz_cvtsepi64_epi32) \
VECTOR_INTRIN(_mm_maskz_cvtsepi64_epi8) \
VECTOR_INTRIN(_mm_maskz_cvttpd_epi32) \
VECTOR_INTRIN(_mm_maskz_cvttpd_epi64) \
VECTOR_INTRIN(_mm_maskz_cvttpd_epu32) \
VECTOR_INTRIN(_mm_maskz_cvttpd_epu64) \
VECTOR_INTRIN(_mm_maskz_cvttps_epi32) \
VECTOR_INTRIN(_mm_maskz_cvttps_epi64) \
VECTOR_INTRIN(_mm_maskz_cvttps_epu32) \
VECTOR_INTRIN(_mm_maskz_cvttps_epu64) \
VECTOR_INTRIN(_mm_maskz_cvtusepi16_epi8) \
VECTOR_INTRIN(_mm_maskz_cvtusepi32_epi16) \
VECTOR_INTRIN(_mm_maskz_cvtusepi32_epi8) \
VECTOR_INTRIN(_mm_maskz_cvtusepi64_epi16) \
VECTOR_INTRIN(_mm_maskz_cvtusepi64_epi32) \
VECTOR_INTRIN(_mm_maskz_cvtusepi64_epi8) \
VECTOR_INTRIN(_mm_maskz_dbsad_epu8) \
VECTOR_INTRIN(_mm_maskz_div_pd) \
VECTOR_INTRIN(_mm_maskz_div_ps) \
VECTOR_INTRIN(_mm_maskz_div_round_sd) \
VECTOR_INTRIN(_mm_maskz_div_round_ss) \
VECTOR_INTRIN(_mm_maskz_expand_epi32) \
VECTOR_INTRIN(_mm_maskz_expand_epi64) \
VECTOR_INTRIN(_mm_maskz_expandloadu_epi32) \
VECTOR_INTRIN(_mm_maskz_expandloadu_epi64) \
VECTOR_INTRIN(_mm_maskz_expandloadu_pd) \
VECTOR_INTRIN(_mm_maskz_expandloadu_ps) \
VECTOR_INTRIN(_mm_maskz_expand_pd) \
VECTOR_INTRIN(_mm_maskz_expand_ps) \
VECTOR_INTRIN(_mm_maskz_fixupimm_pd) \
VECTOR_INTRIN(_mm_maskz_fixupimm_ps) \
VECTOR_INTRIN(_mm_maskz_fixupimm_round_sd) \
VECTOR_INTRIN(_mm_maskz_fixupimm_round_ss) \
VECTOR_INTRIN(_mm_maskz_fmadd_pd) \
VECTOR_INTRIN(_mm_maskz_fmadd_ps) \
VECTOR_INTRIN(_mm_maskz_fmadd_round_sd) \
VECTOR_INTRIN(_mm_maskz_fmadd_round_ss) \
VECTOR_INTRIN(_mm_maskz_fmaddsub_pd) \
VECTOR_INTRIN(_mm_maskz_fmaddsub_ps) \
VECTOR_INTRIN(_mm_maskz_fmsubadd_pd) \
VECTOR_INTRIN(_mm_maskz_fmsubadd_ps) \
VECTOR_INTRIN(_mm_maskz_fmsub_pd) \
VECTOR_INTRIN(_mm_maskz_fmsub_ps) \
VECTOR_INTRIN(_mm_maskz_fmsub_round_sd) \
VECTOR_INTRIN(_mm_maskz_fmsub_round_ss) \
VECTOR_INTRIN(_mm_maskz_fnmadd_pd) \
VECTOR_INTRIN(_mm_maskz_fnmadd_ps) \
VECTOR_INTRIN(_mm_maskz_fnmadd_round_sd) \
VECTOR_INTRIN(_mm_maskz_fnmadd_round_ss) \
VECTOR_INTRIN(_mm_maskz_fnmsub_pd) \
VECTOR_INTRIN(_mm_maskz_fnmsub_ps) \
VECTOR_INTRIN(_mm_maskz_fnmsub_round_sd) \
VECTOR_INTRIN(_mm_maskz_fnmsub_round_ss) \
VECTOR_INTRIN(_mm_maskz_getexp_pd) \
VECTOR_INTRIN(_mm_maskz_getexp_ps) \
VECTOR_INTRIN(_mm_maskz_getexp_round_sd) \
VECTOR_INTRIN(_mm_maskz_getexp_round_ss) \
VECTOR_INTRIN(_mm_maskz_getmant_pd) \
VECTOR_INTRIN(_mm_maskz_getmant_ps) \
VECTOR_INTRIN(_mm_maskz_getmant_round_sd) \
VECTOR_INTRIN(_mm_maskz_getmant_round_ss) \
VECTOR_INTRIN(_mm_maskz_load_epi32) \
VECTOR_INTRIN(_mm_maskz_load_epi64) \
VECTOR_INTRIN(_mm_maskz_load_pd) \
VECTOR_INTRIN(_mm_maskz_load_ps) \
VECTOR_INTRIN(_mm_maskz_load_sd) \
VECTOR_INTRIN(_mm_maskz_load_ss) \
VECTOR_INTRIN(_mm_maskz_loadu_epi16) \
VECTOR_INTRIN(_mm_maskz_loadu_epi32) \
VECTOR_INTRIN(_mm_maskz_loadu_epi64) \
VECTOR_INTRIN(_mm_maskz_loadu_epi8) \
VECTOR_INTRIN(_mm_maskz_loadu_pd) \
VECTOR_INTRIN(_mm_maskz_loadu_ps) \
VECTOR_INTRIN(_mm_maskz_lzcnt_epi32) \
VECTOR_INTRIN(_mm_maskz_lzcnt_epi64) \
VECTOR_INTRIN(_mm_maskz_madd52hi_epu64) \
VECTOR_INTRIN(_mm_maskz_madd52lo_epu64) \
VECTOR_INTRIN(_mm_maskz_madd_epi16) \
VECTOR_INTRIN(_mm_maskz_maddubs_epi16) \
VECTOR_INTRIN(_mm_maskz_max_epi16) \
VECTOR_INTRIN(_mm_maskz_max_epi32) \
VECTOR_INTRIN(_mm_maskz_max_epi64) \
VECTOR_INTRIN(_mm_maskz_max_epi8) \
VECTOR_INTRIN(_mm_maskz_max_epu16) \
VECTOR_INTRIN(_mm_maskz_max_epu32) \
VECTOR_INTRIN(_mm_maskz_max_epu64) \
VECTOR_INTRIN(_mm_maskz_max_epu8) \
VECTOR_INTRIN(_mm_maskz_max_pd) \
VECTOR_INTRIN(_mm_maskz_max_ps) \
VECTOR_INTRIN(_mm_maskz_max_round_sd) \
VECTOR_INTRIN(_mm_maskz_max_round_ss) \
VECTOR_INTRIN(_mm_maskz_min_epi16) \
VECTOR_INTRIN(_mm_maskz_min_epi32) \
VECTOR_INTRIN(_mm_maskz_min_epi64) \
VECTOR_INTRIN(_mm_maskz_min_epi8) \
VECTOR_INTRIN(_mm_maskz_min_epu16) \
VECTOR_INTRIN(_mm_maskz_min_epu32) \
VECTOR_INTRIN(_mm_maskz_min_epu64) \
VECTOR_INTRIN(_mm_maskz_min_epu8) \
VECTOR_INTRIN(_mm_maskz_min_pd) \
VECTOR_INTRIN(_mm_maskz_min_ps) \
VECTOR_INTRIN(_mm_maskz_min_round_sd) \
VECTOR_INTRIN(_mm_maskz_min_round_ss) \
VECTOR_INTRIN(_mm_maskz_movedup_pd) \
VECTOR_INTRIN(_mm_maskz_movehdup_ps) \
VECTOR_INTRIN(_mm_maskz_moveldup_ps) \
VECTOR_INTRIN(_mm_maskz_mov_epi16) \
VECTOR_INTRIN(_mm_maskz_mov_epi32) \
VECTOR_INTRIN(_mm_maskz_mov_epi64) \
VECTOR_INTRIN(_mm_maskz_mov_epi8) \
VECTOR_INTRIN(_mm_maskz_move_sd) \
VECTOR_INTRIN(_mm_maskz_move_ss) \
VECTOR_INTRIN(_mm_maskz_mov_pd) \
VECTOR_INTRIN(_mm_maskz_mov_ps) \
VECTOR_INTRIN(_mm_maskz_mul_epi32) \
VECTOR_INTRIN(_mm_maskz_mul_epu32) \
VECTOR_INTRIN(_mm_maskz_mulhi_epi16) \
VECTOR_INTRIN(_mm_maskz_mulhi_epu16) \
VECTOR_INTRIN(_mm_maskz_mulhrs_epi16) \
VECTOR_INTRIN(_mm_maskz_mullo_epi16) \
VECTOR_INTRIN(_mm_maskz_mullo_epi32) \
VECTOR_INTRIN(_mm_maskz_mullo_epi64) \
VECTOR_INTRIN(_mm_maskz_mul_pd) \
VECTOR_INTRIN(_mm_maskz_mul_ps) \
VECTOR_INTRIN(_mm_maskz_mul_round_sd) \
VECTOR_INTRIN(_mm_maskz_mul_round_ss) \
VECTOR_INTRIN(_mm_maskz_multishift_epi64_epi8) \
VECTOR_INTRIN(_mm_maskz_or_epi32) \
VECTOR_INTRIN(_mm_maskz_or_epi64) \
VECTOR_INTRIN(_mm_maskz_or_pd) \
VECTOR_INTRIN(_mm_maskz_or_ps) \
VECTOR_INTRIN(_mm_maskz_packs_epi16) \
VECTOR_INTRIN(_mm_maskz_packs_epi32) \
VECTOR_INTRIN(_mm_maskz_packus_epi16) \
VECTOR_INTRIN(_mm_maskz_packus_epi32) \
VECTOR_INTRIN(_mm_maskz_permute_pd) \
VECTOR_INTRIN(_mm_maskz_permute_ps) \
VECTOR_INTRIN(_mm_maskz_permutevar_pd) \
VECTOR_INTRIN(_mm_maskz_permutevar_ps) \
VECTOR_INTRIN(_mm_maskz_permutex2var_epi16) \
VECTOR_INTRIN(_mm_maskz_permutex2var_epi32) \
VECTOR_INTRIN(_mm_maskz_permutex2var_epi64) \
VECTOR_INTRIN(_mm_maskz_permutex2var_epi8) \
VECTOR_INTRIN(_mm_maskz_permutex2var_pd) \
VECTOR_INTRIN(_mm_maskz_permutex2var_ps) \
VECTOR_INTRIN(_mm_maskz_permutexvar_epi16) \
VECTOR_INTRIN(_mm_maskz_permutexvar_epi8) \
VECTOR_INTRIN(_mm_maskz_range_pd) \
VECTOR_INTRIN(_mm_maskz_range_ps) \
VECTOR_INTRIN(_mm_maskz_range_round_sd) \
VECTOR_INTRIN(_mm_maskz_range_round_ss) \
VECTOR_INTRIN(_mm_maskz_rcp14_pd) \
VECTOR_INTRIN(_mm_maskz_rcp14_ps) \
VECTOR_INTRIN(_mm_maskz_rcp14_sd) \
VECTOR_INTRIN(_mm_maskz_rcp14_ss) \
VECTOR_INTRIN(_mm_maskz_rcp28_round_sd) \
VECTOR_INTRIN(_mm_maskz_rcp28_round_ss) \
VECTOR_INTRIN(_mm_maskz_reduce_pd) \
VECTOR_INTRIN(_mm_maskz_reduce_ps) \
VECTOR_INTRIN(_mm_maskz_reduce_round_sd) \
VECTOR_INTRIN(_mm_maskz_reduce_round_ss) \
VECTOR_INTRIN(_mm_maskz_reduce_sd) \
VECTOR_INTRIN(_mm_maskz_reduce_ss) \
VECTOR_INTRIN(_mm_maskz_rol_epi32) \
VECTOR_INTRIN(_mm_maskz_rol_epi64) \
VECTOR_INTRIN(_mm_maskz_rolv_epi32) \
VECTOR_INTRIN(_mm_maskz_rolv_epi64) \
VECTOR_INTRIN(_mm_maskz_ror_epi32) \
VECTOR_INTRIN(_mm_maskz_ror_epi64) \
VECTOR_INTRIN(_mm_maskz_rorv_epi32) \
VECTOR_INTRIN(_mm_maskz_rorv_epi64) \
VECTOR_INTRIN(_mm_maskz_roundscale_pd) \
VECTOR_INTRIN(_mm_maskz_roundscale_ps) \
VECTOR_INTRIN(_mm_maskz_roundscale_round_sd) \
VECTOR_INTRIN(_mm_maskz_roundscale_round_ss) \
VECTOR_INTRIN(_mm_maskz_rsqrt14_pd) \
VECTOR_INTRIN(_mm_maskz_rsqrt14_ps) \
VECTOR_INTRIN(_mm_maskz_rsqrt14_sd) \
VECTOR_INTRIN(_mm_maskz_rsqrt14_ss) \
VECTOR_INTRIN(_mm_maskz_rsqrt28_round_sd) \
VECTOR_INTRIN(_mm_maskz_rsqrt28_round_ss) \
VECTOR_INTRIN(_mm_maskz_scalef_pd) \
VECTOR_INTRIN(_mm_maskz_scalef_ps) \
VECTOR_INTRIN(_mm_maskz_scalef_round_sd) \
VECTOR_INTRIN(_mm_maskz_scalef_round_ss) \
VECTOR_INTRIN(_mm_maskz_set1_epi16) \
VECTOR_INTRIN(_mm_maskz_set1_epi32) \
VECTOR_INTRIN(_mm_maskz_set1_epi64) \
VECTOR_INTRIN(_mm_maskz_set1_epi8) \
VECTOR_INTRIN(_mm_maskz_shuffle_epi32) \
VECTOR_INTRIN(_mm_maskz_shuffle_epi8) \
VECTOR_INTRIN(_mm_maskz_shufflehi_epi16) \
VECTOR_INTRIN(_mm_maskz_shufflelo_epi16) \
VECTOR_INTRIN(_mm_maskz_shuffle_pd) \
VECTOR_INTRIN(_mm_maskz_shuffle_ps) \
VECTOR_INTRIN(_mm_maskz_sll_epi16) \
VECTOR_INTRIN(_mm_maskz_sll_epi32) \
VECTOR_INTRIN(_mm_maskz_sll_epi64) \
VECTOR_INTRIN(_mm_maskz_slli_epi16) \
VECTOR_INTRIN(_mm_maskz_slli_epi32) \
VECTOR_INTRIN(_mm_maskz_slli_epi64) \
VECTOR_INTRIN(_mm_maskz_sllv_epi16) \
VECTOR_INTRIN(_mm_maskz_sllv_epi32) \
VECTOR_INTRIN(_mm_maskz_sllv_epi64) \
VECTOR_INTRIN(_mm_maskz_sqrt_pd) \
VECTOR_INTRIN(_mm_maskz_sqrt_ps) \
VECTOR_INTRIN(_mm_maskz_sqrt_round_sd) \
VECTOR_INTRIN(_mm_maskz_sqrt_round_ss) \
VECTOR_INTRIN(_mm_maskz_sra_epi16) \
VECTOR_INTRIN(_mm_maskz_sra_epi32) \
VECTOR_INTRIN(_mm_maskz_sra_epi64) \
VECTOR_INTRIN(_mm_maskz_srai_epi16) \
VECTOR_INTRIN(_mm_maskz_srai_epi32) \
VECTOR_INTRIN(_mm_maskz_srai_epi64) \
VECTOR_INTRIN(_mm_maskz_srav_epi16) \
VECTOR_INTRIN(_mm_maskz_srav_epi32) \
VECTOR_INTRIN(_mm_maskz_srav_epi64) \
VECTOR_INTRIN(_mm_maskz_srl_epi16) \
VECTOR_INTRIN(_mm_maskz_srl_epi32) \
VECTOR_INTRIN(_mm_maskz_srl_epi64) \
VECTOR_INTRIN(_mm_maskz_srli_epi16) \
VECTOR_INTRIN(_mm_maskz_srli_epi32) \
VECTOR_INTRIN(_mm_maskz_srli_epi64) \
VECTOR_INTRIN(_mm_maskz_srlv_epi16) \
VECTOR_INTRIN(_mm_maskz_srlv_epi32) \
VECTOR_INTRIN(_mm_maskz_srlv_epi64) \
VECTOR_INTRIN(_mm_maskz_sub_epi16) \
VECTOR_INTRIN(_mm_maskz_sub_epi32) \
VECTOR_INTRIN(_mm_maskz_sub_epi64) \
VECTOR_INTRIN(_mm_maskz_sub_epi8) \
VECTOR_INTRIN(_mm_maskz_sub_pd) \
VECTOR_INTRIN(_mm_maskz_sub_ps) \
VECTOR_INTRIN(_mm_maskz_sub_round_sd) \
VECTOR_INTRIN(_mm_maskz_sub_round_ss) \
VECTOR_INTRIN(_mm_maskz_subs_epi16) \
VECTOR_INTRIN(_mm_maskz_subs_epi8) \
VECTOR_INTRIN(_mm_maskz_subs_epu16) \
VECTOR_INTRIN(_mm_maskz_subs_epu8) \
VECTOR_INTRIN(_mm_maskz_ternarylogic_epi32) \
VECTOR_INTRIN(_mm_maskz_ternarylogic_epi64) \
VECTOR_INTRIN(_mm_maskz_unpackhi_epi16) \
VECTOR_INTRIN(_mm_maskz_unpackhi_epi32) \
VECTOR_INTRIN(_mm_maskz_unpackhi_epi64) \
VECTOR_INTRIN(_mm_maskz_unpackhi_epi8) \
VECTOR_INTRIN(_mm_maskz_unpackhi_pd) \
VECTOR_INTRIN(_mm_maskz_unpackhi_ps) \
VECTOR_INTRIN(_mm_maskz_unpacklo_epi16) \
VECTOR_INTRIN(_mm_maskz_unpacklo_epi32) \
VECTOR_INTRIN(_mm_maskz_unpacklo_epi64) \
VECTOR_INTRIN(_mm_maskz_unpacklo_epi8) \
VECTOR_INTRIN(_mm_maskz_unpacklo_pd) \
VECTOR_INTRIN(_mm_maskz_unpacklo_ps) \
VECTOR_INTRIN(_mm_maskz_xor_epi32) \
VECTOR_INTRIN(_mm_maskz_xor_epi64) \
VECTOR_INTRIN(_mm_maskz_xor_pd) \
VECTOR_INTRIN(_mm_maskz_xor_ps) \
VECTOR_INTRIN(_mm_max_epi16) \
VECTOR_INTRIN(_mm_max_epi32) \
VECTOR_INTRIN(_mm_max_epi64) \
VECTOR_INTRIN(_mm_max_epu16) \
VECTOR_INTRIN(_mm_max_epu32) \
VECTOR_INTRIN(_mm_max_epu64) \
VECTOR_INTRIN(_mm_max_epu8) \
VECTOR_INTRIN(_mm_max_pd) \
VECTOR_INTRIN(_mm_max_ps) \
VECTOR_INTRIN(_mm_max_round_sd) \
VECTOR_INTRIN(_mm_max_round_ss) \
VECTOR_INTRIN(_mm_max_sd) \
VECTOR_INTRIN(_mm_max_ss) \
VECTOR_INTRIN(_mm_mfence) \
VECTOR_INTRIN(_mm_min_epi16) \
VECTOR_INTRIN(_mm_min_epi32) \
VECTOR_INTRIN(_mm_min_epi64) \
VECTOR_INTRIN(_mm_min_epu16) \
VECTOR_INTRIN(_mm_min_epu32) \
VECTOR_INTRIN(_mm_min_epu64) \
VECTOR_INTRIN(_mm_min_epu8) \
VECTOR_INTRIN(_mm_min_pd) \
VECTOR_INTRIN(_mm_minpos_epu16) \
VECTOR_INTRIN(_mm_min_ps) \
VECTOR_INTRIN(_mm_min_round_sd) \
VECTOR_INTRIN(_mm_min_round_ss) \
VECTOR_INTRIN(_mm_min_sd) \
VECTOR_INTRIN(_mm_min_ss) \
VECTOR_INTRIN(_mm_mmask_i32gather_epi32) \
VECTOR_INTRIN(_mm_mmask_i32gather_epi64) \
VECTOR_INTRIN(_mm_mmask_i32gather_pd) \
VECTOR_INTRIN(_mm_mmask_i32gather_ps) \
VECTOR_INTRIN(_mm_mmask_i64gather_epi32) \
VECTOR_INTRIN(_mm_mmask_i64gather_epi64) \
VECTOR_INTRIN(_mm_mmask_i64gather_pd) \
VECTOR_INTRIN(_mm_mmask_i64gather_ps) \
VECTOR_INTRIN(_mm_monitor) \
VECTOR_INTRIN(_mm_movedup_pd) \
VECTOR_INTRIN(_mm_move_epi64) \
VECTOR_INTRIN(_mm_movehdup_ps) \
VECTOR_INTRIN(_mm_movehl_ps) \
VECTOR_INTRIN(_mm_moveldup_ps) \
VECTOR_INTRIN(_mm_movelh_ps) \
VECTOR_INTRIN(_mm_movemask_epi8) \
VECTOR_INTRIN(_mm_movemask_pd) \
VECTOR_INTRIN(_mm_movemask_ps) \
VECTOR_INTRIN(_mm_movepi16_mask) \
VECTOR_INTRIN(_mm_movepi32_mask) \
VECTOR_INTRIN(_mm_movepi64_mask) \
VECTOR_INTRIN(_mm_movepi64_pi64) \
VECTOR_INTRIN(_mm_movepi8_mask) \
VECTOR_INTRIN(_mm_move_sd) \
VECTOR_INTRIN(_mm_move_ss) \
VECTOR_INTRIN(_mm_movm_epi16) \
VECTOR_INTRIN(_mm_movm_epi32) \
VECTOR_INTRIN(_mm_movm_epi64) \
VECTOR_INTRIN(_mm_movm_epi8) \
VECTOR_INTRIN(_mm_movpi64_epi64) \
VECTOR_INTRIN(_mm_mpsadbw_epu8) \
VECTOR_INTRIN(_mm_mul_epi32) \
VECTOR_INTRIN(_mm_mul_epu32) \
VECTOR_INTRIN(_mm_mulhi_epi16) \
VECTOR_INTRIN(_mm_mulhi_epu16) \
VECTOR_INTRIN(_mm_mullo_epi16) \
VECTOR_INTRIN(_mm_mullo_epi32) \
VECTOR_INTRIN(_mm_mullo_epi64) \
VECTOR_INTRIN(_mm_mul_pd) \
VECTOR_INTRIN(_mm_mul_ps) \
VECTOR_INTRIN(_mm_mul_round_sd) \
VECTOR_INTRIN(_mm_mul_round_ss) \
VECTOR_INTRIN(_mm_mul_sd) \
VECTOR_INTRIN(_mm_mul_ss) \
VECTOR_INTRIN(_mm_mul_su32) \
VECTOR_INTRIN(_mm_multishift_epi64_epi8) \
VECTOR_INTRIN(_mm_mwait) \
VECTOR_INTRIN(_mm_or_pd) \
VECTOR_INTRIN(_mm_or_ps) \
VECTOR_INTRIN(_mm_or_si128) \
VECTOR_INTRIN(_mm_packs_epi16) \
VECTOR_INTRIN(_mm_packs_epi32) \
VECTOR_INTRIN(_mm_packus_epi16) \
VECTOR_INTRIN(_mm_packus_epi32) \
VECTOR_INTRIN(_mm_pause) \
VECTOR_INTRIN(_mm_pcommit) \
VECTOR_INTRIN(_mm_permute_pd) \
VECTOR_INTRIN(_mm_permute_ps) \
VECTOR_INTRIN(_mm_permutevar_pd) \
VECTOR_INTRIN(_mm_permutevar_ps) \
VECTOR_INTRIN(_mm_permutex2var_epi16) \
VECTOR_INTRIN(_mm_permutex2var_epi32) \
VECTOR_INTRIN(_mm_permutex2var_epi64) \
VECTOR_INTRIN(_mm_permutex2var_epi8) \
VECTOR_INTRIN(_mm_permutex2var_pd) \
VECTOR_INTRIN(_mm_permutex2var_ps) \
VECTOR_INTRIN(_mm_permutexvar_epi16) \
VECTOR_INTRIN(_mm_permutexvar_epi8) \
VECTOR_INTRIN(_mm_popcnt_u32) \
VECTOR_INTRIN(_mm_popcnt_u64) \
VECTOR_INTRIN(_mm_pow_pd) \
VECTOR_INTRIN(_mm_pow_ps) \
VECTOR_INTRIN(_mm_prefetch) \
VECTOR_INTRIN(_mm_range_pd) \
VECTOR_INTRIN(_mm_range_ps) \
VECTOR_INTRIN(_mm_range_round_sd) \
VECTOR_INTRIN(_mm_range_round_ss) \
VECTOR_INTRIN(_mm_rcp14_pd) \
VECTOR_INTRIN(_mm_rcp14_ps) \
VECTOR_INTRIN(_mm_rcp14_sd) \
VECTOR_INTRIN(_mm_rcp14_ss) \
VECTOR_INTRIN(_mm_rcp28_round_sd) \
VECTOR_INTRIN(_mm_rcp28_round_ss) \
VECTOR_INTRIN(_mm_rcp_ps) \
VECTOR_INTRIN(_mm_rcp_ss) \
VECTOR_INTRIN(_mm_reduce_pd) \
VECTOR_INTRIN(_mm_reduce_ps) \
VECTOR_INTRIN(_mm_reduce_round_sd) \
VECTOR_INTRIN(_mm_reduce_round_ss) \
VECTOR_INTRIN(_mm_reduce_sd) \
VECTOR_INTRIN(_mm_reduce_ss) \
VECTOR_INTRIN(_mm_rem_epi16) \
VECTOR_INTRIN(_mm_rem_epi32) \
VECTOR_INTRIN(_mm_rem_epi64) \
VECTOR_INTRIN(_mm_rem_epi8) \
VECTOR_INTRIN(_mm_rem_epu16) \
VECTOR_INTRIN(_mm_rem_epu32) \
VECTOR_INTRIN(_mm_rem_epu64) \
VECTOR_INTRIN(_mm_rem_epu8) \
VECTOR_INTRIN(_mm_rol_epi32) \
VECTOR_INTRIN(_mm_rol_epi64) \
VECTOR_INTRIN(_mm_rolv_epi32) \
VECTOR_INTRIN(_mm_rolv_epi64) \
VECTOR_INTRIN(_mm_ror_epi32) \
VECTOR_INTRIN(_mm_ror_epi64) \
VECTOR_INTRIN(_mm_rorv_epi32) \
VECTOR_INTRIN(_mm_rorv_epi64) \
VECTOR_INTRIN(_mm_round_pd) \
VECTOR_INTRIN(_mm_round_ps) \
VECTOR_INTRIN(_mm_roundscale_pd) \
VECTOR_INTRIN(_mm_roundscale_ps) \
VECTOR_INTRIN(_mm_roundscale_round_sd) \
VECTOR_INTRIN(_mm_roundscale_round_ss) \
VECTOR_INTRIN(_mm_round_sd) \
VECTOR_INTRIN(_mm_round_ss) \
VECTOR_INTRIN(_mm_rsqrt14_sd) \
VECTOR_INTRIN(_mm_rsqrt14_ss) \
VECTOR_INTRIN(_mm_rsqrt28_round_sd) \
VECTOR_INTRIN(_mm_rsqrt28_round_ss) \
VECTOR_INTRIN(_mm_rsqrt_ps) \
VECTOR_INTRIN(_mm_rsqrt_ss) \
VECTOR_INTRIN(_mm_sad_epu8) \
VECTOR_INTRIN(_mm_scalef_pd) \
VECTOR_INTRIN(_mm_scalef_ps) \
VECTOR_INTRIN(_mm_scalef_round_sd) \
VECTOR_INTRIN(_mm_scalef_round_ss) \
VECTOR_INTRIN(_mm_set1_epi16) \
VECTOR_INTRIN(_mm_set1_epi32) \
VECTOR_INTRIN(_mm_set1_epi64) \
VECTOR_INTRIN(_mm_set1_epi64x) \
VECTOR_INTRIN(_mm_set1_epi8) \
VECTOR_INTRIN(_mm_set1_pd) \
VECTOR_INTRIN(_mm_set1_pi16) \
VECTOR_INTRIN(_mm_set1_pi32) \
VECTOR_INTRIN(_mm_set1_pi8) \
VECTOR_INTRIN(_mm_setcsr) \
VECTOR_INTRIN(_mm_set_epi16) \
VECTOR_INTRIN(_mm_set_epi32) \
VECTOR_INTRIN(_mm_set_epi64) \
VECTOR_INTRIN(_mm_set_epi64x) \
VECTOR_INTRIN(_mm_set_epi8) \
VECTOR_INTRIN(_mm_set_pd) \
VECTOR_INTRIN(_mm_set_pi16) \
VECTOR_INTRIN(_mm_set_pi32) \
VECTOR_INTRIN(_mm_set_pi8) \
VECTOR_INTRIN(_mm_set_ps) \
VECTOR_INTRIN(_mm_set_ps1) \
VECTOR_INTRIN(_mm_setr_epi16) \
VECTOR_INTRIN(_mm_setr_epi32) \
VECTOR_INTRIN(_mm_setr_epi64) \
VECTOR_INTRIN(_mm_setr_epi8) \
VECTOR_INTRIN(_mm_setr_pd) \
VECTOR_INTRIN(_mm_setr_pi16) \
VECTOR_INTRIN(_mm_setr_pi32) \
VECTOR_INTRIN(_mm_setr_pi8) \
VECTOR_INTRIN(_mm_setr_ps) \
VECTOR_INTRIN(_mm_set_sd) \
VECTOR_INTRIN(_mm_set_ss) \
VECTOR_INTRIN(_mm_setzero_pd) \
VECTOR_INTRIN(_mm_setzero_ps) \
VECTOR_INTRIN(_mm_setzero_si128) \
VECTOR_INTRIN(_mm_setzero_si64) \
VECTOR_INTRIN(_mm_sfence) \
VECTOR_INTRIN(_mm_sha1msg1_epu32) \
VECTOR_INTRIN(_mm_sha1msg2_epu32) \
VECTOR_INTRIN(_mm_sha1nexte_epu32) \
VECTOR_INTRIN(_mm_sha1rnds4_epu32) \
VECTOR_INTRIN(_mm_sha256msg1_epu32) \
VECTOR_INTRIN(_mm_sha256msg2_epu32) \
VECTOR_INTRIN(_mm_sha256rnds2_epu32) \
VECTOR_INTRIN(_mm_shuffle_epi32) \
VECTOR_INTRIN(_mm_shufflehi_epi16) \
VECTOR_INTRIN(_mm_shufflelo_epi16) \
VECTOR_INTRIN(_mm_shuffle_pd) \
VECTOR_INTRIN(_mm_shuffle_ps) \
VECTOR_INTRIN(_mm_sincos_pd) \
VECTOR_INTRIN(_mm_sincos_ps) \
VECTOR_INTRIN(_mm_sind_pd) \
VECTOR_INTRIN(_mm_sind_ps) \
VECTOR_INTRIN(_mm_sinh_pd) \
VECTOR_INTRIN(_mm_sinh_ps) \
VECTOR_INTRIN(_mm_sin_pd) \
VECTOR_INTRIN(_mm_sin_ps) \
VECTOR_INTRIN(_mm_sll_epi16) \
VECTOR_INTRIN(_mm_sll_epi32) \
VECTOR_INTRIN(_mm_sll_epi64) \
VECTOR_INTRIN(_mm_slli_epi16) \
VECTOR_INTRIN(_mm_slli_epi32) \
VECTOR_INTRIN(_mm_slli_epi64) \
VECTOR_INTRIN(_mm_slli_si128) \
VECTOR_INTRIN(_mm_sllv_epi16) \
VECTOR_INTRIN(_mm_sllv_epi32) \
VECTOR_INTRIN(_mm_sllv_epi64) \
VECTOR_INTRIN(_mm_spflt_32) \
VECTOR_INTRIN(_mm_spflt_64) \
VECTOR_INTRIN(_mm_sqrt_pd) \
VECTOR_INTRIN(_mm_sqrt_ps) \
VECTOR_INTRIN(_mm_sqrt_round_sd) \
VECTOR_INTRIN(_mm_sqrt_round_ss) \
VECTOR_INTRIN(_mm_sqrt_sd) \
VECTOR_INTRIN(_mm_sqrt_ss) \
VECTOR_INTRIN(_mm_sra_epi16) \
VECTOR_INTRIN(_mm_sra_epi32) \
VECTOR_INTRIN(_mm_sra_epi64) \
VECTOR_INTRIN(_mm_srai_epi16) \
VECTOR_INTRIN(_mm_srai_epi32) \
VECTOR_INTRIN(_mm_srai_epi64) \
VECTOR_INTRIN(_mm_srav_epi16) \
VECTOR_INTRIN(_mm_srav_epi32) \
VECTOR_INTRIN(_mm_srav_epi64) \
VECTOR_INTRIN(_mm_srl_epi16) \
VECTOR_INTRIN(_mm_srl_epi32) \
VECTOR_INTRIN(_mm_srl_epi64) \
VECTOR_INTRIN(_mm_srli_epi16) \
VECTOR_INTRIN(_mm_srli_epi32) \
VECTOR_INTRIN(_mm_srli_epi64) \
VECTOR_INTRIN(_mm_srli_si128) \
VECTOR_INTRIN(_mm_srlv_epi16) \
VECTOR_INTRIN(_mm_srlv_epi32) \
VECTOR_INTRIN(_mm_srlv_epi64) \
VECTOR_INTRIN(_mm_store1_pd) \
VECTOR_INTRIN(_mm_storeh_pd) \
VECTOR_INTRIN(_mm_storeh_pi) \
VECTOR_INTRIN(_mm_storel_epi64) \
VECTOR_INTRIN(_mm_storel_pd) \
VECTOR_INTRIN(_mm_storel_pi) \
VECTOR_INTRIN(_mm_store_pd) \
VECTOR_INTRIN(_mm_store_ps) \
VECTOR_INTRIN(_mm_store_ps1) \
VECTOR_INTRIN(_mm_storer_pd) \
VECTOR_INTRIN(_mm_storer_ps) \
VECTOR_INTRIN(_mm_store_sd) \
VECTOR_INTRIN(_mm_store_si128) \
VECTOR_INTRIN(_mm_store_ss) \
VECTOR_INTRIN(_mm_storeu_pd) \
VECTOR_INTRIN(_mm_storeu_ps) \
VECTOR_INTRIN(_mm_storeu_si128) \
VECTOR_INTRIN(_mm_storeu_si16) \
VECTOR_INTRIN(_mm_storeu_si32) \
VECTOR_INTRIN(_mm_storeu_si64) \
VECTOR_INTRIN(_mm_stream_load_si128) \
VECTOR_INTRIN(_mm_stream_pd) \
VECTOR_INTRIN(_mm_stream_pi) \
VECTOR_INTRIN(_mm_stream_ps) \
VECTOR_INTRIN(_mm_stream_si128) \
VECTOR_INTRIN(_mm_stream_si32) \
VECTOR_INTRIN(_mm_stream_si64) \
VECTOR_INTRIN(_mm_sub_epi16) \
VECTOR_INTRIN(_mm_sub_epi32) \
VECTOR_INTRIN(_mm_sub_epi64) \
VECTOR_INTRIN(_mm_sub_epi8) \
VECTOR_INTRIN(_mm_sub_pd) \
VECTOR_INTRIN(_mm_sub_ps) \
VECTOR_INTRIN(_mm_sub_round_sd) \
VECTOR_INTRIN(_mm_sub_round_ss) \
VECTOR_INTRIN(_mm_sub_sd) \
VECTOR_INTRIN(_mm_subs_epi16) \
VECTOR_INTRIN(_mm_subs_epi8) \
VECTOR_INTRIN(_mm_subs_epu16) \
VECTOR_INTRIN(_mm_subs_epu8) \
VECTOR_INTRIN(_mm_sub_si64) \
VECTOR_INTRIN(_mm_sub_ss) \
VECTOR_INTRIN(_mm_svml_ceil_pd) \
VECTOR_INTRIN(_mm_svml_ceil_ps) \
VECTOR_INTRIN(_mm_svml_floor_pd) \
VECTOR_INTRIN(_mm_svml_floor_ps) \
VECTOR_INTRIN(_mm_svml_round_pd) \
VECTOR_INTRIN(_mm_svml_round_ps) \
VECTOR_INTRIN(_mm_svml_sqrt_pd) \
VECTOR_INTRIN(_mm_svml_sqrt_ps) \
VECTOR_INTRIN(_mm_tand_pd) \
VECTOR_INTRIN(_mm_tand_ps) \
VECTOR_INTRIN(_mm_tanh_pd) \
VECTOR_INTRIN(_mm_tanh_ps) \
VECTOR_INTRIN(_mm_tan_pd) \
VECTOR_INTRIN(_mm_tan_ps) \
VECTOR_INTRIN(_mm_ternarylogic_epi32) \
VECTOR_INTRIN(_mm_ternarylogic_epi64) \
VECTOR_INTRIN(_mm_testc_pd) \
VECTOR_INTRIN(_mm_testc_ps) \
VECTOR_INTRIN(_mm_testc_si128) \
VECTOR_INTRIN(_mm_test_epi16_mask) \
VECTOR_INTRIN(_mm_test_epi32_mask) \
VECTOR_INTRIN(_mm_test_epi64_mask) \
VECTOR_INTRIN(_mm_test_epi8_mask) \
VECTOR_INTRIN(_mm_testn_epi16_mask) \
VECTOR_INTRIN(_mm_testn_epi32_mask) \
VECTOR_INTRIN(_mm_testn_epi64_mask) \
VECTOR_INTRIN(_mm_testn_epi8_mask) \
VECTOR_INTRIN(_mm_testnzc_pd) \
VECTOR_INTRIN(_mm_testnzc_ps) \
VECTOR_INTRIN(_mm_testnzc_si128) \
VECTOR_INTRIN(_mm_testz_pd) \
VECTOR_INTRIN(_mm_testz_ps) \
VECTOR_INTRIN(_mm_testz_si128) \
VECTOR_INTRIN(_mm_trunc_pd) \
VECTOR_INTRIN(_mm_trunc_ps) \
VECTOR_INTRIN(_mm_tzcnt_32) \
VECTOR_INTRIN(_mm_tzcnt_64) \
VECTOR_INTRIN(_mm_tzcnti_32) \
VECTOR_INTRIN(_mm_tzcnti_64) \
VECTOR_INTRIN(_mm_ucomieq_sd) \
VECTOR_INTRIN(_mm_ucomieq_ss) \
VECTOR_INTRIN(_mm_ucomige_sd) \
VECTOR_INTRIN(_mm_ucomige_ss) \
VECTOR_INTRIN(_mm_ucomigt_sd) \
VECTOR_INTRIN(_mm_ucomigt_ss) \
VECTOR_INTRIN(_mm_ucomile_sd) \
VECTOR_INTRIN(_mm_ucomile_ss) \
VECTOR_INTRIN(_mm_ucomilt_sd) \
VECTOR_INTRIN(_mm_ucomilt_ss) \
VECTOR_INTRIN(_mm_ucomineq_sd) \
VECTOR_INTRIN(_mm_ucomineq_ss) \
VECTOR_INTRIN(_mm_udivrem_epi32) \
VECTOR_INTRIN(_mm_undefined_pd) \
VECTOR_INTRIN(_mm_undefined_ps) \
VECTOR_INTRIN(_mm_undefined_si128) \
VECTOR_INTRIN(_mm_unpackhi_epi16) \
VECTOR_INTRIN(_mm_unpackhi_epi32) \
VECTOR_INTRIN(_mm_unpackhi_epi64) \
VECTOR_INTRIN(_mm_unpackhi_epi8) \
VECTOR_INTRIN(_mm_unpackhi_pd) \
VECTOR_INTRIN(_mm_unpackhi_ps) \
VECTOR_INTRIN(_mm_unpacklo_epi16) \
VECTOR_INTRIN(_mm_unpacklo_epi32) \
VECTOR_INTRIN(_mm_unpacklo_epi64) \
VECTOR_INTRIN(_mm_unpacklo_epi8) \
VECTOR_INTRIN(_mm_unpacklo_pd) \
VECTOR_INTRIN(_mm_unpacklo_ps) \
VECTOR_INTRIN(_mm_xor_pd) \
VECTOR_INTRIN(_mm_xor_ps) \
VECTOR_INTRIN(_mm_xor_si128) \
VECTOR_INTRIN(_m_packssdw) \
VECTOR_INTRIN(_m_packsswb) \
VECTOR_INTRIN(_m_packuswb) \
VECTOR_INTRIN(_m_paddb) \
VECTOR_INTRIN(_m_paddd) \
VECTOR_INTRIN(_m_paddsb) \
VECTOR_INTRIN(_m_paddsw) \
VECTOR_INTRIN(_m_paddusb) \
VECTOR_INTRIN(_m_paddusw) \
VECTOR_INTRIN(_m_paddw) \
VECTOR_INTRIN(_m_pand) \
VECTOR_INTRIN(_m_pandn) \
VECTOR_INTRIN(_m_pavgb) \
VECTOR_INTRIN(_m_pavgw) \
VECTOR_INTRIN(_m_pcmpeqb) \
VECTOR_INTRIN(_m_pcmpeqd) \
VECTOR_INTRIN(_m_pcmpeqw) \
VECTOR_INTRIN(_m_pcmpgtb) \
VECTOR_INTRIN(_m_pcmpgtd) \
VECTOR_INTRIN(_m_pcmpgtw) \
VECTOR_INTRIN(_m_pextrw) \
VECTOR_INTRIN(_m_pinsrw) \
VECTOR_INTRIN(_m_pmaddwd) \
VECTOR_INTRIN(_m_pmaxsw) \
VECTOR_INTRIN(_m_pmaxub) \
VECTOR_INTRIN(_m_pminsw) \
VECTOR_INTRIN(_m_pminub) \
VECTOR_INTRIN(_m_pmovmskb) \
VECTOR_INTRIN(_m_pmulhuw) \
VECTOR_INTRIN(_m_pmulhw) \
VECTOR_INTRIN(_m_pmullw) \
VECTOR_INTRIN(_m_por) \
VECTOR_INTRIN(_m_psadbw) \
VECTOR_INTRIN(_m_pshufw) \
VECTOR_INTRIN(_m_pslld) \
VECTOR_INTRIN(_m_pslldi) \
VECTOR_INTRIN(_m_psllq) \
VECTOR_INTRIN(_m_psllqi) \
VECTOR_INTRIN(_m_psllw) \
VECTOR_INTRIN(_m_psllwi) \
VECTOR_INTRIN(_m_psrad) \
VECTOR_INTRIN(_m_psradi) \
VECTOR_INTRIN(_m_psraw) \
VECTOR_INTRIN(_m_psrawi) \
VECTOR_INTRIN(_m_psrld) \
VECTOR_INTRIN(_m_psrldi) \
VECTOR_INTRIN(_m_psrlq) \
VECTOR_INTRIN(_m_psrlqi) \
VECTOR_INTRIN(_m_psrlw) \
VECTOR_INTRIN(_m_psrlwi) \
VECTOR_INTRIN(_m_psubb) \
VECTOR_INTRIN(_m_psubd) \
VECTOR_INTRIN(_m_psubsb) \
VECTOR_INTRIN(_m_psubsw) \
VECTOR_INTRIN(_m_psubusb) \
VECTOR_INTRIN(_m_psubusw) \
VECTOR_INTRIN(_m_psubw) \
VECTOR_INTRIN(_m_punpckhbw) \
VECTOR_INTRIN(_m_punpckhdq) \
VECTOR_INTRIN(_m_punpckhwd) \
VECTOR_INTRIN(_m_punpcklbw) \
VECTOR_INTRIN(_m_punpckldq) \
VECTOR_INTRIN(_m_punpcklwd) \
VECTOR_INTRIN(_m_pxor) \
VECTOR_INTRIN(_m_to_int) \
VECTOR_INTRIN(_m_to_int64) \
VECTOR_INTRIN(_mulx_u32) \
VECTOR_INTRIN(_mulx_u64) \
VECTOR_INTRIN(_pdep_u32) \
VECTOR_INTRIN(_pdep_u64) \
VECTOR_INTRIN(_pext_u32) \
VECTOR_INTRIN(_pext_u64) \
VECTOR_INTRIN(_popcnt32) \
VECTOR_INTRIN(_popcnt64) \
VECTOR_INTRIN(_rdpmc) \
VECTOR_INTRIN(_rdrand16_step) \
VECTOR_INTRIN(_rdrand32_step) \
VECTOR_INTRIN(_rdrand64_step) \
VECTOR_INTRIN(_rdseed16_step) \
VECTOR_INTRIN(_rdseed32_step) \
VECTOR_INTRIN(_rdseed64_step) \
VECTOR_INTRIN(_rdtsc) \
VECTOR_INTRIN(__rdtscp) \
VECTOR_INTRIN(_readfsbase_u32) \
VECTOR_INTRIN(_readfsbase_u64) \
VECTOR_INTRIN(_readgsbase_u32) \
VECTOR_INTRIN(_readgsbase_u64) \
VECTOR_INTRIN(_rotl) \
VECTOR_INTRIN(_rotl64) \
VECTOR_INTRIN(_rotr) \
VECTOR_INTRIN(_rotr64) \
VECTOR_INTRIN(_rotwl) \
VECTOR_INTRIN(_rotwr) \
VECTOR_INTRIN(_storebe_i16) \
VECTOR_INTRIN(_storebe_i32) \
VECTOR_INTRIN(_storebe_i64) \
VECTOR_INTRIN(_subborrow_u32) \
VECTOR_INTRIN(_subborrow_u64) \
VECTOR_INTRIN(_tzcnt_u32) \
VECTOR_INTRIN(_tzcnt_u64) \
VECTOR_INTRIN(_writefsbase_u32) \
VECTOR_INTRIN(_writefsbase_u64) \
VECTOR_INTRIN(_writegsbase_u32) \
VECTOR_INTRIN(_writegsbase_u64) \
VECTOR_INTRIN(_xabort) \
VECTOR_INTRIN(_xbegin) \
VECTOR_INTRIN(_xend) \
VECTOR_INTRIN(_xgetbv) \
VECTOR_INTRIN(_xrstor) \
VECTOR_INTRIN(_xrstor64) \
VECTOR_INTRIN(_xrstors) \
VECTOR_INTRIN(_xrstors64) \
VECTOR_INTRIN(_xsave) \
VECTOR_INTRIN(_xsave64) \
VECTOR_INTRIN(_xsavec) \
VECTOR_INTRIN(_xsavec64) \
VECTOR_INTRIN(_xsaveopt) \
VECTOR_INTRIN(_xsaveopt64) \
VECTOR_INTRIN(_xsaves) \
VECTOR_INTRIN(_xsaves64) \
VECTOR_INTRIN(_xsetbv) \
VECTOR_INTRIN(_xtest) \
\
END

template <typename T>
struct RemoveTopLevelPointer
{
    typedef T type;
};

template <typename P>
struct RemoveTopLevelPointer<P*>
{
    typedef P type;
};

static void do_alias(const char* newname, const char* existing)
{
    std::cout << "{\n"
        << "scope_entry_list_t *entry_list = query_in_scope_str(decl_context, uniquestr(\"" << existing << "\"), /* field_path */ NULL);\n"
        << "ERROR_CONDITION(entry_list == NULL, \"Symbol '" << existing << "' should have been declared\",0);\n"
        << "scope_entry_t* orig_sym = entry_list_head(entry_list);\n"
        << "entry_list_free(entry_list);\n"
        << "scope_entry_t* new_sym = new_symbol(decl_context, decl_context->current_scope, uniquestr(\"" << newname << "\"));\n"
        << "new_sym->kind = SK_FUNCTION;"
        << "new_sym->do_not_print = 1;\n"
        << "new_sym->type_information = orig_sym->type_information;\n"
        << "symbol_entity_specs_set_is_builtin(new_sym, 1);\n"
        << "}\n";
}

int main(int, char**)
{
#define VECTOR_INTRIN(X) \
    f<RemoveTopLevelPointer<decltype(X)>::type>(#X);
#define VECTOR_ALIAS(newname, existing) \
    do_alias(#newname, #existing);
    VECTOR_INTRINSICS_LIST
#undef VECTOR_INTRIN
}
